{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Copy of AI Evaluator NER.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cohC16/nlpSoph/blob/main/Copy_of_AI_Evaluator_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t04pqGy0Ez6K"
      },
      "source": [
        "# New entities recogniser, annotatation with BILUO scheme, using spaCy\n",
        "\n",
        " * Use of pretrained Machine Learning (ML) model is quite prevalent in vision-related problems, where it is tuned for the desired task, nonetheless, last couple of years ([Peters et al.](https://www.aclweb.org/anthology/N18-1202/), [Akbik et al.](https://alanakbik.github.io/papers/coling2018.pdf)) has spurred the use of pretrained Natural Language Processing (NLP) models to do the same for NLP tasks. \n",
        " \n",
        " * This notebook uses a pretrained [spaCy](https://spacy.io/models/en) model to train for user-specific entities in texts. \n",
        " \n",
        " * Read [here](https://ruder.io/state-of-transfer-learning-in-nlp/) for the latest state of transfer learning in NLP.\n",
        " \n",
        " * The pretrained [model](https://spacy.io/models/en) used here is convolution neural network (CNN) architecture trained on [OneNotes](https://catalog.ldc.upenn.edu/LDC2013T19) \n",
        " \n",
        " * The customised entity recogniser is trained on [BILUO](https://spacy.io/api/annotation#biluo) scheme. Note here that the BILUO scheme trains and performs better than IOB scheme. Read faq of [README](README.md) \n",
        " \n",
        " * This is an extension with explanation for already provided [example](https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py) by spaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44NiGjJIEz6T"
      },
      "source": [
        "## Load a NLP model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucAdW4hiEz6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b9121f-b1c7-4766-b35d-9f3f9519f86e"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "!pip install -U spacy>=3.2.0\n",
        "!pip install -U https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0.tar.gz\n",
        "import spacy\n",
        "import numpy as np\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "parser = nlp.get_pipe(\"parser\")\n",
        "ner.add_label(\"FEATURE\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (59.5.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (59.5.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "     |████████████████████████████████| 13.9 MB 505 kB/s            \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.5.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0.tar.gz (13.9 MB)\n",
            "     |████████████████████████████████| 13.9 MB 518 kB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.5.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UeKtwkHEz6Y"
      },
      "source": [
        "## Data Annotations\n",
        "\n",
        " * [Using BILUO scheme](#biluo)\n",
        " * [Using offset indices](#offset)\n",
        " * [Custom Doc](#customdoc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P90mlvASwT35"
      },
      "source": [
        "text = \"One of the goals of traffic engineering is to achieve a flexible tradeoff between fairness and throughput so that users are satisfied with their bandwidth allocation and the network operator is satisfied with the utilization of network resources In this paper we propose a novel way to balance the throughput and fairness objectives with linear programming It allows the network operator to precisely control the tradeoff by bounding the fairness degradation for each commodity compared to the maxmin fair solution or the throughput degradation compared to the optimal throughput We also present improvements to a previous algorithm that achieves maxmin fairness by solving a series of linear programs We significantly reduce the number of steps needed when the access rate of commodities is limited We extend the algorithm to two important practical use cases importance weights and piecewise linear utility functions for commodities Our experiments on synthetic and real networks show that our algorithms achieve a significant speedup and provide practical insights on the tradeoff between fairness and throughput The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data such as medical records or other personal information To address those concerns one promising approach is Private Aggregation of Teacher Ensembles or PATE which transfers to a student model the knowledge of an ensemble of teacher models with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers’ answers However PATE has so far been evaluated only on simple classification tasks like MNIST leaving unclear its utility when applied to largerscale learning tasks and realworld datasets In this work we show how PATE can scale to learning tasks with large numbers of output classes and uncurated imbalanced training data with errors For this we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise and prove their tighter differential guarantees Our new mechanisms build on two insights the chance of teacher consensus is increased by using more concentrated noise and lacking consensus no answer need be given to a student The consensus answers used are more likely to be correct offer better intuitive privacy and incur lower privacy cost Our evaluation shows our mechanisms improve on the original PATE on all measures and scale to larger tasks with both high utility and very strong privacy Can we efficiently extract useful information from a large user dataset while protecting the privacy of the users and ensuring fairness in representation We cast this problem as an instance of a deletion submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria We propose the first memory centralized streaming and distributed methods with constant approximation guarantees against number of adversarial deletions We extensively evaluate the performance of our algorithms against prior state on realworld applications including Uber up locations with location privacy constraints ii fairness constraints for income prediction and crime rate prediction and iii robust to deletion summarization of census data consisting of 2,458,285 feature vectors We study risksensitive imitation learning where the agents goal is to perform at least as well as the expert in terms of a risk profile We first formulate our risksensitive imitation learning setting We consider the generative adversarial approach to imitation learning GAIL and derive an optimization problem for our formulation which we call it risksensitive GAIL RSGAIL We then derive two different versions of our RSGAIL optimization problem that aim at matching the risk profiles of the agent and the expert distance and develop risksensitive generative adversarial imitation learning algorithms based on these optimization problems We evaluate the performance of our algorithms and compare them with GAIL and the risk imitation learning RAIL algorithms in two MuJoCo and two OpenAI classical control tasks With the public release of embedding models it’s important to understand the various biases that they contain Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models In this post we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias How should we decide which fairness criteria or definitions to adopt in machine learning systems? To answer this question we must study the fairness preferences of actual users of machine learn ing systems Stringent parity constraints on treat ment or impact can come with trade and may not even be preferred by the social groups in question Thus it might be beneficial to elicit what the group prefer ences are rather than rely on a priori defined mathematical fairness constraints Simply asking for self rankings of users is challenging because research has shown that there are often gaps between people stated and actual preferences paper outlines a research program and ex perimental designs for investigating these ques tions Participants in the experiments are invited to perform a set of tasks in exchange for a base payment are told upfront that they may receive a bonus later on and the bonus could de pend on some combination of output quantity and quality The same group of workers then votes on a bonus payment structure to elicit preferences The voting is hypothetical not tied to an outcome for half the group and actual tied to the actual payment outcome for the other half so that we can understand the relation between a group’s actual preferences and hypothetical stated preferences Connections and lessons from fairness in machine learning are explored Differentially Private Stochastic Gradient Descent DP forms a fundamental building block in many applications for learning over sensitive data Two standard approaches privacy amplification by subsampling and privacy amplification by shuffling permit adding lower noise in DP than via schemes A key assumption in both these approaches is that the elements in the data set can be uniformly sampled or be uniformly permuted constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion In this paper we focus on conducting iterative methods like DP in the setting of federated learning FL wherein the data is distributed among many devices clients Our main contribution is the random check distributed protocol which crucially relies only on randomized participation decisions made locally and independently by each client It has privacy accuracy trade similar to privacy amplification by subsampling However our method does not require server communication or even knowledge of the population size To our knowledge this is the first privacy amplification tailored for a distributed learning framework and it may have broader applicability beyond FL Along the way we extend privacy amplification by shuffling to incorporate local randomizers and exponentially improve its guarantees In practical regimes this improvement allows for similar privacy and utility using data from an order of magnitude fewer users In this paper we study counterfactual fairness in text classification which asks the question How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that Some people are gay is toxic while Some people are straight is nontoxic We offer a metric counterfactual token fairness CTF for measuring this particular form of fairness in text classifiers and describe its relationship with group fairness Further we offer three approaches blindness counterfactual augmentation and counterfactual logit pairing CLP for optimizing counterfactual token fairness during training bridging the robustness and fairness literature Empirically we find that blindness and CLP address counterfactual token fairness The methods do not harm classifier performance and have varying tradeoffs with group fairness These approaches both for measurement and optimization provide a new path forward for addressing fairness concerns in text classification Machine learning ML is increasingly being used in image retrieval systems for medical decision making One application of ML is to retrieve visually similar medical images from past patients eg tissue from biopsies to reference when making a medical decision with a new patient However no algorithm can perfectly capture an expert ideal notion of similarity for every case an image that is algorithmically determined to be similar may not be medically relevant to a doctors specific diagnostic needs In this paper we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm and developed tools that empower users to cope with the search algorithm onthefly communicating what types of similarity are most important at different moments in time In two evaluations with pathologists we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm The tools were preferred over a traditional interface without a loss in diagnostic accuracy We also observed that users adopted new strategies when using refinement tools repurposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors Taken together these findings inform future humanML collaborative systems for expert decisionmaking Machine learning is often viewed as an inherently valueneutral process statistical tendencies in the training inputs are simply used to generalize to new examples However when models impact social systems such as interactions between humans these patterns learned by models have normative implications It is important that we ask not only patterns exist in the data? but also how do we want our system to impact people? In particular because minority and marginalized members of society are often statistically underrepresented in data sets models may have undesirable disparate impact on such groups As such objectives of social equity and distributive justice require that we develop tools for both identifying and interpreting harms introduced by models This paper directly addresses the challenge of interpreting how human values are implicitly encoded by deep neural networks a machine learning paradigm often seen as inscrutable Doing so requires understanding how the node activations of neural networks relate to valueladen human concepts such as respectful and abusive as well as to concepts about human social identities such as gay straight male female etc To do this we present the first application of Testing with Concept Activation Vectors to models for analyzing human language Diversity including gender diversity is valued by many software development organizations yet the field remains dominated by men One reason for this lack of diversity is gender bias In this paper we study the effects of that bias by using an existing framework derived from the gender studies literature We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub then evaluate those hypotheses quantitatively While our results show that effects of gender bias are largely invisible on the GitHub platform itself there are still signals of women concentrating their work in fewer places and being more restrained in communication than men This paper describes a testing methodology for quantitatively assessing the risk of of rare or unique sequences in generative sequence models common type of neural network Such models are sometimes trained on sensitive data the text of users private messages our methodology allows deeplearning to choose configurations that minimize memorization during training thereby benefiting privacy In experiments we show that unintended memorization is a persistent hardtoavoid issue that can have serious consequences Specifically if not addressed during training we show that new efficient procedures can allow extracting unique secret sequences such as credit card numbers from trained models We also show that our testing strategy is practical and easytoapply eg by describing its use for quantitatively preventing data exposure in a production commercial neural network predictive emailcomposition assistant trained on millions of users email messages Classifiers can be trained with datadependent constraints to satisfy fairness goals reduce churn achieve a targeted positive rate or other policy goals We study the generalization performance for such constrained optimization problems in terms of how well the constraints are satisfied at evaluation time given that they are satisfied at training time To improve generalization we frame the problem as a twoplayer game where one player optimizes the model parameters on a training dataset and the other player enforces the constraints on an independent validation dataset We build on recent work in twoplayer constrained optimization to show that if one uses this twodataset approach then constraint generalization can be significantly improved As we illustrate experimentally this approach works not only in theory but also in practice The potential for learned models to amplify existing societal biases has been broadly recognized Fairness classifier constraints which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender seek to rectify inequity but can yield nonuniform degradation in performance for skewed datasets In certain domains imbalanced degradation of performance can yield another form of unintentional bias In the spirit of constructing fairness aware algorithms as societal imperative we explore an alternative ParetoEfficient Fairness PEF PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane maximizing multiple subgroup accuracies Empirically we demonstrate that PEF increases performance of all subgroups in several UCI datasets In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of blackbox machine learning models may be misplaced If we presume for the sake of this paper that machine learning can be a source of knowledge then it makes sense to wonder what kind of justification it involves How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that in general people implicitly adopt reliabilism regarding machine learning Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method We argue that in cases where model deployments require moral justification reliabilism is not sufficient and instead justifying deployment requires establishing robust human processes as a moral wrapper around machine outputs We then suggest that in certain highstakes domains with moral consequences reliabilism does not provide another kind of necessary justification moral justification Finally we offer cautions relevant to the implicit or explicit adoption of the reliabilist interpretation of machine learning We study the task of extracting covert or veiled toxicity labels from user comments Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing We introduce an initial dataset COVERTTOXICITY which aims to identify such comments from a refined rater template with rater associated categories Finally we finetune a commentdomain BERT model to classify covertly offensive comments and compare against existing baselines When collecting annotations and labeled data from humans a standard practice is to use interrater reliability IRR as a measure of data goodness Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances especially on subjective topics We present a new alternative to interpreting IRR that is more empirical and contextualized It is based upon benchmarking IRR against baseline measures in a replication one of which is a novel crossreplication reliability xRR measure based on Cohen’s 196O kappa We call this approach the xRR framework We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework We argue this framework can be used to measure the quality of crowdsourced datasets Speech samples from over 1OOO individuals with impaired speech have been submitted for Project Euphonia aimed at improving automated speech recognition for atypical speech We provide an update on the contents of the corpus which recently passed 1 million utterances and review key lessons learned from this project The reasoning behind decisions such as phrase set composition prompted vs extemporaneous speech metadata and data quality efforts are explained based on findings from both technical and userfacing research Code review is a powerful technique to ensure high quality software and spread knowledge of best coding practices between engineers Unfortunately code reviewers may have biases about authors of the code they are reviewing which can lead to inequitable experiences and outcomes In this paper we describe a field experiment with anonymous author code review where we withheld author identity information during code reviews from 3OO professional software engineers at one company Our results suggest that during anonymous author code review reviewers can frequently guess authors’ identities that focus is reduced on reviewerauthor power dynamics and that the practice poses a barrier to offline highbandwidth conversations Based on our findings we recommend that those who choose to implement anonymous author code review should reveal the time zone of the author by default have a breaktheglass option for revealing author identity and reveal author identity directly after the review Deep neural networks DNNs routinely achieve stateoftheart performance in a wide range of tasks This case study reports on the development of onboarding ie training materials for a DNNbased medical AI Assistant to aid in the grading of prostate cancer Specifically we describe how the process of developing these materials deepened the teams understanding of enduser requirements leading to changes in the development and assessment of the underlying machine learning model In this sense the onboarding materials served as a useful boundary object for a crossfunctional team We also present evidence of the utility of the subsequent onboarding materials by describing which information was found useful by participants in an experimental study Conventional algorithmic fairness is Westcentric as seen in its subgroups values and optimisations In this paper we decenter algorithmic fairness and analyse AI power in India Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India we find that several assumptions of algorithmic fairness are challenged in India We find that data is not always reliable due to socioeconomic factors users are given third world treatment by ML makers and AI signifies unquestioning aspiration We contend that localising model fairness alone can be window dressing in India where the distance between models and oppressed communities is large Instead we reimagine algorithmic fairness in India and provide a roadmap to recontextualise data and models empower oppressed communities and enable FairML ecosystems The widespread availability of cell phones has enabled nonprofits to deliver critical health information to their beneficiaries in a timely manner This paper describes our work in assisting nonprofits employing automated messaging programs to deliver timely preventive care information to new and expecting mothers during pregnancy and after delivery Unfortunately a key challenge in such information delivery programs is that a significant fraction of beneficiaries tend to drop out Yet nonprofits often have limited healthworker resources time to place crucial service calls for live interaction with beneficiaries to prevent such engagement drops To assist nonprofits in optimizing this limited resource we developed a Restless MultiArmed Bandits system One key technical contribution in this system is a novel clustering method of offline historical data to infer unknown RMAB parameters Our second major contribution is evaluation of our RMAB system in collaboration with an NGO via a realworld service quality improvement study The study compared strategies for optimizing service calls to 23OO3 participants over a period of 7 weeks to reduce engagement drops We show that the  RMAB group provides statistically significant improvement over other comparison groups reducing ∼ 3O% engagement drops To the best of our knowledge this is the first study demonstrating the utility of RMABs in real world public health settings We are transitioning our system to the NGO for realworld use We present SonicHoop an augmented aerial hoop with capacitive touch sensing and interactive sonification SonicHoop is equipped with 42 electrodes equally distributed over the hoop which detect touch events between the hoop and the performer body We add interactive sonification of the touch events with the goal of first providing auditory feedback of the movements and second transforming the aerial hoop into a digital musical instrument that can be played by the performers body We explored 3 sonification strategies ambient lounge and electro dance Structured observation with 2 professional aerial hoop performers shows that fundamentally changes their perception and choreographic processes instead of translating music into movement they search for bodily expressions to compose music Different sound designs affect their movement differently and auditory feedback regardless of types of sound improves movement quality We discuss opportunities for using SonicHoop as a creative object a pedagogical tool and a digital musical instrument as well as using interactive sonification in other acrobatic practices to explore fullbody vertical interaction As people all over the world adopt machine translation MT to communicate across languages there is increased need for affordances that aid users in understanding when to rely on automated translations Identifying the information and interactions that will most help users meet their translation needs is an open area of research at the intersection of HumanComputer Interaction HCI and Natural Language Processing NLP This paper advances work in this area by drawing on a survey of users strategies in assessing translations We identify three directions for the design of translation systems that support more reliable and effective use of machine translation helping users craft good inputs helping users understand translations and expanding interactivity and adaptivity We describe how these can be introduced in current MT systems and highlight open questions for HCI and NLP research Artificial intelligence AI offers opportunities to solve complex problems facing smallholder farmers in the Global South However there is currently a dearth of research and resources available to organizations and policymakers for building farmercentered AI systems As technologists we believe it is our responsibility to draw from and contribute to research on farmers needs practices value systems social worlds and daily agricultural ecosystem realities Drawing from our own fieldwork experience and scholarship we propose concrete future directions for building AI solutions and tools that are meaningful to farmers and will significantly improve their lives We also discuss tensions that may arise when incorporating AI into farming ecosystems We hope that a closer look into these research areas will serve as a guide for technologists looking to leverage AI to help smallholder farmers in the Global South As mobile internet growth continues to bring New Internet Users NIUs online technology has adapted to fit this user segment User barriers like devices and connectivity have declined as mobile phone prices have become more affordable and infrastructure has continued to develop connecting more communities globally App development has also evolved to better suit users on lowcost Android devices Lite apps have entered the space as a solution for users in constrained environments While there are many benefits to lite app designs their effectiveness is unclear for their likely target beneficiaries NIUs coming online In this mixedmethod study we explore the experience for NIUs trying out a smartphone with lite apps for a month in Brazil and India We conducted this research by collecting diary data and followup inperson interviews Results found that three phases of challenges occurred in the first 28 days with a lite smartphone 1 getting started with accounts 2 learning how to use the mobile platform and apps and 3 meeting expectations and mastering the internet Through understanding the friction points in each phase insights surfaced design principles for future NIU technology Machine learning is challenging the way we make music Although research in deep generative models has dramatically improved the capability and fluency of music models recent work has shown that it can be challenging for humans to partner with this new class of algorithms In this paper we present findings on what 13 musician developer teams a total of 61 users needed when cocreating a song with AI the challenges they faced and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges Many teams adopted modular approaches such as independently running multiple smaller models that align with the musical building blocks of a song before recombining their results As ML models are not easily steerable teams also generated massive numbers of samples and curated them posthoc or used a range of strategies to direct the generation or algorithmically ranked the samples Ultimately teams not only had to manage the flare and focus aspects of the creative process but also juggle that with a parallel process of exploring and curating multiple ML models and outputs These findings reflect a need to design machine learningpowered music interfaces that are more decomposable steerable interpretable and adaptive which in return will enable artists to more effectively explore how AI can extend their personal expression Wikipedia’s mission is a world in which everyone can share in the sum of all knowledge That mission has been very unevenly achieved in the first two decades of Wikipedia and one of the largest hindrances is the sheer number of languages Wikipedia needs to cover in order to achieve that goal We argue that we need a new approach to tackle this problem more effectively a multilingual Wikipedia where content can be shared between language editions This paper proposes an architecture for a system that fulfills this goal It separates the goal in two parts creating and maintaining content in an abstract notation within a project called Abstract Wikipedia and creating an infrastructure called Wikilambda that can translate this notation to natural language Both parts are fully owned and maintained by the community as is the integration of the results in the existing Wikipedia editions This architecture will make more encyclopedic content available to more people in their own language and at the same time allow more people to contribute knowledge and reach more people with their contributions no matter what their respective language backgrounds Additionally Wikilambda will unlock a new type of knowledge asset people can share in through the Wikimedia projects functions which will vastly expand what people can do with knowledge from Wikimedia and provide a new venue to collaborate and capture the creativity of contributors from all around the world These two projects will considerably expand the capabilities of the Wikimedia platform to enable every single human being to freely share share in the sum of all knowledge Headbased pointing is an alternative input method for people with motor impairments to access computing devices This paper proposes a calibration tracking input mechanism for mobile devices that makes use of the front camera that is standard on most devices To evaluate our design we performed two Fitts’ Law studies First a comparison study of our method with an existing headbased pointing solution Eva Facial Mouse with subjects without motor impairments Second we conducted what we believe is the first Fitts’ Law study using a mobile head tracker with subjects with motor impairments We extend prior studies with a greater range of index of difficulties IDs bits and achieved promising throughput average O61 bps with motor impairments and O9 bps without We found that users throughput was O95 bps on average in our most difficult task IDs 52 bits which involved selecting a target half the size of the Android recommendation for a touch target after moving nearly the full height of the screen This suggests the system is capable of fine precision tasks We summarize our observations and the lessons from our user studies into a set of design guidelines for headbased pointing systems Video summaries or highlights are a compelling alternative for exploring and contextualizing unprecedented amounts of video material However the summarization process is commonly automatic non transparent and potentially biased towards particular aspects depicted in the original video Therefore our aim is to help users like archivists or collection managers to quickly understand which summaries are the most representative for an original video In this paper we present empirical results on the utility of different types of visual explanations to achieve transparency for end users on how representative video summaries are with respect to the original video We consider four types of video summary explanations which use in different ways the concepts extracted from the original video subtitles and the video stream and their prominence The explanations are generated to meet target user preferences and express different dimensions of transparency prominence semantic coverage distance and quantity of coverage In two user studies we evaluate the utility of the visual explanations for achieving transparency for end users Our results show that explanations representing all of the dimensions have the highest utility for transparency\"\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfVkIqZpEz6a"
      },
      "source": [
        "# For reproducing same results during mutiple run\n",
        "s = 999\n",
        "np.random.seed(s)\n",
        "spacy.util.fix_random_seed(s)\n",
        "import pandas as pd\n",
        "\n",
        "# if Training with GPU also\n",
        "#CuPy.random.seed(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "citbXX44Ez6e"
      },
      "source": [
        "### Add all the new annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRBzp28bEz6k"
      },
      "source": [
        "### Training\n",
        "<a id='training'> </a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D62Up6WWe4Zo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4418f2b-899c-43d0-f61a-7e9537f2303f",
        "collapsed": true,
        "cellView": "form"
      },
      "source": [
        "#@title first training\n",
        "from spacy.training import biluo_tags_to_offsets\n",
        "\n",
        "doc = nlp(\"One of the goals of traffic engineering is to achieve a flexible tradeoff between fairness and throughput so that users are satisfied with their bandwidth allocation and the network operator is satisfied with the utilization of network resources In this paper we propose a novel way to balance the throughput and fairness objectives with linear programming It allows the network operator to precisely control the tradeoff by bounding the fairness degradation for each commodity compared to the maxmin fair solution or the throughput degradation compared to the optimal throughput We also present improvements to a previous algorithm that achieves maxmin fairness by solving a series of linear programs We significantly reduce the number of steps needed when the access rate of commodities is limited We extend the algorithm to two important practical use cases importance weights and piecewise linear utility functions for commodities Our experiments on synthetic and real networks show that our algorithms achieve a significant speedup and provide practical insights on the tradeoff between fairness and throughput The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data such as medical records or other personal information To address those concerns one promising approach is Private Aggregation of Teacher Ensembles or PATE which transfers to a student model the knowledge of an ensemble of teacher models with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers’ answers However PATE has so far been evaluated only on simple classification tasks like MNIST leaving unclear its utility when applied to largerscale learning tasks and realworld datasets In this work we show how PATE can scale to learning tasks with large numbers of output classes and uncurated imbalanced training data with errors For this we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise and prove their tighter differential guarantees Our new mechanisms build on two insights the chance of teacher consensus is increased by using more concentrated noise and lacking consensus no answer need be given to a student The consensus answers used are more likely to be correct offer better intuitive privacy and incur lower privacy cost Our evaluation shows our mechanisms improve on the original PATE on all measures and scale to larger tasks with both high utility and very strong privacy Can we efficiently extract useful information from a large user dataset while protecting the privacy of the users and ensuring fairness in representation We cast this problem as an instance of a deletion submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria We propose the first memory centralized streaming and distributed methods with constant approximation guarantees against number of adversarial deletions We extensively evaluate the performance of our algorithms against prior state on realworld applications including Uber up locations with location privacy constraints ii fairness constraints for income prediction and crime rate prediction and iii robust to deletion summarization of census data consisting of 2,458,285 feature vectors We study risksensitive imitation learning where the agents goal is to perform at least as well as the expert in terms of a risk profile We first formulate our risksensitive imitation learning setting We consider the generative adversarial approach to imitation learning GAIL and derive an optimization problem for our formulation which we call it risksensitive GAIL RSGAIL We then derive two different versions of our RSGAIL optimization problem that aim at matching the risk profiles of the agent and the expert distance and develop risksensitive generative adversarial imitation learning algorithms based on these optimization problems We evaluate the performance of our algorithms and compare them with GAIL and the risk imitation learning RAIL algorithms in two MuJoCo and two OpenAI classical control tasks With the public release of embedding models it’s important to understand the various biases that they contain Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models In this post we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias How should we decide which fairness criteria or definitions to adopt in machine learning systems? To answer this question we must study the fairness preferences of actual users of machine learn ing systems Stringent parity constraints on treat ment or impact can come with trade and may not even be preferred by the social groups in question Thus it might be beneficial to elicit what the group prefer ences are rather than rely on a priori defined mathematical fairness constraints Simply asking for self rankings of users is challenging because research has shown that there are often gaps between people stated and actual preferences paper outlines a research program and ex perimental designs for investigating these ques tions Participants in the experiments are invited to perform a set of tasks in exchange for a base payment are told upfront that they may receive a bonus later on and the bonus could de pend on some combination of output quantity and quality The same group of workers then votes on a bonus payment structure to elicit preferences The voting is hypothetical not tied to an outcome for half the group and actual tied to the actual payment outcome for the other half so that we can understand the relation between a group’s actual preferences and hypothetical stated preferences Connections and lessons from fairness in machine learning are explored Differentially Private Stochastic Gradient Descent DP forms a fundamental building block in many applications for learning over sensitive data Two standard approaches privacy amplification by subsampling and privacy amplification by shuffling permit adding lower noise in DP than via schemes A key assumption in both these approaches is that the elements in the data set can be uniformly sampled or be uniformly permuted constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion In this paper we focus on conducting iterative methods like DP in the setting of federated learning FL wherein the data is distributed among many devices clients Our main contribution is the random check distributed protocol which crucially relies only on randomized participation decisions made locally and independently by each client It has privacy accuracy trade similar to privacy amplification by subsampling However our method does not require server communication or even knowledge of the population size To our knowledge this is the first privacy amplification tailored for a distributed learning framework and it may have broader applicability beyond FL Along the way we extend privacy amplification by shuffling to incorporate local randomizers and exponentially improve its guarantees In practical regimes this improvement allows for similar privacy and utility using data from an order of magnitude fewer users In this paper we study counterfactual fairness in text classification which asks the question How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that Some people are gay is toxic while Some people are straight is nontoxic We offer a metric counterfactual token fairness CTF for measuring this particular form of fairness in text classifiers and describe its relationship with group fairness Further we offer three approaches blindness counterfactual augmentation and counterfactual logit pairing CLP for optimizing counterfactual token fairness during training bridging the robustness and fairness literature Empirically we find that blindness and CLP address counterfactual token fairness The methods do not harm classifier performance and have varying tradeoffs with group fairness These approaches both for measurement and optimization provide a new path forward for addressing fairness concerns in text classification Machine learning ML is increasingly being used in image retrieval systems for medical decision making One application of ML is to retrieve visually similar medical images from past patients eg tissue from biopsies to reference when making a medical decision with a new patient However no algorithm can perfectly capture an expert ideal notion of similarity for every case an image that is algorithmically determined to be similar may not be medically relevant to a doctors specific diagnostic needs In this paper we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm and developed tools that empower users to cope with the search algorithm onthefly communicating what types of similarity are most important at different moments in time In two evaluations with pathologists we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm The tools were preferred over a traditional interface without a loss in diagnostic accuracy We also observed that users adopted new strategies when using refinement tools repurposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors Taken together these findings inform future humanML collaborative systems for expert decisionmaking Machine learning is often viewed as an inherently valueneutral process statistical tendencies in the training inputs are simply used to generalize to new examples However when models impact social systems such as interactions between humans these patterns learned by models have normative implications It is important that we ask not only patterns exist in the data? but also how do we want our system to impact people? In particular because minority and marginalized members of society are often statistically underrepresented in data sets models may have undesirable disparate impact on such groups As such objectives of social equity and distributive justice require that we develop tools for both identifying and interpreting harms introduced by models This paper directly addresses the challenge of interpreting how human values are implicitly encoded by deep neural networks a machine learning paradigm often seen as inscrutable Doing so requires understanding how the node activations of neural networks relate to valueladen human concepts such as respectful and abusive as well as to concepts about human social identities such as gay straight male female etc To do this we present the first application of Testing with Concept Activation Vectors to models for analyzing human language Diversity including gender diversity is valued by many software development organizations yet the field remains dominated by men One reason for this lack of diversity is gender bias In this paper we study the effects of that bias by using an existing framework derived from the gender studies literature We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub then evaluate those hypotheses quantitatively While our results show that effects of gender bias are largely invisible on the GitHub platform itself there are still signals of women concentrating their work in fewer places and being more restrained in communication than men This paper describes a testing methodology for quantitatively assessing the risk of of rare or unique sequences in generative sequence models common type of neural network Such models are sometimes trained on sensitive data the text of users private messages our methodology allows deeplearning to choose configurations that minimize memorization during training thereby benefiting privacy In experiments we show that unintended memorization is a persistent hardtoavoid issue that can have serious consequences Specifically if not addressed during training we show that new efficient procedures can allow extracting unique secret sequences such as credit card numbers from trained models We also show that our testing strategy is practical and easytoapply eg by describing its use for quantitatively preventing data exposure in a production commercial neural network predictive emailcomposition assistant trained on millions of users email messages Classifiers can be trained with datadependent constraints to satisfy fairness goals reduce churn achieve a targeted positive rate or other policy goals We study the generalization performance for such constrained optimization problems in terms of how well the constraints are satisfied at evaluation time given that they are satisfied at training time To improve generalization we frame the problem as a twoplayer game where one player optimizes the model parameters on a training dataset and the other player enforces the constraints on an independent validation dataset We build on recent work in twoplayer constrained optimization to show that if one uses this twodataset approach then constraint generalization can be significantly improved As we illustrate experimentally this approach works not only in theory but also in practice The potential for learned models to amplify existing societal biases has been broadly recognized Fairness classifier constraints which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender seek to rectify inequity but can yield nonuniform degradation in performance for skewed datasets In certain domains imbalanced degradation of performance can yield another form of unintentional bias In the spirit of constructing fairness aware algorithms as societal imperative we explore an alternative ParetoEfficient Fairness PEF PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane maximizing multiple subgroup accuracies Empirically we demonstrate that PEF increases performance of all subgroups in several UCI datasets In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of blackbox machine learning models may be misplaced If we presume for the sake of this paper that machine learning can be a source of knowledge then it makes sense to wonder what kind of justification it involves How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that in general people implicitly adopt reliabilism regarding machine learning Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method We argue that in cases where model deployments require moral justification reliabilism is not sufficient and instead justifying deployment requires establishing robust human processes as a moral wrapper around machine outputs We then suggest that in certain highstakes domains with moral consequences reliabilism does not provide another kind of necessary justification moral justification Finally we offer cautions relevant to the implicit or explicit adoption of the reliabilist interpretation of machine learning We study the task of extracting covert or veiled toxicity labels from user comments Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing We introduce an initial dataset COVERTTOXICITY which aims to identify such comments from a refined rater template with rater associated categories Finally we finetune a commentdomain BERT model to classify covertly offensive comments and compare against existing baselines When collecting annotations and labeled data from humans a standard practice is to use interrater reliability IRR as a measure of data goodness Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances especially on subjective topics We present a new alternative to interpreting IRR that is more empirical and contextualized It is based upon benchmarking IRR against baseline measures in a replication one of which is a novel crossreplication reliability xRR measure based on Cohen’s 196O kappa We call this approach the xRR framework We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework We argue this framework can be used to measure the quality of crowdsourced datasets Speech samples from over 1OOO individuals with impaired speech have been submitted for Project Euphonia aimed at improving automated speech recognition for atypical speech We provide an update on the contents of the corpus which recently passed 1 million utterances and review key lessons learned from this project The reasoning behind decisions such as phrase set composition prompted vs extemporaneous speech metadata and data quality efforts are explained based on findings from both technical and userfacing research Code review is a powerful technique to ensure high quality software and spread knowledge of best coding practices between engineers Unfortunately code reviewers may have biases about authors of the code they are reviewing which can lead to inequitable experiences and outcomes In this paper we describe a field experiment with anonymous author code review where we withheld author identity information during code reviews from 3OO professional software engineers at one company Our results suggest that during anonymous author code review reviewers can frequently guess authors’ identities that focus is reduced on reviewerauthor power dynamics and that the practice poses a barrier to offline highbandwidth conversations Based on our findings we recommend that those who choose to implement anonymous author code review should reveal the time zone of the author by default have a breaktheglass option for revealing author identity and reveal author identity directly after the review Deep neural networks DNNs routinely achieve stateoftheart performance in a wide range of tasks This case study reports on the development of onboarding ie training materials for a DNNbased medical AI Assistant to aid in the grading of prostate cancer Specifically we describe how the process of developing these materials deepened the teams understanding of enduser requirements leading to changes in the development and assessment of the underlying machine learning model In this sense the onboarding materials served as a useful boundary object for a crossfunctional team We also present evidence of the utility of the subsequent onboarding materials by describing which information was found useful by participants in an experimental study Conventional algorithmic fairness is Westcentric as seen in its subgroups values and optimisations In this paper we decenter algorithmic fairness and analyse AI power in India Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India we find that several assumptions of algorithmic fairness are challenged in India We find that data is not always reliable due to socioeconomic factors users are given third world treatment by ML makers and AI signifies unquestioning aspiration We contend that localising model fairness alone can be window dressing in India where the distance between models and oppressed communities is large Instead we reimagine algorithmic fairness in India and provide a roadmap to recontextualise data and models empower oppressed communities and enable FairML ecosystems The widespread availability of cell phones has enabled nonprofits to deliver critical health information to their beneficiaries in a timely manner This paper describes our work in assisting nonprofits employing automated messaging programs to deliver timely preventive care information to new and expecting mothers during pregnancy and after delivery Unfortunately a key challenge in such information delivery programs is that a significant fraction of beneficiaries tend to drop out Yet nonprofits often have limited healthworker resources time to place crucial service calls for live interaction with beneficiaries to prevent such engagement drops To assist nonprofits in optimizing this limited resource we developed a Restless MultiArmed Bandits system One key technical contribution in this system is a novel clustering method of offline historical data to infer unknown RMAB parameters Our second major contribution is evaluation of our RMAB system in collaboration with an NGO via a realworld service quality improvement study The study compared strategies for optimizing service calls to 23OO3 participants over a period of 7 weeks to reduce engagement drops We show that the  RMAB group provides statistically significant improvement over other comparison groups reducing ∼ 3O% engagement drops To the best of our knowledge this is the first study demonstrating the utility of RMABs in real world public health settings We are transitioning our system to the NGO for realworld use We present SonicHoop an augmented aerial hoop with capacitive touch sensing and interactive sonification SonicHoop is equipped with 42 electrodes equally distributed over the hoop which detect touch events between the hoop and the performer body We add interactive sonification of the touch events with the goal of first providing auditory feedback of the movements and second transforming the aerial hoop into a digital musical instrument that can be played by the performers body We explored 3 sonification strategies ambient lounge and electro dance Structured observation with 2 professional aerial hoop performers shows that fundamentally changes their perception and choreographic processes instead of translating music into movement they search for bodily expressions to compose music Different sound designs affect their movement differently and auditory feedback regardless of types of sound improves movement quality We discuss opportunities for using SonicHoop as a creative object a pedagogical tool and a digital musical instrument as well as using interactive sonification in other acrobatic practices to explore fullbody vertical interaction As people all over the world adopt machine translation MT to communicate across languages there is increased need for affordances that aid users in understanding when to rely on automated translations Identifying the information and interactions that will most help users meet their translation needs is an open area of research at the intersection of HumanComputer Interaction HCI and Natural Language Processing NLP This paper advances work in this area by drawing on a survey of users strategies in assessing translations We identify three directions for the design of translation systems that support more reliable and effective use of machine translation helping users craft good inputs helping users understand translations and expanding interactivity and adaptivity We describe how these can be introduced in current MT systems and highlight open questions for HCI and NLP research Artificial intelligence AI offers opportunities to solve complex problems facing smallholder farmers in the Global South However there is currently a dearth of research and resources available to organizations and policymakers for building farmercentered AI systems As technologists we believe it is our responsibility to draw from and contribute to research on farmers needs practices value systems social worlds and daily agricultural ecosystem realities Drawing from our own fieldwork experience and scholarship we propose concrete future directions for building AI solutions and tools that are meaningful to farmers and will significantly improve their lives We also discuss tensions that may arise when incorporating AI into farming ecosystems We hope that a closer look into these research areas will serve as a guide for technologists looking to leverage AI to help smallholder farmers in the Global South As mobile internet growth continues to bring New Internet Users NIUs online technology has adapted to fit this user segment User barriers like devices and connectivity have declined as mobile phone prices have become more affordable and infrastructure has continued to develop connecting more communities globally App development has also evolved to better suit users on lowcost Android devices Lite apps have entered the space as a solution for users in constrained environments While there are many benefits to lite app designs their effectiveness is unclear for their likely target beneficiaries NIUs coming online In this mixedmethod study we explore the experience for NIUs trying out a smartphone with lite apps for a month in Brazil and India We conducted this research by collecting diary data and followup inperson interviews Results found that three phases of challenges occurred in the first 28 days with a lite smartphone 1 getting started with accounts 2 learning how to use the mobile platform and apps and 3 meeting expectations and mastering the internet Through understanding the friction points in each phase insights surfaced design principles for future NIU technology Machine learning is challenging the way we make music Although research in deep generative models has dramatically improved the capability and fluency of music models recent work has shown that it can be challenging for humans to partner with this new class of algorithms In this paper we present findings on what 13 musician developer teams a total of 61 users needed when cocreating a song with AI the challenges they faced and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges Many teams adopted modular approaches such as independently running multiple smaller models that align with the musical building blocks of a song before recombining their results As ML models are not easily steerable teams also generated massive numbers of samples and curated them posthoc or used a range of strategies to direct the generation or algorithmically ranked the samples Ultimately teams not only had to manage the flare and focus aspects of the creative process but also juggle that with a parallel process of exploring and curating multiple ML models and outputs These findings reflect a need to design machine learningpowered music interfaces that are more decomposable steerable interpretable and adaptive which in return will enable artists to more effectively explore how AI can extend their personal expression Wikipedia’s mission is a world in which everyone can share in the sum of all knowledge That mission has been very unevenly achieved in the first two decades of Wikipedia and one of the largest hindrances is the sheer number of languages Wikipedia needs to cover in order to achieve that goal We argue that we need a new approach to tackle this problem more effectively a multilingual Wikipedia where content can be shared between language editions This paper proposes an architecture for a system that fulfills this goal It separates the goal in two parts creating and maintaining content in an abstract notation within a project called Abstract Wikipedia and creating an infrastructure called Wikilambda that can translate this notation to natural language Both parts are fully owned and maintained by the community as is the integration of the results in the existing Wikipedia editions This architecture will make more encyclopedic content available to more people in their own language and at the same time allow more people to contribute knowledge and reach more people with their contributions no matter what their respective language backgrounds Additionally Wikilambda will unlock a new type of knowledge asset people can share in through the Wikimedia projects functions which will vastly expand what people can do with knowledge from Wikimedia and provide a new venue to collaborate and capture the creativity of contributors from all around the world These two projects will considerably expand the capabilities of the Wikimedia platform to enable every single human being to freely share share in the sum of all knowledge Headbased pointing is an alternative input method for people with motor impairments to access computing devices This paper proposes a calibration tracking input mechanism for mobile devices that makes use of the front camera that is standard on most devices To evaluate our design we performed two Fitts’ Law studies First a comparison study of our method with an existing headbased pointing solution Eva Facial Mouse with subjects without motor impairments Second we conducted what we believe is the first Fitts’ Law study using a mobile head tracker with subjects with motor impairments We extend prior studies with a greater range of index of difficulties IDs bits and achieved promising throughput average O61 bps with motor impairments and O9 bps without We found that users throughput was O95 bps on average in our most difficult task IDs 52 bits which involved selecting a target half the size of the Android recommendation for a touch target after moving nearly the full height of the screen This suggests the system is capable of fine precision tasks We summarize our observations and the lessons from our user studies into a set of design guidelines for headbased pointing systems Video summaries or highlights are a compelling alternative for exploring and contextualizing unprecedented amounts of video material However the summarization process is commonly automatic non transparent and potentially biased towards particular aspects depicted in the original video Therefore our aim is to help users like archivists or collection managers to quickly understand which summaries are the most representative for an original video In this paper we present empirical results on the utility of different types of visual explanations to achieve transparency for end users on how representative video summaries are with respect to the original video We consider four types of video summary explanations which use in different ways the concepts extracted from the original video subtitles and the video stream and their prominence The explanations are generated to meet target user preferences and express different dimensions of transparency prominence semantic coverage distance and quantity of coverage In two user studies we evaluate the utility of the visual explanations for achieving transparency for end users Our results show that explanations representing all of the dimensions have the highest utility for transparency\")\n",
        "tags = [\"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-SUCCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-SUCCESS\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-PROCESS\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"U-IDENTITY\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-PROCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-IDENTITY\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-IDENTITY\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-IDENTITY\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-PROCESS\", \"U-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-PROCESS\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"B-IDENTITY\", \"B-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-SUCCESS\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"U-SUCCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"U-IDENTITY\", \"U-IDENTITY\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-PROCESS\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"B-MEASURE\", \"L-MEASURE\", \"U-SUCCESS\", \"U-MEASURE\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-SUCCESS\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"I-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"I-PROCESS\", \"L-PROCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"U-SUCCESS\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-IDENTITY\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"O\", \"U-PROCESS\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"I-IDENTITY\", \"L-IDENTITY\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"U-IDENTITY\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-IDENTITY\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"U-MEASURE\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"U-MEASURE\", \"U-MEASURE\", \"U-MEASURE\", \"U-MEASURE\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-SUCCESS\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"I-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"I-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"U-SUCCESS\", \"U-SUCCESS\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-IDENTITY\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"B-PROCESS\", \"I-PROCESS\", \"L-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-PROCESS\", \"U-MEASURE\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"B-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"U-PROCESS\", \"O\", \"U-PROCESS\", \"O\", \"U-MEASURE\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SUCCESS\", \"I-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"B-MEASURE\", \"I-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"B-SUCCESS\", \"L-SUCCESS\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-IDENTITY\", \"O\", \"U-IDENTITY\", \"O\", \"B-IDENTITY\", \"L-IDENTITY\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"U-MEASURE\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"U-IDENTITY\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"B-MEASURE\", \"L-MEASURE\", \"U-MEASURE\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"O\", \"U-IDENTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PROCESS\", \"O\", \"O\", \"O\", \"U-MEASURE\", \"O\", \"O\", \"U-SUCCESS\", \"U-MEASURE\", \"O\", \"U-MEASURE\"] \n",
        "entities = biluo_tags_to_offsets(doc, tags)\n",
        "print(entities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53, 'SUCCESS'), (65, 73, 'MEASURE'), (82, 90, 'MEASURE'), (95, 105, 'MEASURE'), (114, 119, 'IDENTITY'), (124, 133, 'SUCCESS'), (194, 203, 'SUCCESS'), (236, 245, 'MEASURE'), (273, 278, 'SUCCESS'), (286, 293, 'MEASURE'), (298, 308, 'MEASURE'), (313, 321, 'MEASURE'), (322, 332, 'MEASURE'), (338, 356, 'PROCESS'), (360, 366, 'SUCCESS'), (391, 400, 'SUCCESS'), (401, 408, 'PROCESS'), (438, 446, 'MEASURE'), (468, 477, 'MEASURE'), (501, 505, 'SUCCESS'), (561, 568, 'SUCCESS'), (596, 608, 'SUCCESS'), (638, 646, 'SUCCESS'), (654, 662, 'MEASURE'), (666, 673, 'SUCCESS'), (719, 725, 'MEASURE'), (803, 809, 'PROCESS'), (831, 840, 'SUCCESS'), (841, 850, 'MEASURE'), (909, 918, 'MEASURE'), (939, 950, 'PROCESS'), (996, 1006, 'PROCESS'), (1007, 1014, 'SUCCESS'), (1029, 1036, 'SUCCESS'), (1059, 1067, 'SUCCESS'), (1120, 1125, 'SUCCESS'), (1126, 1134, 'PROCESS'), (1159, 1168, 'SUCCESS'), (1188, 1195, 'MEASURE'), (1229, 1235, 'MEASURE'), (1247, 1256, 'MEASURE'), (1270, 1285, 'MEASURE'), (1346, 1355, 'SUCCESS'), (1376, 1387, 'PROCESS'), (1391, 1398, 'IDENTITY'), (1423, 1432, 'PROCESS'), (1438, 1445, 'IDENTITY'), (1446, 1451, 'MEASURE'), (1456, 1465, 'MEASURE'), (1484, 1491, 'IDENTITY'), (1492, 1498, 'MEASURE'), (1504, 1513, 'SUCCESS'), (1534, 1542, 'PROCESS'), (1543, 1551, 'IDENTITY'), (1573, 1579, 'SUCCESS'), (1588, 1598, 'SUCCESS'), (1727, 1734, 'MEASURE'), (1743, 1746, 'SUCCESS'), (1755, 1759, 'PROCESS'), (1846, 1854, 'PROCESS'), (1861, 1872, 'PROCESS'), (1979, 1988, 'SUCCESS'), (2022, 2025, 'IDENTITY'), (2049, 2057, 'SUCCESS'), (2068, 2071, 'PROCESS'), (2087, 2090, 'SUCCESS'), (2111, 2123, 'SUCCESS'), (2135, 2138, 'SUCCESS'), (2163, 2166, 'SUCCESS'), (2187, 2189, 'IDENTITY'), (2208, 2210, 'SUCCESS'), (2303, 2304, 'IDENTITY'), (2340, 2348, 'SUCCESS'), (2362, 2369, 'SUCCESS'), (2370, 2375, 'SUCCESS'), (2376, 2382, 'SUCCESS'), (2455, 2465, 'SUCCESS'), (2521, 2523, 'SUCCESS'), (2542, 2551, 'SUCCESS'), (2560, 2568, 'SUCCESS'), (2588, 2590, 'SUCCESS'), (2591, 2602, 'PROCESS'), (2603, 2610, 'SUCCESS'), (2656, 2661, 'SUCCESS'), (2688, 2691, 'IDENTITY'), (2698, 2710, 'SUCCESS'), (2720, 2722, 'SUCCESS'), (2906, 2909, 'SUCCESS'), (2923, 2934, 'PROCESS'), (2983, 2996, 'SUCCESS'), (3051, 3062, 'PROCESS'), (3072, 3075, 'MEASURE'), (3091, 3094, 'PROCESS'), (3129, 3138, 'PROCESS'), (3214, 3216, 'SUCCESS'), (3238, 3241, 'MEASURE'), (3242, 3248, 'MEASURE'), (3264, 3269, 'MEASURE'), (3270, 3274, 'MEASURE'), (3290, 3293, 'SUCCESS'), (3304, 3312, 'PROCESS'), (3391, 3414, 'PROCESS'), (3434, 3440, 'MEASURE'), (3449, 3451, 'PROCESS'), (3469, 3471, 'SUCCESS'), (3480, 3483, 'SUCCESS'), (3521, 3526, 'PROCESS'), (3541, 3564, 'PROCESS'), (3657, 3660, 'PROCESS'), (3668, 3670, 'PROCESS'), (3849, 3852, 'MEASURE'), (3904, 3907, 'MEASURE'), (3953, 3971, 'PROCESS'), (4032, 4035, 'SUCCESS'), (4125, 4129, 'PROCESS'), (4199, 4202, 'IDENTITY'), (4218, 4220, 'PROCESS'), (4238, 4240, 'SUCCESS'), (4267, 4270, 'MEASURE'), (4291, 4295, 'IDENTITY'), (4319, 4322, 'SUCCESS'), (4396, 4402, 'PROCESS'), (4468, 4472, 'IDENTITY'), (4473, 4477, 'PROCESS'), (4504, 4509, 'MEASURE'), (4518, 4521, 'PROCESS'), (4554, 4558, 'SUCCESS'), (4573, 4577, 'SUCCESS'), (4578, 4581, 'IDENTITY'), (4592, 4598, 'MEASURE'), (4626, 4637, 'PROCESS'), (4679, 4685, 'IDENTITY'), (4703, 4707, 'MEASURE'), (4727, 4738, 'IDENTITY'), (4772, 4775, 'MEASURE'), (4776, 4783, 'SUCCESS'), (4816, 4821, 'MEASURE'), (4877, 4889, 'IDENTITY'), (4925, 4927, 'SUCCESS'), (4934, 4936, 'PROCESS'), (4951, 4957, 'IDENTITY'), (5012, 5018, 'MEASURE'), (5068, 5074, 'MEASURE'), (5079, 5083, 'IDENTITY'), (5159, 5164, 'IDENTITY'), (5185, 5191, 'MEASURE'), (5196, 5202, 'PROCESS'), (5221, 5229, 'PROCESS'), (5256, 5266, 'PROCESS'), (5293, 5298, 'IDENTITY'), (5342, 5345, 'PROCESS'), (5392, 5395, 'MEASURE'), (5415, 5419, 'IDENTITY'), (5428, 5432, 'MEASURE'), (5550, 5554, 'IDENTITY'), (5594, 5601, 'PROCESS'), (5602, 5611, 'MEASURE'), (5665, 5669, 'MEASURE'), (5771, 5775, 'PROCESS'), (5798, 5806, 'IDENTITY'), (5815, 5816, 'MEASURE'), (5832, 5843, 'MEASURE'), (5844, 5847, 'SUCCESS'), (5861, 5867, 'SUCCESS'), (5880, 5891, 'MEASURE'), (5918, 5920, 'SUCCESS'), (5994, 6001, 'SUCCESS'), (6040, 6042, 'PROCESS'), (6048, 6060, 'MEASURE'), (6094, 6097, 'SUCCESS'), (6107, 6117, 'PROCESS'), (6140, 6142, 'PROCESS'), (6226, 6230, 'SUCCESS'), (6318, 6321, 'PROCESS'), (6329, 6346, 'PROCESS'), (6412, 6416, 'PROCESS'), (6489, 6494, 'PROCESS'), (6584, 6591, 'PROCESS'), (6604, 6615, 'IDENTITY'), (6679, 6684, 'SUCCESS'), (6712, 6721, 'MEASURE'), (6737, 6747, 'MEASURE'), (6762, 6771, 'MEASURE'), (6785, 6788, 'IDENTITY'), (6806, 6810, 'MEASURE'), (6811, 6817, 'MEASURE'), (6833, 6841, 'MEASURE'), (6842, 6847, 'PROCESS'), (6932, 6938, 'MEASURE'), (6956, 6960, 'IDENTITY'), (6978, 6988, 'IDENTITY'), (7001, 7010, 'SUCCESS'), (7019, 7022, 'SUCCESS'), (7060, 7063, 'MEASURE'), (7097, 7103, 'SUCCESS'), (7145, 7150, 'PROCESS'), (7191, 7193, 'IDENTITY'), (7194, 7203, 'PROCESS'), (7219, 7224, 'SUCCESS'), (7237, 7240, 'SUCCESS'), (7278, 7280, 'SUCCESS'), (7304, 7315, 'MEASURE'), (7323, 7326, 'MEASURE'), (7366, 7370, 'MEASURE'), (7374, 7379, 'IDENTITY'), (7399, 7404, 'IDENTITY'), (7413, 7418, 'MEASURE'), (7428, 7442, 'PROCESS'), (7513, 7523, 'MEASURE'), (7572, 7575, 'MEASURE'), (7600, 7608, 'MEASURE'), (7633, 7634, 'PROCESS'), (7659, 7664, 'IDENTITY'), (7668, 7678, 'IDENTITY'), (7684, 7688, 'MEASURE'), (7700, 7703, 'IDENTITY'), (7707, 7712, 'IDENTITY'), (7719, 7723, 'MEASURE'), (7759, 7764, 'MEASURE'), (7812, 7821, 'MEASURE'), (7846, 7854, 'PROCESS'), (7875, 7878, 'IDENTITY'), (7925, 7932, 'MEASURE'), (8001, 8015, 'PROCESS'), (8030, 8033, 'MEASURE'), (8049, 8063, 'SUCCESS'), (8070, 8078, 'MEASURE'), (8108, 8118, 'IDENTITY'), (8158, 8162, 'PROCESS'), (8178, 8181, 'MEASURE'), (8228, 8235, 'SUCCESS'), (8271, 8274, 'IDENTITY'), (8275, 8279, 'MEASURE'), (8288, 8297, 'PROCESS'), (8324, 8334, 'SUCCESS'), (8344, 8355, 'SUCCESS'), (8360, 8372, 'SUCCESS'), (8383, 8386, 'MEASURE'), (8503, 8505, 'MEASURE'), (8562, 8573, 'PROCESS'), (8577, 8579, 'MEASURE'), (8604, 8611, 'IDENTITY'), (8646, 8648, 'PROCESS'), (8670, 8682, 'MEASURE'), (8697, 8704, 'IDENTITY'), (8733, 8740, 'PROCESS'), (8744, 8753, 'IDENTITY'), (8776, 8778, 'MEASURE'), (8842, 8844, 'MEASURE'), (8878, 8885, 'MEASURE'), (8894, 8896, 'IDENTITY'), (8916, 8918, 'MEASURE'), (8958, 8962, 'MEASURE'), (8969, 8971, 'IDENTITY'), (9043, 9052, 'PROCESS'), (9075, 9084, 'SUCCESS'), (9085, 9088, 'IDENTITY'), (9099, 9104, 'SUCCESS'), (9132, 9136, 'PROCESS'), (9158, 9166, 'MEASURE'), (9181, 9191, 'SUCCESS'), (9238, 9245, 'PROCESS'), (9249, 9253, 'IDENTITY'), (9278, 9290, 'SUCCESS'), (9291, 9293, 'PROCESS'), (9294, 9299, 'SUCCESS'), (9311, 9321, 'MEASURE'), (9353, 9360, 'SUCCESS'), (9361, 9363, 'IDENTITY'), (9364, 9370, 'MEASURE'), (9473, 9480, 'MEASURE'), (9502, 9510, 'IDENTITY'), (9511, 9513, 'PROCESS'), (9514, 9518, 'SUCCESS'), (9551, 9561, 'PROCESS'), (9645, 9654, 'MEASURE'), (9675, 9677, 'MEASURE'), (9700, 9706, 'SUCCESS'), (9707, 9712, 'MEASURE'), (9744, 9750, 'MEASURE'), (9827, 9832, 'MEASURE'), (9843, 9845, 'MEASURE'), (9917, 9923, 'PROCESS'), (9957, 9960, 'PROCESS'), (10004, 10011, 'IDENTITY'), (10054, 10062, 'MEASURE'), (10081, 10085, 'MEASURE'), (10125, 10129, 'MEASURE'), (10168, 10172, 'IDENTITY'), (10174, 10177, 'IDENTITY'), (10178, 10182, 'MEASURE'), (10187, 10189, 'SUCCESS'), (10190, 10192, 'IDENTITY'), (10209, 10211, 'IDENTITY'), (10219, 10226, 'IDENTITY'), (10230, 10240, 'IDENTITY'), (10262, 10274, 'MEASURE'), (10318, 10334, 'MEASURE'), (10335, 10337, 'MEASURE'), (10338, 10342, 'SUCCESS'), (10355, 10358, 'IDENTITY'), (10393, 10400, 'MEASURE'), (10408, 10415, 'MEASURE'), (10437, 10443, 'PROCESS'), (10469, 10476, 'PROCESS'), (10482, 10484, 'PROCESS'), (10485, 10492, 'MEASURE'), (10537, 10542, 'PROCESS'), (10575, 10583, 'MEASURE'), (10584, 10593, 'MEASURE'), (10628, 10640, 'MEASURE'), (10688, 10689, 'MEASURE'), (10716, 10721, 'SUCCESS'), (10782, 10786, 'MEASURE'), (10787, 10798, 'IDENTITY'), (10799, 10801, 'MEASURE'), (10818, 10824, 'MEASURE'), (10828, 10838, 'MEASURE'), (10877, 10884, 'IDENTITY'), (10885, 10892, 'MEASURE'), (10899, 10907, 'IDENTITY'), (10908, 10913, 'IDENTITY'), (10914, 10919, 'IDENTITY'), (10920, 10926, 'IDENTITY'), (11030, 11034, 'PROCESS'), (11035, 11042, 'MEASURE'), (11043, 11053, 'MEASURE'), (11054, 11061, 'MEASURE'), (11065, 11071, 'MEASURE'), (11072, 11075, 'MEASURE'), (11121, 11127, 'MEASURE'), (11128, 11137, 'IDENTITY'), (11177, 11190, 'IDENTITY'), (11226, 11229, 'MEASURE'), (11234, 11240, 'IDENTITY'), (11241, 11244, 'MEASURE'), (11268, 11270, 'PROCESS'), (11291, 11296, 'MEASURE'), (11318, 11320, 'MEASURE'), (11334, 11339, 'IDENTITY'), (11362, 11369, 'PROCESS'), (11414, 11417, 'MEASURE'), (11462, 11464, 'SUCCESS'), (11489, 11492, 'PROCESS'), (11569, 11574, 'IDENTITY'), (11575, 11578, 'MEASURE'), (11592, 11596, 'MEASURE'), (11645, 11648, 'MEASURE'), (11656, 11664, 'IDENTITY'), (11725, 11729, 'MEASURE'), (11733, 11738, 'MEASURE'), (11746, 11749, 'IDENTITY'), (11775, 11788, 'PROCESS'), (11803, 11808, 'PROCESS'), (11819, 11820, 'MEASURE'), (11841, 11844, 'MEASURE'), (11860, 11869, 'MEASURE'), (11870, 11873, 'MEASURE'), (11910, 11923, 'MEASURE'), (11962, 11974, 'MEASURE'), (11996, 12003, 'IDENTITY'), (12031, 12033, 'PROCESS'), (12061, 12079, 'SUCCESS'), (12103, 12117, 'SUCCESS'), (12118, 12122, 'MEASURE'), (12203, 12205, 'MEASURE'), (12243, 12244, 'MEASURE'), (12329, 12338, 'SUCCESS'), (12339, 12345, 'PROCESS'), (12358, 12362, 'PROCESS'), (12403, 12420, 'MEASURE'), (12489, 12493, 'MEASURE'), (12516, 12524, 'PROCESS'), (12554, 12556, 'SUCCESS'), (12560, 12570, 'MEASURE'), (12614, 12622, 'MEASURE'), (12665, 12675, 'IDENTITY'), (12676, 12702, 'MEASURE'), (12759, 12762, 'SUCCESS'), (12763, 12773, 'MEASURE'), (12774, 12778, 'SUCCESS'), (12779, 12792, 'MEASURE'), (12793, 12804, 'SUCCESS'), (12816, 12824, 'SUCCESS'), (12825, 12830, 'MEASURE'), (12844, 12851, 'MEASURE'), (12880, 12885, 'SUCCESS'), (12948, 12959, 'SUCCESS'), (12985, 12990, 'SUCCESS'), (12994, 12997, 'PROCESS'), (13033, 13035, 'SUCCESS'), (13063, 13067, 'SUCCESS'), (13068, 13071, 'MEASURE'), (13082, 13084, 'PROCESS'), (13138, 13145, 'IDENTITY'), (13146, 13148, 'SUCCESS'), (13214, 13216, 'IDENTITY'), (13217, 13218, 'PROCESS'), (13228, 13235, 'MEASURE'), (13244, 13249, 'MEASURE'), (13322, 13327, 'SUCCESS'), (13411, 13430, 'SUCCESS'), (13509, 13523, 'MEASURE'), (13553, 13555, 'SUCCESS'), (13575, 13583, 'PROCESS'), (13588, 13601, 'MEASURE'), (13628, 13636, 'MEASURE'), (13662, 13669, 'MEASURE'), (13690, 13700, 'MEASURE'), (13713, 13718, 'IDENTITY'), (13734, 13744, 'MEASURE'), (13764, 13773, 'MEASURE'), (13782, 13784, 'MEASURE'), (13806, 13810, 'PROCESS'), (13811, 13813, 'MEASURE'), (13846, 13854, 'PROCESS'), (13892, 13894, 'MEASURE'), (13969, 13971, 'MEASURE'), (13972, 13983, 'MEASURE'), (14010, 14023, 'MEASURE'), (14024, 14028, 'MEASURE'), (14029, 14031, 'PROCESS'), (14036, 14042, 'MEASURE'), (14043, 14045, 'SUCCESS'), (14088, 14096, 'MEASURE'), (14111, 14118, 'PROCESS'), (14163, 14177, 'MEASURE'), (14182, 14191, 'IDENTITY'), (14192, 14197, 'MEASURE'), (14212, 14217, 'MEASURE'), (14221, 14229, 'SUCCESS'), (14243, 14250, 'IDENTITY'), (14251, 14253, 'MEASURE'), (14289, 14297, 'MEASURE'), (14298, 14317, 'SUCCESS'), (14333, 14344, 'IDENTITY'), (14354, 14375, 'MEASURE'), (14408, 14419, 'MEASURE'), (14425, 14430, 'MEASURE'), (14454, 14463, 'MEASURE'), (14534, 14541, 'IDENTITY'), (14610, 14628, 'SUCCESS'), (14654, 14656, 'PROCESS'), (14675, 14680, 'MEASURE'), (14702, 14706, 'IDENTITY'), (14707, 14709, 'PROCESS'), (14746, 14757, 'MEASURE'), (14778, 14785, 'MEASURE'), (14851, 14858, 'IDENTITY'), (14859, 14867, 'MEASURE'), (14869, 14871, 'MEASURE'), (14886, 14893, 'MEASURE'), (14912, 14929, 'MEASURE'), (14975, 14997, 'MEASURE'), (15001, 15010, 'MEASURE'), (15056, 15068, 'MEASURE'), (15125, 15135, 'MEASURE'), (15136, 15138, 'MEASURE'), (15145, 15156, 'MEASURE'), (15209, 15211, 'PROCESS'), (15212, 15230, 'SUCCESS'), (15250, 15260, 'MEASURE'), (15351, 15363, 'MEASURE'), (15364, 15368, 'MEASURE'), (15410, 15422, 'SUCCESS'), (15435, 15439, 'MEASURE'), (15523, 15528, 'PROCESS'), (15618, 15620, 'PROCESS'), (15621, 15628, 'MEASURE'), (15638, 15646, 'MEASURE'), (15656, 15658, 'IDENTITY'), (15757, 15767, 'MEASURE'), (15780, 15788, 'MEASURE'), (15836, 15852, 'PROCESS'), (15872, 15879, 'SUCCESS'), (15891, 15894, 'MEASURE'), (15971, 15980, 'PROCESS'), (16123, 16136, 'MEASURE'), (16231, 16240, 'IDENTITY'), (16246, 16268, 'PROCESS'), (16291, 16297, 'MEASURE'), (16324, 16338, 'SUCCESS'), (16358, 16367, 'MEASURE'), (16371, 16384, 'MEASURE'), (16462, 16464, 'MEASURE'), (16486, 16488, 'IDENTITY'), (16504, 16514, 'MEASURE'), (16532, 16535, 'MEASURE'), (16554, 16569, 'MEASURE'), (16589, 16601, 'SUCCESS'), (16612, 16622, 'PROCESS'), (16623, 16625, 'MEASURE'), (16644, 16656, 'SUCCESS'), (16657, 16660, 'SUCCESS'), (16693, 16697, 'PROCESS'), (16698, 16700, 'MEASURE'), (16786, 16794, 'PROCESS'), (16819, 16829, 'MEASURE'), (16877, 16885, 'MEASURE'), (16927, 16930, 'IDENTITY'), (16931, 16934, 'MEASURE'), (16945, 16947, 'MEASURE'), (16948, 16958, 'MEASURE'), (17033, 17047, 'SUCCESS'), (17066, 17075, 'MEASURE'), (17079, 17089, 'MEASURE'), (17090, 17103, 'MEASURE'), (17115, 17122, 'IDENTITY'), (17202, 17206, 'SUCCESS'), (17233, 17246, 'MEASURE'), (17270, 17272, 'SUCCESS'), (17369, 17375, 'SUCCESS'), (17426, 17429, 'MEASURE'), (17441, 17448, 'MEASURE'), (17449, 17456, 'MEASURE'), (17457, 17461, 'MEASURE'), (17605, 17618, 'PROCESS'), (17647, 17656, 'SUCCESS'), (17657, 17680, 'SUCCESS'), (17686, 17695, 'SUCCESS'), (17698, 17706, 'SUCCESS'), (17770, 17772, 'MEASURE'), (17778, 17784, 'IDENTITY'), (17864, 17874, 'MEASURE'), (17921, 17932, 'PROCESS'), (17945, 17957, 'IDENTITY'), (18037, 18054, 'IDENTITY'), (18071, 18082, 'IDENTITY'), (18125, 18133, 'IDENTITY'), (18147, 18150, 'IDENTITY'), (18191, 18212, 'SUCCESS'), (18293, 18316, 'SUCCESS'), (18356, 18363, 'IDENTITY'), (18403, 18408, 'IDENTITY'), (18425, 18427, 'SUCCESS'), (18463, 18472, 'IDENTITY'), (18525, 18529, 'IDENTITY'), (18576, 18582, 'MEASURE'), (18587, 18603, 'MEASURE'), (18617, 18639, 'SUCCESS'), (18736, 18738, 'PROCESS'), (18741, 18745, 'MEASURE'), (18777, 18791, 'PROCESS'), (18804, 18806, 'SUCCESS'), (18821, 18843, 'MEASURE'), (18844, 18845, 'MEASURE'), (18901, 18909, 'SUCCESS'), (18930, 18932, 'MEASURE'), (18988, 18996, 'PROCESS'), (19001, 19006, 'PROCESS'), (19053, 19055, 'MEASURE'), (19116, 19123, 'SUCCESS'), (19124, 19138, 'MEASURE'), (19197, 19205, 'SUCCESS'), (19272, 19279, 'MEASURE'), (19283, 19286, 'IDENTITY'), (19309, 19321, 'PROCESS'), (19339, 19350, 'MEASURE'), (19355, 19360, 'IDENTITY'), (19387, 19389, 'IDENTITY'), (19390, 19402, 'MEASURE'), (19409, 19421, 'MEASURE'), (19458, 19460, 'PROCESS'), (19466, 19468, 'MEASURE'), (19567, 19569, 'PROCESS'), (19576, 19584, 'MEASURE'), (19636, 19647, 'MEASURE'), (19729, 19737, 'MEASURE'), (19753, 19761, 'MEASURE'), (19762, 19764, 'IDENTITY'), (19783, 19786, 'MEASURE'), (19794, 19806, 'IDENTITY'), (19860, 19869, 'PROCESS'), (19873, 19875, 'MEASURE'), (19883, 19899, 'SUCCESS'), (19900, 19913, 'SUCCESS'), (19973, 19979, 'IDENTITY'), (20005, 20010, 'PROCESS'), (20011, 20023, 'MEASURE'), (20043, 20052, 'SUCCESS'), (20074, 20081, 'SUCCESS'), (20107, 20115, 'SUCCESS'), (20116, 20124, 'IDENTITY'), (20129, 20136, 'SUCCESS'), (20137, 20146, 'MEASURE'), (20200, 20211, 'SUCCESS'), (20223, 20229, 'SUCCESS'), (20230, 20255, 'MEASURE'), (20272, 20276, 'IDENTITY'), (20349, 20354, 'SUCCESS'), (20355, 20368, 'IDENTITY'), (20369, 20371, 'PROCESS'), (20399, 20408, 'MEASURE'), (20409, 20412, 'MEASURE'), (20413, 20420, 'MEASURE'), (20452, 20471, 'IDENTITY'), (20556, 20572, 'PROCESS'), (20612, 20621, 'IDENTITY'), (20660, 20662, 'IDENTITY'), (20691, 20693, 'MEASURE'), (20759, 20771, 'IDENTITY'), (20782, 20795, 'SUCCESS'), (20796, 20803, 'SUCCESS'), (20812, 20817, 'SUCCESS'), (20827, 20838, 'SUCCESS'), (20885, 20900, 'MEASURE'), (20901, 20911, 'MEASURE'), (20972, 20982, 'SUCCESS'), (21002, 21028, 'MEASURE'), (21032, 21036, 'PROCESS'), (21044, 21048, 'MEASURE'), (21084, 21094, 'PROCESS'), (21144, 21149, 'IDENTITY'), (21180, 21188, 'SUCCESS'), (21225, 21228, 'PROCESS'), (21229, 21240, 'MEASURE'), (21257, 21268, 'IDENTITY'), (21328, 21336, 'MEASURE'), (21373, 21374, 'IDENTITY'), (21375, 21390, 'SUCCESS'), (21391, 21401, 'SUCCESS'), (21472, 21483, 'SUCCESS'), (21524, 21527, 'SUCCESS'), (21548, 21551, 'SUCCESS'), (21557, 21559, 'MEASURE'), (21636, 21640, 'IDENTITY'), (21647, 21660, 'PROCESS'), (21713, 21726, 'MEASURE'), (21836, 21845, 'PROCESS'), (21846, 21857, 'MEASURE'), (21897, 21901, 'IDENTITY'), (21924, 21929, 'PROCESS'), (21945, 21953, 'MEASURE'), (21984, 22008, 'MEASURE'), (22016, 22021, 'MEASURE'), (22097, 22120, 'SUCCESS'), (22132, 22136, 'IDENTITY'), (22160, 22175, 'MEASURE'), (22225, 22250, 'IDENTITY'), (22251, 22258, 'IDENTITY'), (22295, 22306, 'MEASURE'), (22312, 22326, 'MEASURE'), (22476, 22482, 'MEASURE'), (22494, 22508, 'MEASURE'), (22547, 22553, 'SUCCESS'), (22554, 22568, 'MEASURE'), (22585, 22593, 'SUCCESS'), (22626, 22640, 'MEASURE'), (22724, 22737, 'PROCESS'), (22765, 22783, 'MEASURE'), (22784, 22786, 'MEASURE'), (22793, 22804, 'IDENTITY'), (22821, 22836, 'IDENTITY'), (22891, 22897, 'MEASURE'), (22923, 22930, 'MEASURE'), (22946, 22948, 'IDENTITY'), (22961, 22967, 'PROCESS'), (23050, 23062, 'SUCCESS'), (23063, 23065, 'IDENTITY'), (23089, 23104, 'MEASURE'), (23224, 23236, 'SUCCESS'), (23302, 23305, 'MEASURE'), (23311, 23316, 'IDENTITY'), (23317, 23325, 'MEASURE'), (23400, 23412, 'SUCCESS'), (23416, 23424, 'SUCCESS'), (23431, 23441, 'SUCCESS'), (23460, 23471, 'SUCCESS'), (23472, 23479, 'IDENTITY'), (23485, 23492, 'SUCCESS'), (23493, 23497, 'MEASURE'), (23507, 23510, 'IDENTITY'), (23521, 23524, 'MEASURE'), (23528, 23535, 'PROCESS'), (23536, 23547, 'MEASURE'), (23661, 23663, 'SUCCESS'), (23742, 23759, 'SUCCESS'), (23764, 23776, 'IDENTITY'), (23950, 23959, 'IDENTITY'), (24043, 24045, 'SUCCESS'), (24071, 24073, 'IDENTITY'), (24074, 24076, 'MEASURE'), (24077, 24080, 'MEASURE'), (24081, 24095, 'MEASURE'), (24096, 24098, 'MEASURE'), (24099, 24108, 'MEASURE'), (24136, 24138, 'MEASURE'), (24224, 24233, 'MEASURE'), (24255, 24264, 'SUCCESS'), (24276, 24279, 'SUCCESS'), (24295, 24302, 'MEASURE'), (24312, 24318, 'IDENTITY'), (24334, 24345, 'SUCCESS'), (24406, 24419, 'PROCESS'), (24434, 24442, 'IDENTITY'), (24558, 24563, 'PROCESS'), (24579, 24583, 'SUCCESS'), (24584, 24592, 'IDENTITY'), (24662, 24676, 'IDENTITY'), (24729, 24734, 'IDENTITY'), (24739, 24747, 'IDENTITY'), (24777, 24780, 'MEASURE'), (24875, 24881, 'SUCCESS'), (24895, 24899, 'IDENTITY'), (24956, 24966, 'SUCCESS'), (24967, 24977, 'IDENTITY'), (25058, 25060, 'SUCCESS'), (25085, 25094, 'MEASURE'), (25118, 25120, 'SUCCESS'), (25145, 25156, 'MEASURE'), (25191, 25202, 'IDENTITY'), (25255, 25260, 'PROCESS'), (25268, 25274, 'MEASURE'), (25408, 25411, 'PROCESS'), (25467, 25469, 'MEASURE'), (25481, 25486, 'MEASURE'), (25658, 25666, 'MEASURE'), (25721, 25733, 'MEASURE'), (25734, 25737, 'SUCCESS'), (25899, 25918, 'SUCCESS'), (25922, 25926, 'MEASURE'), (25933, 25941, 'MEASURE'), (25954, 25958, 'MEASURE'), (26033, 26038, 'IDENTITY'), (26062, 26075, 'MEASURE'), (26076, 26079, 'MEASURE'), (26131, 26150, 'IDENTITY'), (26168, 26175, 'IDENTITY'), (26196, 26204, 'MEASURE'), (26223, 26228, 'MEASURE'), (26299, 26304, 'SUCCESS'), (26318, 26327, 'MEASURE'), (26332, 26342, 'IDENTITY'), (26377, 26390, 'PROCESS'), (26454, 26456, 'MEASURE'), (26479, 26487, 'MEASURE'), (26523, 26530, 'MEASURE'), (26552, 26556, 'IDENTITY'), (26564, 26575, 'PROCESS'), (26593, 26595, 'MEASURE'), (26657, 26664, 'MEASURE'), (26717, 26719, 'IDENTITY'), (26759, 26774, 'MEASURE'), (26782, 26785, 'MEASURE'), (26805, 26819, 'MEASURE'), (26907, 26911, 'MEASURE'), (26934, 26943, 'MEASURE'), (26994, 27012, 'SUCCESS'), (27013, 27017, 'SUCCESS'), (27018, 27020, 'SUCCESS'), (27021, 27027, 'SUCCESS'), (27069, 27073, 'SUCCESS'), (27096, 27105, 'SUCCESS'), (27106, 27119, 'MEASURE'), (27139, 27141, 'SUCCESS'), (27149, 27160, 'MEASURE'), (27189, 27196, 'IDENTITY'), (27204, 27207, 'IDENTITY'), (27208, 27220, 'SUCCESS'), (27253, 27263, 'MEASURE'), (27294, 27299, 'SUCCESS'), (27364, 27372, 'MEASURE'), (27398, 27400, 'MEASURE'), (27445, 27447, 'SUCCESS'), (27497, 27505, 'SUCCESS'), (27512, 27514, 'SUCCESS'), (27533, 27535, 'MEASURE'), (27542, 27546, 'MEASURE'), (27557, 27572, 'PROCESS'), (27656, 27662, 'SUCCESS'), (27729, 27730, 'PROCESS'), (27738, 27742, 'PROCESS'), (27743, 27751, 'MEASURE'), (27762, 27774, 'MEASURE'), (27810, 27821, 'PROCESS'), (27830, 27832, 'MEASURE'), (27861, 27862, 'PROCESS'), (27887, 27900, 'MEASURE'), (27935, 27945, 'MEASURE'), (27951, 27954, 'MEASURE'), (27965, 27978, 'IDENTITY'), (28064, 28079, 'SUCCESS'), (28080, 28086, 'MEASURE'), (28087, 28094, 'MEASURE'), (28102, 28110, 'IDENTITY'), (28184, 28196, 'SUCCESS'), (28212, 28217, 'MEASURE'), (28222, 28234, 'SUCCESS'), (28235, 28237, 'IDENTITY'), (28294, 28297, 'MEASURE'), (28316, 28320, 'PROCESS'), (28327, 28340, 'SUCCESS'), (28351, 28361, 'MEASURE'), (28362, 28372, 'IDENTITY'), (28444, 28459, 'SUCCESS'), (28467, 28470, 'IDENTITY'), (28488, 28491, 'MEASURE'), (28532, 28545, 'SUCCESS'), (28551, 28557, 'PROCESS'), (28562, 28564, 'PROCESS'), (28570, 28579, 'MEASURE'), (28585, 28594, 'IDENTITY'), (28609, 28618, 'IDENTITY'), (28650, 28660, 'SUCCESS'), (28703, 28708, 'SUCCESS'), (28722, 28739, 'IDENTITY'), (28747, 28750, 'MEASURE'), (28793, 28805, 'MEASURE'), (28835, 28840, 'PROCESS'), (28850, 28853, 'IDENTITY'), (28858, 28864, 'MEASURE'), (28875, 28884, 'SUCCESS'), (28929, 28935, 'PROCESS'), (28979, 28997, 'SUCCESS'), (29009, 29020, 'MEASURE'), (29036, 29045, 'MEASURE'), (29070, 29075, 'PROCESS'), (29100, 29107, 'MEASURE'), (29180, 29183, 'SUCCESS'), (29198, 29199, 'IDENTITY'), (29211, 29216, 'IDENTITY'), (29224, 29230, 'MEASURE'), (29239, 29247, 'PROCESS'), (29287, 29292, 'SUCCESS'), (29293, 29306, 'MEASURE'), (29358, 29360, 'IDENTITY'), (29382, 29387, 'SUCCESS'), (29405, 29406, 'SUCCESS'), (29407, 29413, 'MEASURE'), (29419, 29440, 'MEASURE'), (29464, 29466, 'SUCCESS'), (29467, 29479, 'SUCCESS'), (29493, 29494, 'MEASURE'), (29509, 29511, 'MEASURE'), (29521, 29533, 'MEASURE'), (29556, 29565, 'IDENTITY'), (29589, 29592, 'MEASURE'), (29666, 29669, 'PROCESS'), (29818, 29824, 'MEASURE'), (29831, 29844, 'SUCCESS'), (29880, 29888, 'SUCCESS'), (29900, 29902, 'IDENTITY'), (30008, 30009, 'SUCCESS'), (30017, 30023, 'PROCESS'), (30035, 30038, 'PROCESS'), (30066, 30081, 'MEASURE'), (30096, 30101, 'MEASURE'), (30125, 30128, 'MEASURE'), (30139, 30142, 'MEASURE'), (30173, 30180, 'MEASURE'), (30275, 30286, 'SUCCESS'), (30287, 30293, 'IDENTITY'), (30302, 30312, 'IDENTITY'), (30321, 30332, 'IDENTITY'), (30346, 30351, 'PROCESS'), (30373, 30380, 'MEASURE'), (30472, 30476, 'MEASURE'), (30508, 30513, 'MEASURE'), (30514, 30516, 'MEASURE'), (30522, 30527, 'SUCCESS'), (30528, 30530, 'MEASURE'), (30549, 30556, 'IDENTITY'), (30564, 30571, 'MEASURE'), (30723, 30728, 'MEASURE'), (30842, 30850, 'SUCCESS'), (30851, 30856, 'MEASURE'), (30857, 30866, 'IDENTITY'), (30867, 30870, 'MEASURE'), (30875, 30880, 'MEASURE'), (30888, 30891, 'MEASURE'), (30898, 30908, 'MEASURE'), (30913, 30929, 'MEASURE'), (30930, 30939, 'MEASURE'), (30943, 30947, 'MEASURE'), (30976, 30983, 'IDENTITY'), (31021, 31031, 'MEASURE'), (31072, 31074, 'SUCCESS'), (31075, 31083, 'MEASURE'), (31091, 31095, 'IDENTITY'), (31128, 31130, 'PROCESS'), (31155, 31158, 'MEASURE'), (31182, 31185, 'SUCCESS'), (31186, 31189, 'MEASURE'), (31196, 31199, 'MEASURE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.training.offsets_to_biluo_tags(nlp.make_doc(\"increases performance\"), entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ILLlx1OGq2",
        "outputId": "82803b11-26df-4ed8-852b-68f23dc17856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"increases performance\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', '-']"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKp6wofAEz6o"
      },
      "source": [
        "### Using Offset\n",
        "<a id='offset'> </a>\n",
        "\n",
        " * Training process is the same as the previous one except data creation is different.\n",
        " * Here annotations are created using offset indices while the scheme is of course still BILUO.\n",
        " * One can see that this is a bit clumsy to use, of course, still works.\n",
        " * I can not make a claim which is better or has similar performance- as one needs to perform experiments to make any claim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kqEi8YHEz6p",
        "cellView": "form"
      },
      "source": [
        "#@title trained data\n",
        "# For one instance\n",
        "text = (\"One of the goals of traffic engineering is to achieve a flexible tradeoff between fairness and throughput so that users are satisfied with their bandwidth allocation and the network operator is satisfied with the utilization of network resources In this paper we propose a novel way to balance the throughput and fairness objectives with linear programming It allows the network operator to precisely control the tradeoff by bounding the fairness degradation for each commodity compared to the maxmin fair solution or the throughput degradation compared to the optimal throughput We also present improvements to a previous algorithm that achieves maxmin fairness by solving a series of linear programs We significantly reduce the number of steps needed when the access rate of commodities is limited We extend the algorithm to two important practical use cases importance weights and piecewise linear utility functions for commodities Our experiments on synthetic and real networks show that our algorithms achieve a significant speedup and provide practical insights on the tradeoff between fairness and throughput The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data such as medical records or other personal information To address those concerns one promising approach is Private Aggregation of Teacher Ensembles or PATE which transfers to a student model the knowledge of an ensemble of teacher models with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers’ answers However PATE has so far been evaluated only on simple classification tasks like MNIST leaving unclear its utility when applied to largerscale learning tasks and realworld datasets In this work we show how PATE can scale to learning tasks with large numbers of output classes and uncurated imbalanced training data with errors For this we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise and prove their tighter differential guarantees Our new mechanisms build on two insights the chance of teacher consensus is increased by using more concentrated noise and lacking consensus no answer need be given to a student The consensus answers used are more likely to be correct offer better intuitive privacy and incur lower privacy cost Our evaluation shows our mechanisms improve on the original PATE on all measures and scale to larger tasks with both high utility and very strong privacy Can we efficiently extract useful information from a large user dataset while protecting the privacy of the users and ensuring fairness in representation We cast this problem as an instance of a deletion submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria We propose the first memory centralized streaming and distributed methods with constant approximation guarantees against number of adversarial deletions We extensively evaluate the performance of our algorithms against prior state on realworld applications including Uber up locations with location privacy constraints ii fairness constraints for income prediction and crime rate prediction and iii robust to deletion summarization of census data consisting of 2,458,285 feature vectors We study risksensitive imitation learning where the agents goal is to perform at least as well as the expert in terms of a risk profile We first formulate our risksensitive imitation learning setting We consider the generative adversarial approach to imitation learning GAIL and derive an optimization problem for our formulation which we call it risksensitive GAIL RSGAIL We then derive two different versions of our RSGAIL optimization problem that aim at matching the risk profiles of the agent and the expert distance and develop risksensitive generative adversarial imitation learning algorithms based on these optimization problems We evaluate the performance of our algorithms and compare them with GAIL and the risk imitation learning RAIL algorithms in two MuJoCo and two OpenAI classical control tasks With the public release of embedding models it’s important to understand the various biases that they contain Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models In this post we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias How should we decide which fairness criteria or definitions to adopt in machine learning systems? To answer this question we must study the fairness preferences of actual users of machine learn ing systems Stringent parity constraints on treat ment or impact can come with trade and may not even be preferred by the social groups in question Thus it might be beneficial to elicit what the group prefer ences are rather than rely on a priori defined mathematical fairness constraints Simply asking for self rankings of users is challenging because research has shown that there are often gaps between people stated and actual preferences paper outlines a research program and ex perimental designs for investigating these ques tions Participants in the experiments are invited to perform a set of tasks in exchange for a base payment are told upfront that they may receive a bonus later on and the bonus could de pend on some combination of output quantity and quality The same group of workers then votes on a bonus payment structure to elicit preferences The voting is hypothetical not tied to an outcome for half the group and actual tied to the actual payment outcome for the other half so that we can understand the relation between a group’s actual preferences and hypothetical stated preferences Connections and lessons from fairness in machine learning are explored Differentially Private Stochastic Gradient Descent DP forms a fundamental building block in many applications for learning over sensitive data Two standard approaches privacy amplification by subsampling and privacy amplification by shuffling permit adding lower noise in DP than via schemes A key assumption in both these approaches is that the elements in the data set can be uniformly sampled or be uniformly permuted constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion In this paper we focus on conducting iterative methods like DP in the setting of federated learning FL wherein the data is distributed among many devices clients Our main contribution is the random check distributed protocol which crucially relies only on randomized participation decisions made locally and independently by each client It has privacy accuracy trade similar to privacy amplification by subsampling However our method does not require server communication or even knowledge of the population size To our knowledge this is the first privacy amplification tailored for a distributed learning framework and it may have broader applicability beyond FL Along the way we extend privacy amplification by shuffling to incorporate local randomizers and exponentially improve its guarantees In practical regimes this improvement allows for similar privacy and utility using data from an order of magnitude fewer users In this paper we study counterfactual fairness in text classification which asks the question How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that Some people are gay is toxic while Some people are straight is nontoxic We offer a metric counterfactual token fairness CTF for measuring this particular form of fairness in text classifiers and describe its relationship with group fairness Further we offer three approaches blindness counterfactual augmentation and counterfactual logit pairing CLP for optimizing counterfactual token fairness during training bridging the robustness and fairness literature Empirically we find that blindness and CLP address counterfactual token fairness The methods do not harm classifier performance and have varying tradeoffs with group fairness These approaches both for measurement and optimization provide a new path forward for addressing fairness concerns in text classification Machine learning ML is increasingly being used in image retrieval systems for medical decision making One application of ML is to retrieve visually similar medical images from past patients eg tissue from biopsies to reference when making a medical decision with a new patient However no algorithm can perfectly capture an expert ideal notion of similarity for every case an image that is algorithmically determined to be similar may not be medically relevant to a doctors specific diagnostic needs In this paper we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm and developed tools that empower users to cope with the search algorithm onthefly communicating what types of similarity are most important at different moments in time In two evaluations with pathologists we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm The tools were preferred over a traditional interface without a loss in diagnostic accuracy We also observed that users adopted new strategies when using refinement tools repurposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors Taken together these findings inform future humanML collaborative systems for expert decisionmaking Machine learning is often viewed as an inherently valueneutral process statistical tendencies in the training inputs are simply used to generalize to new examples However when models impact social systems such as interactions between humans these patterns learned by models have normative implications It is important that we ask not only patterns exist in the data? but also how do we want our system to impact people? In particular because minority and marginalized members of society are often statistically underrepresented in data sets models may have undesirable disparate impact on such groups As such objectives of social equity and distributive justice require that we develop tools for both identifying and interpreting harms introduced by models This paper directly addresses the challenge of interpreting how human values are implicitly encoded by deep neural networks a machine learning paradigm often seen as inscrutable Doing so requires understanding how the node activations of neural networks relate to valueladen human concepts such as respectful and abusive as well as to concepts about human social identities such as gay straight male female etc To do this we present the first application of Testing with Concept Activation Vectors to models for analyzing human language Diversity including gender diversity is valued by many software development organizations yet the field remains dominated by men One reason for this lack of diversity is gender bias In this paper we study the effects of that bias by using an existing framework derived from the gender studies literature We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub then evaluate those hypotheses quantitatively While our results show that effects of gender bias are largely invisible on the GitHub platform itself there are still signals of women concentrating their work in fewer places and being more restrained in communication than men This paper describes a testing methodology for quantitatively assessing the risk of of rare or unique sequences in generative sequence models common type of neural network Such models are sometimes trained on sensitive data the text of users private messages our methodology allows deeplearning to choose configurations that minimize memorization during training thereby benefiting privacy In experiments we show that unintended memorization is a persistent hardtoavoid issue that can have serious consequences Specifically if not addressed during training we show that new efficient procedures can allow extracting unique secret sequences such as credit card numbers from trained models We also show that our testing strategy is practical and easytoapply eg by describing its use for quantitatively preventing data exposure in a production commercial neural network predictive emailcomposition assistant trained on millions of users email messages Classifiers can be trained with datadependent constraints to satisfy fairness goals reduce churn achieve a targeted positive rate or other policy goals We study the generalization performance for such constrained optimization problems in terms of how well the constraints are satisfied at evaluation time given that they are satisfied at training time To improve generalization we frame the problem as a twoplayer game where one player optimizes the model parameters on a training dataset and the other player enforces the constraints on an independent validation dataset We build on recent work in twoplayer constrained optimization to show that if one uses this twodataset approach then constraint generalization can be significantly improved As we illustrate experimentally this approach works not only in theory but also in practice The potential for learned models to amplify existing societal biases has been broadly recognized Fairness classifier constraints which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender seek to rectify inequity but can yield nonuniform degradation in performance for skewed datasets In certain domains imbalanced degradation of performance can yield another form of unintentional bias In the spirit of constructing fairness aware algorithms as societal imperative we explore an alternative ParetoEfficient Fairness PEF PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane maximizing multiple subgroup accuracies Empirically we demonstrate that PEF increases performance of all subgroups in several UCI datasets In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of blackbox machine learning models may be misplaced If we presume for the sake of this paper that machine learning can be a source of knowledge then it makes sense to wonder what kind of justification it involves How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that in general people implicitly adopt reliabilism regarding machine learning Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method We argue that in cases where model deployments require moral justification reliabilism is not sufficient and instead justifying deployment requires establishing robust human processes as a moral wrapper around machine outputs We then suggest that in certain highstakes domains with moral consequences reliabilism does not provide another kind of necessary justification moral justification Finally we offer cautions relevant to the implicit or explicit adoption of the reliabilist interpretation of machine learning We study the task of extracting covert or veiled toxicity labels from user comments Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing We introduce an initial dataset COVERTTOXICITY which aims to identify such comments from a refined rater template with rater associated categories Finally we finetune a commentdomain BERT model to classify covertly offensive comments and compare against existing baselines When collecting annotations and labeled data from humans a standard practice is to use interrater reliability IRR as a measure of data goodness Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances especially on subjective topics We present a new alternative to interpreting IRR that is more empirical and contextualized It is based upon benchmarking IRR against baseline measures in a replication one of which is a novel crossreplication reliability xRR measure based on Cohen’s 196O kappa We call this approach the xRR framework We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework We argue this framework can be used to measure the quality of crowdsourced datasets Speech samples from over 1OOO individuals with impaired speech have been submitted for Project Euphonia aimed at improving automated speech recognition for atypical speech We provide an update on the contents of the corpus which recently passed 1 million utterances and review key lessons learned from this project The reasoning behind decisions such as phrase set composition prompted vs extemporaneous speech metadata and data quality efforts are explained based on findings from both technical and userfacing research Code review is a powerful technique to ensure high quality software and spread knowledge of best coding practices between engineers Unfortunately code reviewers may have biases about authors of the code they are reviewing which can lead to inequitable experiences and outcomes In this paper we describe a field experiment with anonymous author code review where we withheld author identity information during code reviews from 3OO professional software engineers at one company Our results suggest that during anonymous author code review reviewers can frequently guess authors’ identities that focus is reduced on reviewerauthor power dynamics and that the practice poses a barrier to offline highbandwidth conversations Based on our findings we recommend that those who choose to implement anonymous author code review should reveal the time zone of the author by default have a breaktheglass option for revealing author identity and reveal author identity directly after the review Deep neural networks DNNs routinely achieve stateoftheart performance in a wide range of tasks This case study reports on the development of onboarding ie training materials for a DNNbased medical AI Assistant to aid in the grading of prostate cancer Specifically we describe how the process of developing these materials deepened the teams understanding of enduser requirements leading to changes in the development and assessment of the underlying machine learning model In this sense the onboarding materials served as a useful boundary object for a crossfunctional team We also present evidence of the utility of the subsequent onboarding materials by describing which information was found useful by participants in an experimental study Conventional algorithmic fairness is Westcentric as seen in its subgroups values and optimisations In this paper we decenter algorithmic fairness and analyse AI power in India Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India we find that several assumptions of algorithmic fairness are challenged in India We find that data is not always reliable due to socioeconomic factors users are given third world treatment by ML makers and AI signifies unquestioning aspiration We contend that localising model fairness alone can be window dressing in India where the distance between models and oppressed communities is large Instead we reimagine algorithmic fairness in India and provide a roadmap to recontextualise data and models empower oppressed communities and enable FairML ecosystems The widespread availability of cell phones has enabled nonprofits to deliver critical health information to their beneficiaries in a timely manner This paper describes our work in assisting nonprofits employing automated messaging programs to deliver timely preventive care information to new and expecting mothers during pregnancy and after delivery Unfortunately a key challenge in such information delivery programs is that a significant fraction of beneficiaries tend to drop out Yet nonprofits often have limited healthworker resources time to place crucial service calls for live interaction with beneficiaries to prevent such engagement drops To assist nonprofits in optimizing this limited resource we developed a Restless MultiArmed Bandits system One key technical contribution in this system is a novel clustering method of offline historical data to infer unknown RMAB parameters Our second major contribution is evaluation of our RMAB system in collaboration with an NGO via a realworld service quality improvement study The study compared strategies for optimizing service calls to 23OO3 participants over a period of 7 weeks to reduce engagement drops We show that the  RMAB group provides statistically significant improvement over other comparison groups reducing ∼ 3O% engagement drops To the best of our knowledge this is the first study demonstrating the utility of RMABs in real world public health settings We are transitioning our system to the NGO for realworld use We present SonicHoop an augmented aerial hoop with capacitive touch sensing and interactive sonification SonicHoop is equipped with 42 electrodes equally distributed over the hoop which detect touch events between the hoop and the performer body We add interactive sonification of the touch events with the goal of first providing auditory feedback of the movements and second transforming the aerial hoop into a digital musical instrument that can be played by the performers body We explored 3 sonification strategies ambient lounge and electro dance Structured observation with 2 professional aerial hoop performers shows that fundamentally changes their perception and choreographic processes instead of translating music into movement they search for bodily expressions to compose music Different sound designs affect their movement differently and auditory feedback regardless of types of sound improves movement quality We discuss opportunities for using SonicHoop as a creative object a pedagogical tool and a digital musical instrument as well as using interactive sonification in other acrobatic practices to explore fullbody vertical interaction As people all over the world adopt machine translation MT to communicate across languages there is increased need for affordances that aid users in understanding when to rely on automated translations Identifying the information and interactions that will most help users meet their translation needs is an open area of research at the intersection of HumanComputer Interaction HCI and Natural Language Processing NLP This paper advances work in this area by drawing on a survey of users strategies in assessing translations We identify three directions for the design of translation systems that support more reliable and effective use of machine translation helping users craft good inputs helping users understand translations and expanding interactivity and adaptivity We describe how these can be introduced in current MT systems and highlight open questions for HCI and NLP research Artificial intelligence AI offers opportunities to solve complex problems facing smallholder farmers in the Global South However there is currently a dearth of research and resources available to organizations and policymakers for building farmercentered AI systems As technologists we believe it is our responsibility to draw from and contribute to research on farmers needs practices value systems social worlds and daily agricultural ecosystem realities Drawing from our own fieldwork experience and scholarship we propose concrete future directions for building AI solutions and tools that are meaningful to farmers and will significantly improve their lives We also discuss tensions that may arise when incorporating AI into farming ecosystems We hope that a closer look into these research areas will serve as a guide for technologists looking to leverage AI to help smallholder farmers in the Global South As mobile internet growth continues to bring New Internet Users NIUs online technology has adapted to fit this user segment User barriers like devices and connectivity have declined as mobile phone prices have become more affordable and infrastructure has continued to develop connecting more communities globally App development has also evolved to better suit users on lowcost Android devices Lite apps have entered the space as a solution for users in constrained environments While there are many benefits to lite app designs their effectiveness is unclear for their likely target beneficiaries NIUs coming online In this mixedmethod study we explore the experience for NIUs trying out a smartphone with lite apps for a month in Brazil and India We conducted this research by collecting diary data and followup inperson interviews Results found that three phases of challenges occurred in the first 28 days with a lite smartphone 1 getting started with accounts 2 learning how to use the mobile platform and apps and 3 meeting expectations and mastering the internet Through understanding the friction points in each phase insights surfaced design principles for future NIU technology Machine learning is challenging the way we make music Although research in deep generative models has dramatically improved the capability and fluency of music models recent work has shown that it can be challenging for humans to partner with this new class of algorithms In this paper we present findings on what 13 musician developer teams a total of 61 users needed when cocreating a song with AI the challenges they faced and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges Many teams adopted modular approaches such as independently running multiple smaller models that align with the musical building blocks of a song before recombining their results As ML models are not easily steerable teams also generated massive numbers of samples and curated them posthoc or used a range of strategies to direct the generation or algorithmically ranked the samples Ultimately teams not only had to manage the flare and focus aspects of the creative process but also juggle that with a parallel process of exploring and curating multiple ML models and outputs These findings reflect a need to design machine learningpowered music interfaces that are more decomposable steerable interpretable and adaptive which in return will enable artists to more effectively explore how AI can extend their personal expression Wikipedia’s mission is a world in which everyone can share in the sum of all knowledge That mission has been very unevenly achieved in the first two decades of Wikipedia and one of the largest hindrances is the sheer number of languages Wikipedia needs to cover in order to achieve that goal We argue that we need a new approach to tackle this problem more effectively a multilingual Wikipedia where content can be shared between language editions This paper proposes an architecture for a system that fulfills this goal It separates the goal in two parts creating and maintaining content in an abstract notation within a project called Abstract Wikipedia and creating an infrastructure called Wikilambda that can translate this notation to natural language Both parts are fully owned and maintained by the community as is the integration of the results in the existing Wikipedia editions This architecture will make more encyclopedic content available to more people in their own language and at the same time allow more people to contribute knowledge and reach more people with their contributions no matter what their respective language backgrounds Additionally Wikilambda will unlock a new type of knowledge asset people can share in through the Wikimedia projects functions which will vastly expand what people can do with knowledge from Wikimedia and provide a new venue to collaborate and capture the creativity of contributors from all around the world These two projects will considerably expand the capabilities of the Wikimedia platform to enable every single human being to freely share share in the sum of all knowledge Headbased pointing is an alternative input method for people with motor impairments to access computing devices This paper proposes a calibration tracking input mechanism for mobile devices that makes use of the front camera that is standard on most devices To evaluate our design we performed two Fitts’ Law studies First a comparison study of our method with an existing headbased pointing solution Eva Facial Mouse with subjects without motor impairments Second we conducted what we believe is the first Fitts’ Law study using a mobile head tracker with subjects with motor impairments We extend prior studies with a greater range of index of difficulties IDs bits and achieved promising throughput average O61 bps with motor impairments and O9 bps without We found that users throughput was O95 bps on average in our most difficult task IDs 52 bits which involved selecting a target half the size of the Android recommendation for a touch target after moving nearly the full height of the screen This suggests the system is capable of fine precision tasks We summarize our observations and the lessons from our user studies into a set of design guidelines for headbased pointing systems Video summaries or highlights are a compelling alternative for exploring and contextualizing unprecedented amounts of video material However the summarization process is commonly automatic non transparent and potentially biased towards particular aspects depicted in the original video Therefore our aim is to help users like archivists or collection managers to quickly understand which summaries are the most representative for an original video In this paper we present empirical results on the utility of different types of visual explanations to achieve transparency for end users on how representative video summaries are with respect to the original video We consider four types of video summary explanations which use in different ways the concepts extracted from the original video subtitles and the video stream and their prominence The explanations are generated to meet target user preferences and express different dimensions of transparency prominence semantic coverage distance and quantity of coverage In two user studies we evaluate the utility of the visual explanations for achieving transparency for end users Our results show that explanations representing all of the dimensions have the highest utility for transparency\")\n",
        "doc = text\n",
        "g = {'entities': [(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53, 'SUCCESS'), (65, 73, 'MEASURE'), (82, 90, 'MEASURE'), (95, 105, 'MEASURE'), (114, 119, 'IDENTITY'), (124, 133, 'SUCCESS'), (194, 203, 'SUCCESS'), (236, 245, 'MEASURE'), (273, 278, 'SUCCESS'), (286, 293, 'MEASURE'), (298, 308, 'MEASURE'), (313, 321, 'MEASURE'), (322, 332, 'MEASURE'), (338, 356, 'PROCESS'), (360, 366, 'SUCCESS'), (391, 400, 'SUCCESS'), (401, 408, 'PROCESS'), (438, 446, 'MEASURE'), (468, 477, 'MEASURE'), (501, 505, 'SUCCESS'), (561, 568, 'SUCCESS'), (596, 608, 'SUCCESS'), (638, 646, 'SUCCESS'), (654, 662, 'MEASURE'), (666, 673, 'SUCCESS'), (719, 725, 'MEASURE'), (803, 809, 'PROCESS'), (831, 840, 'SUCCESS'), (841, 850, 'MEASURE'), (909, 918, 'MEASURE'), (939, 950, 'PROCESS'), (996, 1006, 'PROCESS'), (1007, 1014, 'SUCCESS'), (1029, 1036, 'SUCCESS'), (1059, 1067, 'SUCCESS'), (1120, 1125, 'SUCCESS'), (1126, 1134, 'PROCESS'), (1159, 1168, 'SUCCESS'), (1188, 1195, 'MEASURE'), (1229, 1235, 'MEASURE'), (1247, 1256, 'MEASURE'), (1270, 1285, 'MEASURE'), (1346, 1355, 'SUCCESS'), (1376, 1387, 'PROCESS'), (1391, 1398, 'IDENTITY'), (1423, 1432, 'PROCESS'), (1438, 1445, 'IDENTITY'), (1446, 1451, 'MEASURE'), (1456, 1465, 'MEASURE'), (1484, 1491, 'IDENTITY'), (1492, 1498, 'MEASURE'), (1504, 1513, 'SUCCESS'), (1534, 1542, 'PROCESS'), (1543, 1551, 'IDENTITY'), (1573, 1579, 'SUCCESS'), (1588, 1598, 'SUCCESS'), (1727, 1734, 'MEASURE'), (1743, 1746, 'SUCCESS'), (1755, 1759, 'PROCESS'), (1846, 1854, 'PROCESS'), (1861, 1872, 'PROCESS'), (1979, 1988, 'SUCCESS'), (2022, 2025, 'IDENTITY'), (2049, 2057, 'SUCCESS'), (2068, 2071, 'PROCESS'), (2087, 2090, 'SUCCESS'), (2111, 2123, 'SUCCESS'), (2135, 2138, 'SUCCESS'), (2163, 2166, 'SUCCESS'), (2187, 2189, 'IDENTITY'), (2208, 2210, 'SUCCESS'), (2303, 2304, 'IDENTITY'), (2340, 2348, 'SUCCESS'), (2362, 2369, 'SUCCESS'), (2370, 2375, 'SUCCESS'), (2376, 2382, 'SUCCESS'), (2455, 2465, 'SUCCESS'), (2521, 2523, 'SUCCESS'), (2542, 2551, 'SUCCESS'), (2560, 2568, 'SUCCESS'), (2588, 2590, 'SUCCESS'), (2591, 2602, 'PROCESS'), (2603, 2610, 'SUCCESS'), (2656, 2661, 'SUCCESS'), (2688, 2691, 'IDENTITY'), (2698, 2710, 'SUCCESS'), (2720, 2722, 'SUCCESS'), (2906, 2909, 'SUCCESS'), (2923, 2934, 'PROCESS'), (2983, 2996, 'SUCCESS'), (3051, 3062, 'PROCESS'), (3072, 3075, 'MEASURE'), (3091, 3094, 'PROCESS'), (3129, 3138, 'PROCESS'), (3214, 3216, 'SUCCESS'), (3238, 3241, 'MEASURE'), (3242, 3248, 'MEASURE'), (3264, 3269, 'MEASURE'), (3270, 3274, 'MEASURE'), (3290, 3293, 'SUCCESS'), (3304, 3312, 'PROCESS'), (3391, 3414, 'PROCESS'), (3434, 3440, 'MEASURE'), (3449, 3451, 'PROCESS'), (3469, 3471, 'SUCCESS'), (3480, 3483, 'SUCCESS'), (3521, 3526, 'PROCESS'), (3541, 3564, 'PROCESS'), (3657, 3660, 'PROCESS'), (3668, 3670, 'PROCESS'), (3849, 3852, 'MEASURE'), (3904, 3907, 'MEASURE'), (3953, 3971, 'PROCESS'), (4032, 4035, 'SUCCESS'), (4125, 4129, 'PROCESS'), (4199, 4202, 'IDENTITY'), (4218, 4220, 'PROCESS'), (4238, 4240, 'SUCCESS'), (4267, 4270, 'MEASURE'), (4291, 4295, 'IDENTITY'), (4319, 4322, 'SUCCESS'), (4396, 4402, 'PROCESS'), (4468, 4472, 'IDENTITY'), (4473, 4477, 'PROCESS'), (4504, 4509, 'MEASURE'), (4518, 4521, 'PROCESS'), (4554, 4558, 'SUCCESS'), (4573, 4577, 'SUCCESS'), (4578, 4581, 'IDENTITY'), (4592, 4598, 'MEASURE'), (4626, 4637, 'PROCESS'), (4679, 4685, 'IDENTITY'), (4703, 4707, 'MEASURE'), (4727, 4738, 'IDENTITY'), (4772, 4775, 'MEASURE'), (4776, 4783, 'SUCCESS'), (4816, 4821, 'MEASURE'), (4877, 4889, 'IDENTITY'), (4925, 4927, 'SUCCESS'), (4934, 4936, 'PROCESS'), (4951, 4957, 'IDENTITY'), (5012, 5018, 'MEASURE'), (5068, 5074, 'MEASURE'), (5079, 5083, 'IDENTITY'), (5159, 5164, 'IDENTITY'), (5185, 5191, 'MEASURE'), (5196, 5202, 'PROCESS'), (5221, 5229, 'PROCESS'), (5256, 5266, 'PROCESS'), (5293, 5298, 'IDENTITY'), (5342, 5345, 'PROCESS'), (5392, 5395, 'MEASURE'), (5415, 5419, 'IDENTITY'), (5428, 5432, 'MEASURE'), (5550, 5554, 'IDENTITY'), (5594, 5601, 'PROCESS'), (5602, 5611, 'MEASURE'), (5665, 5669, 'MEASURE'), (5771, 5775, 'PROCESS'), (5798, 5806, 'IDENTITY'), (5815, 5816, 'MEASURE'), (5832, 5843, 'MEASURE'), (5844, 5847, 'SUCCESS'), (5861, 5867, 'SUCCESS'), (5880, 5891, 'MEASURE'), (5918, 5920, 'SUCCESS'), (5994, 6001, 'SUCCESS'), (6040, 6042, 'PROCESS'), (6048, 6060, 'MEASURE'), (6094, 6097, 'SUCCESS'), (6107, 6117, 'PROCESS'), (6140, 6142, 'PROCESS'), (6226, 6230, 'SUCCESS'), (6318, 6321, 'PROCESS'), (6329, 6346, 'PROCESS'), (6412, 6416, 'PROCESS'), (6489, 6494, 'PROCESS'), (6584, 6591, 'PROCESS'), (6604, 6615, 'IDENTITY'), (6679, 6684, 'SUCCESS'), (6712, 6721, 'MEASURE'), (6737, 6747, 'MEASURE'), (6762, 6771, 'MEASURE'), (6785, 6788, 'IDENTITY'), (6806, 6810, 'MEASURE'), (6811, 6817, 'MEASURE'), (6833, 6841, 'MEASURE'), (6842, 6847, 'PROCESS'), (6932, 6938, 'MEASURE'), (6956, 6960, 'IDENTITY'), (6978, 6988, 'IDENTITY'), (7001, 7010, 'SUCCESS'), (7019, 7022, 'SUCCESS'), (7060, 7063, 'MEASURE'), (7097, 7103, 'SUCCESS'), (7145, 7150, 'PROCESS'), (7191, 7193, 'IDENTITY'), (7194, 7203, 'PROCESS'), (7219, 7224, 'SUCCESS'), (7237, 7240, 'SUCCESS'), (7278, 7280, 'SUCCESS'), (7304, 7315, 'MEASURE'), (7323, 7326, 'MEASURE'), (7366, 7370, 'MEASURE'), (7374, 7379, 'IDENTITY'), (7399, 7404, 'IDENTITY'), (7413, 7418, 'MEASURE'), (7428, 7442, 'PROCESS'), (7513, 7523, 'MEASURE'), (7572, 7575, 'MEASURE'), (7600, 7608, 'MEASURE'), (7633, 7634, 'PROCESS'), (7659, 7664, 'IDENTITY'), (7668, 7678, 'IDENTITY'), (7684, 7688, 'MEASURE'), (7700, 7703, 'IDENTITY'), (7707, 7712, 'IDENTITY'), (7719, 7723, 'MEASURE'), (7759, 7764, 'MEASURE'), (7812, 7821, 'MEASURE'), (7846, 7854, 'PROCESS'), (7875, 7878, 'IDENTITY'), (7925, 7932, 'MEASURE'), (8001, 8015, 'PROCESS'), (8030, 8033, 'MEASURE'), (8049, 8063, 'SUCCESS'), (8070, 8078, 'MEASURE'), (8108, 8118, 'IDENTITY'), (8158, 8162, 'PROCESS'), (8178, 8181, 'MEASURE'), (8228, 8235, 'SUCCESS'), (8271, 8274, 'IDENTITY'), (8275, 8279, 'MEASURE'), (8288, 8297, 'PROCESS'), (8324, 8334, 'SUCCESS'), (8344, 8355, 'SUCCESS'), (8360, 8372, 'SUCCESS'), (8383, 8386, 'MEASURE'), (8503, 8505, 'MEASURE'), (8562, 8573, 'PROCESS'), (8577, 8579, 'MEASURE'), (8604, 8611, 'IDENTITY'), (8646, 8648, 'PROCESS'), (8670, 8682, 'MEASURE'), (8697, 8704, 'IDENTITY'), (8733, 8740, 'PROCESS'), (8744, 8753, 'IDENTITY'), (8776, 8778, 'MEASURE'), (8842, 8844, 'MEASURE'), (8878, 8885, 'MEASURE'), (8894, 8896, 'IDENTITY'), (8916, 8918, 'MEASURE'), (8958, 8962, 'MEASURE'), (8969, 8971, 'IDENTITY'), (9043, 9052, 'PROCESS'), (9075, 9084, 'SUCCESS'), (9085, 9088, 'IDENTITY'), (9099, 9104, 'SUCCESS'), (9132, 9136, 'PROCESS'), (9158, 9166, 'MEASURE'), (9181, 9191, 'SUCCESS'), (9238, 9245, 'PROCESS'), (9249, 9253, 'IDENTITY'), (9278, 9290, 'SUCCESS'), (9291, 9293, 'PROCESS'), (9294, 9299, 'SUCCESS'), (9311, 9321, 'MEASURE'), (9353, 9360, 'SUCCESS'), (9361, 9363, 'IDENTITY'), (9364, 9370, 'MEASURE'), (9473, 9480, 'MEASURE'), (9502, 9510, 'IDENTITY'), (9511, 9513, 'PROCESS'), (9514, 9518, 'SUCCESS'), (9551, 9561, 'PROCESS'), (9645, 9654, 'MEASURE'), (9675, 9677, 'MEASURE'), (9700, 9706, 'SUCCESS'), (9707, 9712, 'MEASURE'), (9744, 9750, 'MEASURE'), (9827, 9832, 'MEASURE'), (9843, 9845, 'MEASURE'), (9917, 9923, 'PROCESS'), (9957, 9960, 'PROCESS'), (10004, 10011, 'IDENTITY'), (10054, 10062, 'MEASURE'), (10081, 10085, 'MEASURE'), (10125, 10129, 'MEASURE'), (10168, 10172, 'IDENTITY'), (10174, 10177, 'IDENTITY'), (10178, 10182, 'MEASURE'), (10187, 10189, 'SUCCESS'), (10190, 10192, 'IDENTITY'), (10209, 10211, 'IDENTITY'), (10219, 10226, 'IDENTITY'), (10230, 10240, 'IDENTITY'), (10262, 10274, 'MEASURE'), (10318, 10334, 'MEASURE'), (10335, 10337, 'MEASURE'), (10338, 10342, 'SUCCESS'), (10355, 10358, 'IDENTITY'), (10393, 10400, 'MEASURE'), (10408, 10415, 'MEASURE'), (10437, 10443, 'PROCESS'), (10469, 10476, 'PROCESS'), (10482, 10484, 'PROCESS'), (10485, 10492, 'MEASURE'), (10537, 10542, 'PROCESS'), (10575, 10583, 'MEASURE'), (10584, 10593, 'MEASURE'), (10628, 10640, 'MEASURE'), (10688, 10689, 'MEASURE'), (10716, 10721, 'SUCCESS'), (10782, 10786, 'MEASURE'), (10787, 10798, 'IDENTITY'), (10799, 10801, 'MEASURE'), (10818, 10824, 'MEASURE'), (10828, 10838, 'MEASURE'), (10877, 10884, 'IDENTITY'), (10885, 10892, 'MEASURE'), (10899, 10907, 'IDENTITY'), (10908, 10913, 'IDENTITY'), (10914, 10919, 'IDENTITY'), (10920, 10926, 'IDENTITY'), (11030, 11034, 'PROCESS'), (11035, 11042, 'MEASURE'), (11043, 11053, 'MEASURE'), (11054, 11061, 'MEASURE'), (11065, 11071, 'MEASURE'), (11072, 11075, 'MEASURE'), (11121, 11127, 'MEASURE'), (11128, 11137, 'IDENTITY'), (11177, 11190, 'IDENTITY'), (11226, 11229, 'MEASURE'), (11234, 11240, 'IDENTITY'), (11241, 11244, 'MEASURE'), (11268, 11270, 'PROCESS'), (11291, 11296, 'MEASURE'), (11318, 11320, 'MEASURE'), (11334, 11339, 'IDENTITY'), (11362, 11369, 'PROCESS'), (11414, 11417, 'MEASURE'), (11462, 11464, 'SUCCESS'), (11489, 11492, 'PROCESS'), (11569, 11574, 'IDENTITY'), (11575, 11578, 'MEASURE'), (11592, 11596, 'MEASURE'), (11645, 11648, 'MEASURE'), (11656, 11664, 'IDENTITY'), (11725, 11729, 'MEASURE'), (11733, 11738, 'MEASURE'), (11746, 11749, 'IDENTITY'), (11775, 11788, 'PROCESS'), (11803, 11808, 'PROCESS'), (11819, 11820, 'MEASURE'), (11841, 11844, 'MEASURE'), (11860, 11869, 'MEASURE'), (11870, 11873, 'MEASURE'), (11910, 11923, 'MEASURE'), (11962, 11974, 'MEASURE'), (11996, 12003, 'IDENTITY'), (12031, 12033, 'PROCESS'), (12061, 12079, 'SUCCESS'), (12103, 12117, 'SUCCESS'), (12118, 12122, 'MEASURE'), (12203, 12205, 'MEASURE'), (12243, 12244, 'MEASURE'), (12329, 12338, 'SUCCESS'), (12339, 12345, 'PROCESS'), (12358, 12362, 'PROCESS'), (12403, 12420, 'MEASURE'), (12489, 12493, 'MEASURE'), (12516, 12524, 'PROCESS'), (12554, 12556, 'SUCCESS'), (12560, 12570, 'MEASURE'), (12614, 12622, 'MEASURE'), (12665, 12675, 'IDENTITY'), (12676, 12702, 'MEASURE'), (12759, 12762, 'SUCCESS'), (12763, 12773, 'MEASURE'), (12774, 12778, 'SUCCESS'), (12779, 12792, 'MEASURE'), (12793, 12804, 'SUCCESS'), (12816, 12824, 'SUCCESS'), (12825, 12830, 'MEASURE'), (12844, 12851, 'MEASURE'), (12880, 12885, 'SUCCESS'), (12948, 12959, 'SUCCESS'), (12985, 12990, 'SUCCESS'), (12994, 12997, 'PROCESS'), (13033, 13035, 'SUCCESS'), (13063, 13067, 'SUCCESS'), (13068, 13071, 'MEASURE'), (13082, 13084, 'PROCESS'), (13138, 13145, 'IDENTITY'), (13146, 13148, 'SUCCESS'), (13214, 13216, 'IDENTITY'), (13217, 13218, 'PROCESS'), (13228, 13235, 'MEASURE'), (13244, 13249, 'MEASURE'), (13322, 13327, 'SUCCESS'), (13411, 13430, 'SUCCESS'), (13509, 13523, 'MEASURE'), (13553, 13555, 'SUCCESS'), (13575, 13583, 'PROCESS'), (13588, 13601, 'MEASURE'), (13628, 13636, 'MEASURE'), (13662, 13669, 'MEASURE'), (13690, 13700, 'MEASURE'), (13713, 13718, 'IDENTITY'), (13734, 13744, 'MEASURE'), (13764, 13773, 'MEASURE'), (13782, 13784, 'MEASURE'), (13806, 13810, 'PROCESS'), (13811, 13813, 'MEASURE'), (13846, 13854, 'PROCESS'), (13892, 13894, 'MEASURE'), (13969, 13971, 'MEASURE'), (13972, 13983, 'MEASURE'), (14010, 14023, 'MEASURE'), (14024, 14028, 'MEASURE'), (14029, 14031, 'PROCESS'), (14036, 14042, 'MEASURE'), (14043, 14045, 'SUCCESS'), (14088, 14096, 'MEASURE'), (14111, 14118, 'PROCESS'), (14163, 14177, 'MEASURE'), (14182, 14191, 'IDENTITY'), (14192, 14197, 'MEASURE'), (14212, 14217, 'MEASURE'), (14221, 14229, 'SUCCESS'), (14243, 14250, 'IDENTITY'), (14251, 14253, 'MEASURE'), (14289, 14297, 'MEASURE'), (14298, 14317, 'SUCCESS'), (14333, 14344, 'IDENTITY'), (14354, 14375, 'MEASURE'), (14408, 14419, 'MEASURE'), (14425, 14430, 'MEASURE'), (14454, 14463, 'MEASURE'), (14534, 14541, 'IDENTITY'), (14610, 14628, 'SUCCESS'), (14654, 14656, 'PROCESS'), (14675, 14680, 'MEASURE'), (14702, 14706, 'IDENTITY'), (14707, 14709, 'PROCESS'), (14746, 14757, 'MEASURE'), (14778, 14785, 'MEASURE'), (14851, 14858, 'IDENTITY'), (14859, 14867, 'MEASURE'), (14869, 14871, 'MEASURE'), (14886, 14893, 'MEASURE'), (14912, 14929, 'MEASURE'), (14975, 14997, 'MEASURE'), (15001, 15010, 'MEASURE'), (15056, 15068, 'MEASURE'), (15125, 15135, 'MEASURE'), (15136, 15138, 'MEASURE'), (15145, 15156, 'MEASURE'), (15209, 15211, 'PROCESS'), (15212, 15230, 'SUCCESS'), (15250, 15260, 'MEASURE'), (15351, 15363, 'MEASURE'), (15364, 15368, 'MEASURE'), (15410, 15422, 'SUCCESS'), (15435, 15439, 'MEASURE'), (15523, 15528, 'PROCESS'), (15618, 15620, 'PROCESS'), (15621, 15628, 'MEASURE'), (15638, 15646, 'MEASURE'), (15656, 15658, 'IDENTITY'), (15757, 15767, 'MEASURE'), (15780, 15788, 'MEASURE'), (15836, 15852, 'PROCESS'), (15872, 15879, 'SUCCESS'), (15891, 15894, 'MEASURE'), (15971, 15980, 'PROCESS'), (16123, 16136, 'MEASURE'), (16231, 16240, 'IDENTITY'), (16246, 16268, 'PROCESS'), (16291, 16297, 'MEASURE'), (16324, 16338, 'SUCCESS'), (16358, 16367, 'MEASURE'), (16371, 16384, 'MEASURE'), (16462, 16464, 'MEASURE'), (16486, 16488, 'IDENTITY'), (16504, 16514, 'MEASURE'), (16532, 16535, 'MEASURE'), (16554, 16569, 'MEASURE'), (16589, 16601, 'SUCCESS'), (16612, 16622, 'PROCESS'), (16623, 16625, 'MEASURE'), (16644, 16656, 'SUCCESS'), (16657, 16660, 'SUCCESS'), (16693, 16697, 'PROCESS'), (16698, 16700, 'MEASURE'), (16786, 16794, 'PROCESS'), (16819, 16829, 'MEASURE'), (16877, 16885, 'MEASURE'), (16927, 16930, 'IDENTITY'), (16931, 16934, 'MEASURE'), (16945, 16947, 'MEASURE'), (16948, 16958, 'MEASURE'), (17033, 17047, 'SUCCESS'), (17066, 17075, 'MEASURE'), (17079, 17089, 'MEASURE'), (17090, 17103, 'MEASURE'), (17115, 17122, 'IDENTITY'), (17202, 17206, 'SUCCESS'), (17233, 17246, 'MEASURE'), (17270, 17272, 'SUCCESS'), (17369, 17375, 'SUCCESS'), (17426, 17429, 'MEASURE'), (17441, 17448, 'MEASURE'), (17449, 17456, 'MEASURE'), (17457, 17461, 'MEASURE'), (17605, 17618, 'PROCESS'), (17647, 17656, 'SUCCESS'), (17657, 17680, 'SUCCESS'), (17686, 17695, 'SUCCESS'), (17698, 17706, 'SUCCESS'), (17770, 17772, 'MEASURE'), (17778, 17784, 'IDENTITY'), (17864, 17874, 'MEASURE'), (17921, 17932, 'PROCESS'), (17945, 17957, 'IDENTITY'), (18037, 18054, 'IDENTITY'), (18071, 18082, 'IDENTITY'), (18125, 18133, 'IDENTITY'), (18147, 18150, 'IDENTITY'), (18191, 18212, 'SUCCESS'), (18293, 18316, 'SUCCESS'), (18356, 18363, 'IDENTITY'), (18403, 18408, 'IDENTITY'), (18425, 18427, 'SUCCESS'), (18463, 18472, 'IDENTITY'), (18525, 18529, 'IDENTITY'), (18576, 18582, 'MEASURE'), (18587, 18603, 'MEASURE'), (18617, 18639, 'SUCCESS'), (18736, 18738, 'PROCESS'), (18741, 18745, 'MEASURE'), (18777, 18791, 'PROCESS'), (18804, 18806, 'SUCCESS'), (18821, 18843, 'MEASURE'), (18844, 18845, 'MEASURE'), (18901, 18909, 'SUCCESS'), (18930, 18932, 'MEASURE'), (18988, 18996, 'PROCESS'), (19001, 19006, 'PROCESS'), (19053, 19055, 'MEASURE'), (19116, 19123, 'SUCCESS'), (19124, 19138, 'MEASURE'), (19197, 19205, 'SUCCESS'), (19272, 19279, 'MEASURE'), (19283, 19286, 'IDENTITY'), (19309, 19321, 'PROCESS'), (19339, 19350, 'MEASURE'), (19355, 19360, 'IDENTITY'), (19387, 19389, 'IDENTITY'), (19390, 19402, 'MEASURE'), (19409, 19421, 'MEASURE'), (19458, 19460, 'PROCESS'), (19466, 19468, 'MEASURE'), (19567, 19569, 'PROCESS'), (19576, 19584, 'MEASURE'), (19636, 19647, 'MEASURE'), (19729, 19737, 'MEASURE'), (19753, 19761, 'MEASURE'), (19762, 19764, 'IDENTITY'), (19783, 19786, 'MEASURE'), (19794, 19806, 'IDENTITY'), (19860, 19869, 'PROCESS'), (19873, 19875, 'MEASURE'), (19883, 19899, 'SUCCESS'), (19900, 19913, 'SUCCESS'), (19973, 19979, 'IDENTITY'), (20005, 20010, 'PROCESS'), (20011, 20023, 'MEASURE'), (20043, 20052, 'SUCCESS'), (20074, 20081, 'SUCCESS'), (20107, 20115, 'SUCCESS'), (20116, 20124, 'IDENTITY'), (20129, 20136, 'SUCCESS'), (20137, 20146, 'MEASURE'), (20200, 20211, 'SUCCESS'), (20223, 20229, 'SUCCESS'), (20230, 20255, 'MEASURE'), (20272, 20276, 'IDENTITY'), (20349, 20354, 'SUCCESS'), (20355, 20368, 'IDENTITY'), (20369, 20371, 'PROCESS'), (20399, 20408, 'MEASURE'), (20409, 20412, 'MEASURE'), (20413, 20420, 'MEASURE'), (20452, 20471, 'IDENTITY'), (20556, 20572, 'PROCESS'), (20612, 20621, 'IDENTITY'), (20660, 20662, 'IDENTITY'), (20691, 20693, 'MEASURE'), (20759, 20771, 'IDENTITY'), (20782, 20795, 'SUCCESS'), (20796, 20803, 'SUCCESS'), (20812, 20817, 'SUCCESS'), (20827, 20838, 'SUCCESS'), (20885, 20900, 'MEASURE'), (20901, 20911, 'MEASURE'), (20972, 20982, 'SUCCESS'), (21002, 21028, 'MEASURE'), (21032, 21036, 'PROCESS'), (21044, 21048, 'MEASURE'), (21084, 21094, 'PROCESS'), (21144, 21149, 'IDENTITY'), (21180, 21188, 'SUCCESS'), (21225, 21228, 'PROCESS'), (21229, 21240, 'MEASURE'), (21257, 21268, 'IDENTITY'), (21328, 21336, 'MEASURE'), (21373, 21374, 'IDENTITY'), (21375, 21390, 'SUCCESS'), (21391, 21401, 'SUCCESS'), (21472, 21483, 'SUCCESS'), (21524, 21527, 'SUCCESS'), (21548, 21551, 'SUCCESS'), (21557, 21559, 'MEASURE'), (21636, 21640, 'IDENTITY'), (21647, 21660, 'PROCESS'), (21713, 21726, 'MEASURE'), (21836, 21845, 'PROCESS'), (21846, 21857, 'MEASURE'), (21897, 21901, 'IDENTITY'), (21924, 21929, 'PROCESS'), (21945, 21953, 'MEASURE'), (21984, 22008, 'MEASURE'), (22016, 22021, 'MEASURE'), (22097, 22120, 'SUCCESS'), (22132, 22136, 'IDENTITY'), (22160, 22175, 'MEASURE'), (22225, 22250, 'IDENTITY'), (22251, 22258, 'IDENTITY'), (22295, 22306, 'MEASURE'), (22312, 22326, 'MEASURE'), (22476, 22482, 'MEASURE'), (22494, 22508, 'MEASURE'), (22547, 22553, 'SUCCESS'), (22554, 22568, 'MEASURE'), (22585, 22593, 'SUCCESS'), (22626, 22640, 'MEASURE'), (22724, 22737, 'PROCESS'), (22765, 22783, 'MEASURE'), (22784, 22786, 'MEASURE'), (22793, 22804, 'IDENTITY'), (22821, 22836, 'IDENTITY'), (22891, 22897, 'MEASURE'), (22923, 22930, 'MEASURE'), (22946, 22948, 'IDENTITY'), (22961, 22967, 'PROCESS'), (23050, 23062, 'SUCCESS'), (23063, 23065, 'IDENTITY'), (23089, 23104, 'MEASURE'), (23224, 23236, 'SUCCESS'), (23302, 23305, 'MEASURE'), (23311, 23316, 'IDENTITY'), (23317, 23325, 'MEASURE'), (23400, 23412, 'SUCCESS'), (23416, 23424, 'SUCCESS'), (23431, 23441, 'SUCCESS'), (23460, 23471, 'SUCCESS'), (23472, 23479, 'IDENTITY'), (23485, 23492, 'SUCCESS'), (23493, 23497, 'MEASURE'), (23507, 23510, 'IDENTITY'), (23521, 23524, 'MEASURE'), (23528, 23535, 'PROCESS'), (23536, 23547, 'MEASURE'), (23661, 23663, 'SUCCESS'), (23742, 23759, 'SUCCESS'), (23764, 23776, 'IDENTITY'), (23950, 23959, 'IDENTITY'), (24043, 24045, 'SUCCESS'), (24071, 24073, 'IDENTITY'), (24074, 24076, 'MEASURE'), (24077, 24080, 'MEASURE'), (24081, 24095, 'MEASURE'), (24096, 24098, 'MEASURE'), (24099, 24108, 'MEASURE'), (24136, 24138, 'MEASURE'), (24224, 24233, 'MEASURE'), (24255, 24264, 'SUCCESS'), (24276, 24279, 'SUCCESS'), (24295, 24302, 'MEASURE'), (24312, 24318, 'IDENTITY'), (24334, 24345, 'SUCCESS'), (24406, 24419, 'PROCESS'), (24434, 24442, 'IDENTITY'), (24558, 24563, 'PROCESS'), (24579, 24583, 'SUCCESS'), (24584, 24592, 'IDENTITY'), (24662, 24676, 'IDENTITY'), (24729, 24734, 'IDENTITY'), (24739, 24747, 'IDENTITY'), (24777, 24780, 'MEASURE'), (24875, 24881, 'SUCCESS'), (24895, 24899, 'IDENTITY'), (24956, 24966, 'SUCCESS'), (24967, 24977, 'IDENTITY'), (25058, 25060, 'SUCCESS'), (25085, 25094, 'MEASURE'), (25118, 25120, 'SUCCESS'), (25145, 25156, 'MEASURE'), (25191, 25202, 'IDENTITY'), (25255, 25260, 'PROCESS'), (25268, 25274, 'MEASURE'), (25408, 25411, 'PROCESS'), (25467, 25469, 'MEASURE'), (25481, 25486, 'MEASURE'), (25658, 25666, 'MEASURE'), (25721, 25733, 'MEASURE'), (25734, 25737, 'SUCCESS'), (25899, 25918, 'SUCCESS'), (25922, 25926, 'MEASURE'), (25933, 25941, 'MEASURE'), (25954, 25958, 'MEASURE'), (26033, 26038, 'IDENTITY'), (26062, 26075, 'MEASURE'), (26076, 26079, 'MEASURE'), (26131, 26150, 'IDENTITY'), (26168, 26175, 'IDENTITY'), (26196, 26204, 'MEASURE'), (26223, 26228, 'MEASURE'), (26299, 26304, 'SUCCESS'), (26318, 26327, 'MEASURE'), (26332, 26342, 'IDENTITY'), (26377, 26390, 'PROCESS'), (26454, 26456, 'MEASURE'), (26479, 26487, 'MEASURE'), (26523, 26530, 'MEASURE'), (26552, 26556, 'IDENTITY'), (26564, 26575, 'PROCESS'), (26593, 26595, 'MEASURE'), (26657, 26664, 'MEASURE'), (26717, 26719, 'IDENTITY'), (26759, 26774, 'MEASURE'), (26782, 26785, 'MEASURE'), (26805, 26819, 'MEASURE'), (26907, 26911, 'MEASURE'), (26934, 26943, 'MEASURE'), (26994, 27012, 'SUCCESS'), (27013, 27017, 'SUCCESS'), (27018, 27020, 'SUCCESS'), (27021, 27027, 'SUCCESS'), (27069, 27073, 'SUCCESS'), (27096, 27105, 'SUCCESS'), (27106, 27119, 'MEASURE'), (27139, 27141, 'SUCCESS'), (27149, 27160, 'MEASURE'), (27189, 27196, 'IDENTITY'), (27204, 27207, 'IDENTITY'), (27208, 27220, 'SUCCESS'), (27253, 27263, 'MEASURE'), (27294, 27299, 'SUCCESS'), (27364, 27372, 'MEASURE'), (27398, 27400, 'MEASURE'), (27445, 27447, 'SUCCESS'), (27497, 27505, 'SUCCESS'), (27512, 27514, 'SUCCESS'), (27533, 27535, 'MEASURE'), (27542, 27546, 'MEASURE'), (27557, 27572, 'PROCESS'), (27656, 27662, 'SUCCESS'), (27729, 27730, 'PROCESS'), (27738, 27742, 'PROCESS'), (27743, 27751, 'MEASURE'), (27762, 27774, 'MEASURE'), (27810, 27821, 'PROCESS'), (27830, 27832, 'MEASURE'), (27861, 27862, 'PROCESS'), (27887, 27900, 'MEASURE'), (27935, 27945, 'MEASURE'), (27951, 27954, 'MEASURE'), (27965, 27978, 'IDENTITY'), (28064, 28079, 'SUCCESS'), (28080, 28086, 'MEASURE'), (28087, 28094, 'MEASURE'), (28102, 28110, 'IDENTITY'), (28184, 28196, 'SUCCESS'), (28212, 28217, 'MEASURE'), (28222, 28234, 'SUCCESS'), (28235, 28237, 'IDENTITY'), (28294, 28297, 'MEASURE'), (28316, 28320, 'PROCESS'), (28327, 28340, 'SUCCESS'), (28351, 28361, 'MEASURE'), (28362, 28372, 'IDENTITY'), (28444, 28459, 'SUCCESS'), (28467, 28470, 'IDENTITY'), (28488, 28491, 'MEASURE'), (28532, 28545, 'SUCCESS'), (28551, 28557, 'PROCESS'), (28562, 28564, 'PROCESS'), (28570, 28579, 'MEASURE'), (28585, 28594, 'IDENTITY'), (28609, 28618, 'IDENTITY'), (28650, 28660, 'SUCCESS'), (28703, 28708, 'SUCCESS'), (28722, 28739, 'IDENTITY'), (28747, 28750, 'MEASURE'), (28793, 28805, 'MEASURE'), (28835, 28840, 'PROCESS'), (28850, 28853, 'IDENTITY'), (28858, 28864, 'MEASURE'), (28875, 28884, 'SUCCESS'), (28929, 28935, 'PROCESS'), (28979, 28997, 'SUCCESS'), (29009, 29020, 'MEASURE'), (29036, 29045, 'MEASURE'), (29070, 29075, 'PROCESS'), (29100, 29107, 'MEASURE'), (29180, 29183, 'SUCCESS'), (29198, 29199, 'IDENTITY'), (29211, 29216, 'IDENTITY'), (29224, 29230, 'MEASURE'), (29239, 29247, 'PROCESS'), (29287, 29292, 'SUCCESS'), (29293, 29306, 'MEASURE'), (29358, 29360, 'IDENTITY'), (29382, 29387, 'SUCCESS'), (29405, 29406, 'SUCCESS'), (29407, 29413, 'MEASURE'), (29419, 29440, 'MEASURE'), (29464, 29466, 'SUCCESS'), (29467, 29479, 'SUCCESS'), (29493, 29494, 'MEASURE'), (29509, 29511, 'MEASURE'), (29521, 29533, 'MEASURE'), (29556, 29565, 'IDENTITY'), (29589, 29592, 'MEASURE'), (29666, 29669, 'PROCESS'), (29818, 29824, 'MEASURE'), (29831, 29844, 'SUCCESS'), (29880, 29888, 'SUCCESS'), (29900, 29902, 'IDENTITY'), (30008, 30009, 'SUCCESS'), (30017, 30023, 'PROCESS'), (30035, 30038, 'PROCESS'), (30066, 30081, 'MEASURE'), (30096, 30101, 'MEASURE'), (30125, 30128, 'MEASURE'), (30139, 30142, 'MEASURE'), (30173, 30180, 'MEASURE'), (30275, 30286, 'SUCCESS'), (30287, 30293, 'IDENTITY'), (30302, 30312, 'IDENTITY'), (30321, 30332, 'IDENTITY'), (30346, 30351, 'PROCESS'), (30373, 30380, 'MEASURE'), (30472, 30476, 'MEASURE'), (30508, 30513, 'MEASURE'), (30514, 30516, 'MEASURE'), (30522, 30527, 'SUCCESS'), (30528, 30530, 'MEASURE'), (30549, 30556, 'IDENTITY'), (30564, 30571, 'MEASURE'), (30723, 30728, 'MEASURE'), (30842, 30850, 'SUCCESS'), (30851, 30856, 'MEASURE'), (30857, 30866, 'IDENTITY'), (30867, 30870, 'MEASURE'), (30875, 30880, 'MEASURE'), (30888, 30891, 'MEASURE'), (30898, 30908, 'MEASURE'), (30913, 30929, 'MEASURE'), (30930, 30939, 'MEASURE'), (30943, 30947, 'MEASURE'), (30976, 30983, 'IDENTITY'), (31021, 31031, 'MEASURE'), (31072, 31074, 'SUCCESS'), (31075, 31083, 'MEASURE'), (31091, 31095, 'IDENTITY'), (31128, 31130, 'PROCESS'), (31155, 31158, 'MEASURE'), (31182, 31185, 'SUCCESS'), (31186, 31189, 'MEASURE'), (31196, 31199, 'MEASURE')]}\n",
        "\n",
        "X = [doc]\n",
        "Y = [g]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoHtQnbkMFhU",
        "cellView": "form"
      },
      "source": [
        "#@title traindata2\n",
        "train_data = [\n",
        "\"One of the goals of traffic engineering is to achieve a flexible tradeoff between fairness and throughput so that users are satisfied with their bandwidth allocation and the network operator is satisfied with the utilization of network resources In this paper we propose a novel way to balance the throughput and fairness objectives with linear programming It allows the network operator to precisely control the tradeoff by bounding the fairness degradation for each commodity compared to the maxmin fair solution or the throughput degradation compared to the optimal throughput We also present improvements to a previous algorithm that achieves maxmin fairness by solving a series of linear programs We significantly reduce the number of steps needed when the access rate of commodities is limited We extend the algorithm to two important practical use cases importance weights and piecewise linear utility functions for commodities Our experiments on synthetic and real networks show that our algorithms achieve a significant speedup and provide practical insights on the tradeoff between fairness and throughput The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data such as medical records or other personal information To address those concerns one promising approach is Private Aggregation of Teacher Ensembles or PATE which transfers to a student model the knowledge of an ensemble of teacher models with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers’ answers However PATE has so far been evaluated only on simple classification tasks like MNIST leaving unclear its utility when applied to largerscale learning tasks and realworld datasets In this work we show how PATE can scale to learning tasks with large numbers of output classes and uncurated imbalanced training data with errors For this we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise and prove their tighter differential guarantees Our new mechanisms build on two insights the chance of teacher consensus is increased by using more concentrated noise and lacking consensus no answer need be given to a student The consensus answers used are more likely to be correct offer better intuitive privacy and incur lower privacy cost Our evaluation shows our mechanisms improve on the original PATE on all measures and scale to larger tasks with both high utility and very strong privacy Can we efficiently extract useful information from a large user dataset while protecting the privacy of the users and ensuring fairness in representation We cast this problem as an instance of a deletion submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria We propose the first memory centralized streaming and distributed methods with constant approximation guarantees against number of adversarial deletions We extensively evaluate the performance of our algorithms against prior state on realworld applications including Uber up locations with location privacy constraints ii fairness constraints for income prediction and crime rate prediction and iii robust to deletion summarization of census data consisting of 2,458,285 feature vectors We study risksensitive imitation learning where the agents goal is to perform at least as well as the expert in terms of a risk profile We first formulate our risksensitive imitation learning setting We consider the generative adversarial approach to imitation learning GAIL and derive an optimization problem for our formulation which we call it risksensitive GAIL RSGAIL We then derive two different versions of our RSGAIL optimization problem that aim at matching the risk profiles of the agent and the expert distance and develop risksensitive generative adversarial imitation learning algorithms based on these optimization problems We evaluate the performance of our algorithms and compare them with GAIL and the risk imitation learning RAIL algorithms in two MuJoCo and two OpenAI classical control tasks With the public release of embedding models it’s important to understand the various biases that they contain Developers who use them should be aware of the biases inherent in the models as well as how biases can manifest in downstream applications that use these models In this post we examine a few specific forms of bias and suggest tools for evaluating as well as mitigating bias How should we decide which fairness criteria or definitions to adopt in machine learning systems? To answer this question we must study the fairness preferences of actual users of machine learn ing systems Stringent parity constraints on treat ment or impact can come with trade and may not even be preferred by the social groups in question Thus it might be beneficial to elicit what the group prefer ences are rather than rely on a priori defined mathematical fairness constraints Simply asking for self rankings of users is challenging because research has shown that there are often gaps between people stated and actual preferences paper outlines a research program and ex perimental designs for investigating these ques tions Participants in the experiments are invited to perform a set of tasks in exchange for a base payment are told upfront that they may receive a bonus later on and the bonus could de pend on some combination of output quantity and quality The same group of workers then votes on a bonus payment structure to elicit preferences The voting is hypothetical not tied to an outcome for half the group and actual tied to the actual payment outcome for the other half so that we can understand the relation between a group’s actual preferences and hypothetical stated preferences Connections and lessons from fairness in machine learning are explored Differentially Private Stochastic Gradient Descent DP forms a fundamental building block in many applications for learning over sensitive data Two standard approaches privacy amplification by subsampling and privacy amplification by shuffling permit adding lower noise in DP than via schemes A key assumption in both these approaches is that the elements in the data set can be uniformly sampled or be uniformly permuted constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion In this paper we focus on conducting iterative methods like DP in the setting of federated learning FL wherein the data is distributed among many devices clients Our main contribution is the random check distributed protocol which crucially relies only on randomized participation decisions made locally and independently by each client It has privacy accuracy trade similar to privacy amplification by subsampling However our method does not require server communication or even knowledge of the population size To our knowledge this is the first privacy amplification tailored for a distributed learning framework and it may have broader applicability beyond FL Along the way we extend privacy amplification by shuffling to incorporate local randomizers and exponentially improve its guarantees In practical regimes this improvement allows for similar privacy and utility using data from an order of magnitude fewer users In this paper we study counterfactual fairness in text classification which asks the question How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that Some people are gay is toxic while Some people are straight is nontoxic We offer a metric counterfactual token fairness CTF for measuring this particular form of fairness in text classifiers and describe its relationship with group fairness Further we offer three approaches blindness counterfactual augmentation and counterfactual logit pairing CLP for optimizing counterfactual token fairness during training bridging the robustness and fairness literature Empirically we find that blindness and CLP address counterfactual token fairness The methods do not harm classifier performance and have varying tradeoffs with group fairness These approaches both for measurement and optimization provide a new path forward for addressing fairness concerns in text classification Machine learning ML is increasingly being used in image retrieval systems for medical decision making One application of ML is to retrieve visually similar medical images from past patients eg tissue from biopsies to reference when making a medical decision with a new patient However no algorithm can perfectly capture an expert ideal notion of similarity for every case an image that is algorithmically determined to be similar may not be medically relevant to a doctors specific diagnostic needs In this paper we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm and developed tools that empower users to cope with the search algorithm onthefly communicating what types of similarity are most important at different moments in time In two evaluations with pathologists we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm The tools were preferred over a traditional interface without a loss in diagnostic accuracy We also observed that users adopted new strategies when using refinement tools repurposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors Taken together these findings inform future humanML collaborative systems for expert decisionmaking Machine learning is often viewed as an inherently valueneutral process statistical tendencies in the training inputs are simply used to generalize to new examples However when models impact social systems such as interactions between humans these patterns learned by models have normative implications It is important that we ask not only patterns exist in the data? but also how do we want our system to impact people? In particular because minority and marginalized members of society are often statistically underrepresented in data sets models may have undesirable disparate impact on such groups As such objectives of social equity and distributive justice require that we develop tools for both identifying and interpreting harms introduced by models This paper directly addresses the challenge of interpreting how human values are implicitly encoded by deep neural networks a machine learning paradigm often seen as inscrutable Doing so requires understanding how the node activations of neural networks relate to valueladen human concepts such as respectful and abusive as well as to concepts about human social identities such as gay straight male female etc To do this we present the first application of Testing with Concept Activation Vectors to models for analyzing human language Diversity including gender diversity is valued by many software development organizations yet the field remains dominated by men One reason for this lack of diversity is gender bias In this paper we study the effects of that bias by using an existing framework derived from the gender studies literature We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub then evaluate those hypotheses quantitatively While our results show that effects of gender bias are largely invisible on the GitHub platform itself there are still signals of women concentrating their work in fewer places and being more restrained in communication than men This paper describes a testing methodology for quantitatively assessing the risk of of rare or unique sequences in generative sequence models common type of neural network Such models are sometimes trained on sensitive data the text of users private messages our methodology allows deeplearning to choose configurations that minimize memorization during training thereby benefiting privacy In experiments we show that unintended memorization is a persistent hardtoavoid issue that can have serious consequences Specifically if not addressed during training we show that new efficient procedures can allow extracting unique secret sequences such as credit card numbers from trained models We also show that our testing strategy is practical and easytoapply eg by describing its use for quantitatively preventing data exposure in a production commercial neural network predictive emailcomposition assistant trained on millions of users email messages Classifiers can be trained with datadependent constraints to satisfy fairness goals reduce churn achieve a targeted positive rate or other policy goals We study the generalization performance for such constrained optimization problems in terms of how well the constraints are satisfied at evaluation time given that they are satisfied at training time To improve generalization we frame the problem as a twoplayer game where one player optimizes the model parameters on a training dataset and the other player enforces the constraints on an independent validation dataset We build on recent work in twoplayer constrained optimization to show that if one uses this twodataset approach then constraint generalization can be significantly improved As we illustrate experimentally this approach works not only in theory but also in practice The potential for learned models to amplify existing societal biases has been broadly recognized Fairness classifier constraints which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender seek to rectify inequity but can yield nonuniform degradation in performance for skewed datasets In certain domains imbalanced degradation of performance can yield another form of unintentional bias In the spirit of constructing fairness aware algorithms as societal imperative we explore an alternative ParetoEfficient Fairness PEF PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane maximizing multiple subgroup accuracies Empirically we demonstrate that PEF increases performance of all subgroups in several UCI datasets In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of blackbox machine learning models may be misplaced If we presume for the sake of this paper that machine learning can be a source of knowledge then it makes sense to wonder what kind of justification it involves How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that in general people implicitly adopt reliabilism regarding machine learning Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method We argue that in cases where model deployments require moral justification reliabilism is not sufficient and instead justifying deployment requires establishing robust human processes as a moral wrapper around machine outputs We then suggest that in certain highstakes domains with moral consequences reliabilism does not provide another kind of necessary justification moral justification Finally we offer cautions relevant to the implicit or explicit adoption of the reliabilist interpretation of machine learning We study the task of extracting covert or veiled toxicity labels from user comments Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing We introduce an initial dataset COVERTTOXICITY which aims to identify such comments from a refined rater template with rater associated categories Finally we finetune a commentdomain BERT model to classify covertly offensive comments and compare against existing baselines When collecting annotations and labeled data from humans a standard practice is to use interrater reliability IRR as a measure of data goodness Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances especially on subjective topics We present a new alternative to interpreting IRR that is more empirical and contextualized It is based upon benchmarking IRR against baseline measures in a replication one of which is a novel crossreplication reliability xRR measure based on Cohen’s 196O kappa We call this approach the xRR framework We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework We argue this framework can be used to measure the quality of crowdsourced datasets Speech samples from over 1OOO individuals with impaired speech have been submitted for Project Euphonia aimed at improving automated speech recognition for atypical speech We provide an update on the contents of the corpus which recently passed 1 million utterances and review key lessons learned from this project The reasoning behind decisions such as phrase set composition prompted vs extemporaneous speech metadata and data quality efforts are explained based on findings from both technical and userfacing research Code review is a powerful technique to ensure high quality software and spread knowledge of best coding practices between engineers Unfortunately code reviewers may have biases about authors of the code they are reviewing which can lead to inequitable experiences and outcomes In this paper we describe a field experiment with anonymous author code review where we withheld author identity information during code reviews from 3OO professional software engineers at one company Our results suggest that during anonymous author code review reviewers can frequently guess authors’ identities that focus is reduced on reviewerauthor power dynamics and that the practice poses a barrier to offline highbandwidth conversations Based on our findings we recommend that those who choose to implement anonymous author code review should reveal the time zone of the author by default have a breaktheglass option for revealing author identity and reveal author identity directly after the review Deep neural networks DNNs routinely achieve stateoftheart performance in a wide range of tasks This case study reports on the development of onboarding ie training materials for a DNNbased medical AI Assistant to aid in the grading of prostate cancer Specifically we describe how the process of developing these materials deepened the teams understanding of enduser requirements leading to changes in the development and assessment of the underlying machine learning model In this sense the onboarding materials served as a useful boundary object for a crossfunctional team We also present evidence of the utility of the subsequent onboarding materials by describing which information was found useful by participants in an experimental study Conventional algorithmic fairness is Westcentric as seen in its subgroups values and optimisations In this paper we decenter algorithmic fairness and analyse AI power in India Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India we find that several assumptions of algorithmic fairness are challenged in India We find that data is not always reliable due to socioeconomic factors users are given third world treatment by ML makers and AI signifies unquestioning aspiration We contend that localising model fairness alone can be window dressing in India where the distance between models and oppressed communities is large Instead we reimagine algorithmic fairness in India and provide a roadmap to recontextualise data and models empower oppressed communities and enable FairML ecosystems The widespread availability of cell phones has enabled nonprofits to deliver critical health information to their beneficiaries in a timely manner This paper describes our work in assisting nonprofits employing automated messaging programs to deliver timely preventive care information to new and expecting mothers during pregnancy and after delivery Unfortunately a key challenge in such information delivery programs is that a significant fraction of beneficiaries tend to drop out Yet nonprofits often have limited healthworker resources time to place crucial service calls for live interaction with beneficiaries to prevent such engagement drops To assist nonprofits in optimizing this limited resource we developed a Restless MultiArmed Bandits system One key technical contribution in this system is a novel clustering method of offline historical data to infer unknown RMAB parameters Our second major contribution is evaluation of our RMAB system in collaboration with an NGO via a realworld service quality improvement study The study compared strategies for optimizing service calls to 23OO3 participants over a period of 7 weeks to reduce engagement drops We show that the  RMAB group provides statistically significant improvement over other comparison groups reducing ∼ 3O% engagement drops To the best of our knowledge this is the first study demonstrating the utility of RMABs in real world public health settings We are transitioning our system to the NGO for realworld use We present SonicHoop an augmented aerial hoop with capacitive touch sensing and interactive sonification SonicHoop is equipped with 42 electrodes equally distributed over the hoop which detect touch events between the hoop and the performer body We add interactive sonification of the touch events with the goal of first providing auditory feedback of the movements and second transforming the aerial hoop into a digital musical instrument that can be played by the performers body We explored 3 sonification strategies ambient lounge and electro dance Structured observation with 2 professional aerial hoop performers shows that fundamentally changes their perception and choreographic processes instead of translating music into movement they search for bodily expressions to compose music Different sound designs affect their movement differently and auditory feedback regardless of types of sound improves movement quality We discuss opportunities for using SonicHoop as a creative object a pedagogical tool and a digital musical instrument as well as using interactive sonification in other acrobatic practices to explore fullbody vertical interaction As people all over the world adopt machine translation MT to communicate across languages there is increased need for affordances that aid users in understanding when to rely on automated translations Identifying the information and interactions that will most help users meet their translation needs is an open area of research at the intersection of HumanComputer Interaction HCI and Natural Language Processing NLP This paper advances work in this area by drawing on a survey of users strategies in assessing translations We identify three directions for the design of translation systems that support more reliable and effective use of machine translation helping users craft good inputs helping users understand translations and expanding interactivity and adaptivity We describe how these can be introduced in current MT systems and highlight open questions for HCI and NLP research Artificial intelligence AI offers opportunities to solve complex problems facing smallholder farmers in the Global South However there is currently a dearth of research and resources available to organizations and policymakers for building farmercentered AI systems As technologists we believe it is our responsibility to draw from and contribute to research on farmers needs practices value systems social worlds and daily agricultural ecosystem realities Drawing from our own fieldwork experience and scholarship we propose concrete future directions for building AI solutions and tools that are meaningful to farmers and will significantly improve their lives We also discuss tensions that may arise when incorporating AI into farming ecosystems We hope that a closer look into these research areas will serve as a guide for technologists looking to leverage AI to help smallholder farmers in the Global South As mobile internet growth continues to bring New Internet Users NIUs online technology has adapted to fit this user segment User barriers like devices and connectivity have declined as mobile phone prices have become more affordable and infrastructure has continued to develop connecting more communities globally App development has also evolved to better suit users on lowcost Android devices Lite apps have entered the space as a solution for users in constrained environments While there are many benefits to lite app designs their effectiveness is unclear for their likely target beneficiaries NIUs coming online In this mixedmethod study we explore the experience for NIUs trying out a smartphone with lite apps for a month in Brazil and India We conducted this research by collecting diary data and followup inperson interviews Results found that three phases of challenges occurred in the first 28 days with a lite smartphone 1 getting started with accounts 2 learning how to use the mobile platform and apps and 3 meeting expectations and mastering the internet Through understanding the friction points in each phase insights surfaced design principles for future NIU technology Machine learning is challenging the way we make music Although research in deep generative models has dramatically improved the capability and fluency of music models recent work has shown that it can be challenging for humans to partner with this new class of algorithms In this paper we present findings on what 13 musician developer teams a total of 61 users needed when cocreating a song with AI the challenges they faced and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges Many teams adopted modular approaches such as independently running multiple smaller models that align with the musical building blocks of a song before recombining their results As ML models are not easily steerable teams also generated massive numbers of samples and curated them posthoc or used a range of strategies to direct the generation or algorithmically ranked the samples Ultimately teams not only had to manage the flare and focus aspects of the creative process but also juggle that with a parallel process of exploring and curating multiple ML models and outputs These findings reflect a need to design machine learningpowered music interfaces that are more decomposable steerable interpretable and adaptive which in return will enable artists to more effectively explore how AI can extend their personal expression Wikipedia’s mission is a world in which everyone can share in the sum of all knowledge That mission has been very unevenly achieved in the first two decades of Wikipedia and one of the largest hindrances is the sheer number of languages Wikipedia needs to cover in order to achieve that goal We argue that we need a new approach to tackle this problem more effectively a multilingual Wikipedia where content can be shared between language editions This paper proposes an architecture for a system that fulfills this goal It separates the goal in two parts creating and maintaining content in an abstract notation within a project called Abstract Wikipedia and creating an infrastructure called Wikilambda that can translate this notation to natural language Both parts are fully owned and maintained by the community as is the integration of the results in the existing Wikipedia editions This architecture will make more encyclopedic content available to more people in their own language and at the same time allow more people to contribute knowledge and reach more people with their contributions no matter what their respective language backgrounds Additionally Wikilambda will unlock a new type of knowledge asset people can share in through the Wikimedia projects functions which will vastly expand what people can do with knowledge from Wikimedia and provide a new venue to collaborate and capture the creativity of contributors from all around the world These two projects will considerably expand the capabilities of the Wikimedia platform to enable every single human being to freely share share in the sum of all knowledge Headbased pointing is an alternative input method for people with motor impairments to access computing devices This paper proposes a calibration tracking input mechanism for mobile devices that makes use of the front camera that is standard on most devices To evaluate our design we performed two Fitts’ Law studies First a comparison study of our method with an existing headbased pointing solution Eva Facial Mouse with subjects without motor impairments Second we conducted what we believe is the first Fitts’ Law study using a mobile head tracker with subjects with motor impairments We extend prior studies with a greater range of index of difficulties IDs bits and achieved promising throughput average O61 bps with motor impairments and O9 bps without We found that users throughput was O95 bps on average in our most difficult task IDs 52 bits which involved selecting a target half the size of the Android recommendation for a touch target after moving nearly the full height of the screen This suggests the system is capable of fine precision tasks We summarize our observations and the lessons from our user studies into a set of design guidelines for headbased pointing systems Video summaries or highlights are a compelling alternative for exploring and contextualizing unprecedented amounts of video material However the summarization process is commonly automatic non transparent and potentially biased towards particular aspects depicted in the original video Therefore our aim is to help users like archivists or collection managers to quickly understand which summaries are the most representative for an original video In this paper we present empirical results on the utility of different types of visual explanations to achieve transparency for end users on how representative video summaries are with respect to the original video We consider four types of video summary explanations which use in different ways the concepts extracted from the original video subtitles and the video stream and their prominence The explanations are generated to meet target user preferences and express different dimensions of transparency prominence semantic coverage distance and quantity of coverage In two user studies we evaluate the utility of the visual explanations for achieving transparency for end users Our results show that explanations representing all of the dimensions have the highest utility for transparency\", {\"entities\": [(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53, 'SUCCESS'), (65, 73, 'MEASURE'), (82, 90, 'MEASURE'), (95, 105, 'MEASURE'), (114, 119, 'IDENTITY'), (124, 133, 'SUCCESS'), (194, 203, 'SUCCESS'), (236, 245, 'MEASURE'), (273, 278, 'SUCCESS'), (286, 293, 'MEASURE'), (298, 308, 'MEASURE'), (313, 321, 'MEASURE'), (322, 332, 'MEASURE'), (338, 356, 'PROCESS'), (360, 366, 'SUCCESS'), (391, 400, 'SUCCESS'), (401, 408, 'PROCESS'), (438, 446, 'MEASURE'), (468, 477, 'MEASURE'), (501, 505, 'SUCCESS'), (561, 568, 'SUCCESS'), (596, 608, 'SUCCESS'), (638, 646, 'SUCCESS'), (654, 662, 'MEASURE'), (666, 673, 'SUCCESS'), (719, 725, 'MEASURE'), (803, 809, 'PROCESS'), (831, 840, 'SUCCESS'), (841, 850, 'MEASURE'), (909, 918, 'MEASURE'), (939, 950, 'PROCESS'), (996, 1006, 'PROCESS'), (1007, 1014, 'SUCCESS'), (1029, 1036, 'SUCCESS'), (1059, 1067, 'SUCCESS'), (1120, 1125, 'SUCCESS'), (1126, 1134, 'PROCESS'), (1159, 1168, 'SUCCESS'), (1188, 1195, 'MEASURE'), (1229, 1235, 'MEASURE'), (1247, 1256, 'MEASURE'), (1270, 1285, 'MEASURE'), (1346, 1355, 'SUCCESS'), (1376, 1387, 'PROCESS'), (1391, 1398, 'IDENTITY'), (1423, 1432, 'PROCESS'), (1438, 1445, 'IDENTITY'), (1446, 1451, 'MEASURE'), (1456, 1465, 'MEASURE'), (1484, 1491, 'IDENTITY'), (1492, 1498, 'MEASURE'), (1504, 1513, 'SUCCESS'), (1534, 1542, 'PROCESS'), (1543, 1551, 'IDENTITY'), (1573, 1579, 'SUCCESS'), (1588, 1598, 'SUCCESS'), (1727, 1734, 'MEASURE'), (1743, 1746, 'SUCCESS'), (1755, 1759, 'PROCESS'), (1846, 1854, 'PROCESS'), (1861, 1872, 'PROCESS'), (1979, 1988, 'SUCCESS'), (2022, 2025, 'IDENTITY'), (2049, 2057, 'SUCCESS'), (2068, 2071, 'PROCESS'), (2087, 2090, 'SUCCESS'), (2111, 2123, 'SUCCESS'), (2135, 2138, 'SUCCESS'), (2163, 2166, 'SUCCESS'), (2187, 2189, 'IDENTITY'), (2208, 2210, 'SUCCESS'), (2303, 2304, 'IDENTITY'), (2340, 2348, 'SUCCESS'), (2362, 2369, 'SUCCESS'), (2370, 2375, 'SUCCESS'), (2376, 2382, 'SUCCESS'), (2455, 2465, 'SUCCESS'), (2521, 2523, 'SUCCESS'), (2542, 2551, 'SUCCESS'), (2560, 2568, 'SUCCESS'), (2588, 2590, 'SUCCESS'), (2591, 2602, 'PROCESS'), (2603, 2610, 'SUCCESS'), (2656, 2661, 'SUCCESS'), (2688, 2691, 'IDENTITY'), (2698, 2710, 'SUCCESS'), (2720, 2722, 'SUCCESS'), (2906, 2909, 'SUCCESS'), (2923, 2934, 'PROCESS'), (2983, 2996, 'SUCCESS'), (3051, 3062, 'PROCESS'), (3072, 3075, 'MEASURE'), (3091, 3094, 'PROCESS'), (3129, 3138, 'PROCESS'), (3214, 3216, 'SUCCESS'), (3238, 3241, 'MEASURE'), (3242, 3248, 'MEASURE'), (3264, 3269, 'MEASURE'), (3270, 3274, 'MEASURE'), (3290, 3293, 'SUCCESS'), (3304, 3312, 'PROCESS'), (3391, 3414, 'PROCESS'), (3434, 3440, 'MEASURE'), (3449, 3451, 'PROCESS'), (3469, 3471, 'SUCCESS'), (3480, 3483, 'SUCCESS'), (3521, 3526, 'PROCESS'), (3541, 3564, 'PROCESS'), (3657, 3660, 'PROCESS'), (3668, 3670, 'PROCESS'), (3849, 3852, 'MEASURE'), (3904, 3907, 'MEASURE'), (3953, 3971, 'PROCESS'), (4032, 4035, 'SUCCESS'), (4125, 4129, 'PROCESS'), (4199, 4202, 'IDENTITY'), (4218, 4220, 'PROCESS'), (4238, 4240, 'SUCCESS'), (4267, 4270, 'MEASURE'), (4291, 4295, 'IDENTITY'), (4319, 4322, 'SUCCESS'), (4396, 4402, 'PROCESS'), (4468, 4472, 'IDENTITY'), (4473, 4477, 'PROCESS'), (4504, 4509, 'MEASURE'), (4518, 4521, 'PROCESS'), (4554, 4558, 'SUCCESS'), (4573, 4577, 'SUCCESS'), (4578, 4581, 'IDENTITY'), (4592, 4598, 'MEASURE'), (4626, 4637, 'PROCESS'), (4679, 4685, 'IDENTITY'), (4703, 4707, 'MEASURE'), (4727, 4738, 'IDENTITY'), (4772, 4775, 'MEASURE'), (4776, 4783, 'SUCCESS'), (4816, 4821, 'MEASURE'), (4877, 4889, 'IDENTITY'), (4925, 4927, 'SUCCESS'), (4934, 4936, 'PROCESS'), (4951, 4957, 'IDENTITY'), (5012, 5018, 'MEASURE'), (5068, 5074, 'MEASURE'), (5079, 5083, 'IDENTITY'), (5159, 5164, 'IDENTITY'), (5185, 5191, 'MEASURE'), (5196, 5202, 'PROCESS'), (5221, 5229, 'PROCESS'), (5256, 5266, 'PROCESS'), (5293, 5298, 'IDENTITY'), (5342, 5345, 'PROCESS'), (5392, 5395, 'MEASURE'), (5415, 5419, 'IDENTITY'), (5428, 5432, 'MEASURE'), (5550, 5554, 'IDENTITY'), (5594, 5601, 'PROCESS'), (5602, 5611, 'MEASURE'), (5665, 5669, 'MEASURE'), (5771, 5775, 'PROCESS'), (5798, 5806, 'IDENTITY'), (5815, 5816, 'MEASURE'), (5832, 5843, 'MEASURE'), (5844, 5847, 'SUCCESS'), (5861, 5867, 'SUCCESS'), (5880, 5891, 'MEASURE'), (5918, 5920, 'SUCCESS'), (5994, 6001, 'SUCCESS'), (6040, 6042, 'PROCESS'), (6048, 6060, 'MEASURE'), (6094, 6097, 'SUCCESS'), (6107, 6117, 'PROCESS'), (6140, 6142, 'PROCESS'), (6226, 6230, 'SUCCESS'), (6318, 6321, 'PROCESS'), (6329, 6346, 'PROCESS'), (6412, 6416, 'PROCESS'), (6489, 6494, 'PROCESS'), (6584, 6591, 'PROCESS'), (6604, 6615, 'IDENTITY'), (6679, 6684, 'SUCCESS'), (6712, 6721, 'MEASURE'), (6737, 6747, 'MEASURE'), (6762, 6771, 'MEASURE'), (6785, 6788, 'IDENTITY'), (6806, 6810, 'MEASURE'), (6811, 6817, 'MEASURE'), (6833, 6841, 'MEASURE'), (6842, 6847, 'PROCESS'), (6932, 6938, 'MEASURE'), (6956, 6960, 'IDENTITY'), (6978, 6988, 'IDENTITY'), (7001, 7010, 'SUCCESS'), (7019, 7022, 'SUCCESS'), (7060, 7063, 'MEASURE'), (7097, 7103, 'SUCCESS'), (7145, 7150, 'PROCESS'), (7191, 7193, 'IDENTITY'), (7194, 7203, 'PROCESS'), (7219, 7224, 'SUCCESS'), (7237, 7240, 'SUCCESS'), (7278, 7280, 'SUCCESS'), (7304, 7315, 'MEASURE'), (7323, 7326, 'MEASURE'), (7366, 7370, 'MEASURE'), (7374, 7379, 'IDENTITY'), (7399, 7404, 'IDENTITY'), (7413, 7418, 'MEASURE'), (7428, 7442, 'PROCESS'), (7513, 7523, 'MEASURE'), (7572, 7575, 'MEASURE'), (7600, 7608, 'MEASURE'), (7633, 7634, 'PROCESS'), (7659, 7664, 'IDENTITY'), (7668, 7678, 'IDENTITY'), (7684, 7688, 'MEASURE'), (7700, 7703, 'IDENTITY'), (7707, 7712, 'IDENTITY'), (7719, 7723, 'MEASURE'), (7759, 7764, 'MEASURE'), (7812, 7821, 'MEASURE'), (7846, 7854, 'PROCESS'), (7875, 7878, 'IDENTITY'), (7925, 7932, 'MEASURE'), (8001, 8015, 'PROCESS'), (8030, 8033, 'MEASURE'), (8049, 8063, 'SUCCESS'), (8070, 8078, 'MEASURE'), (8108, 8118, 'IDENTITY'), (8158, 8162, 'PROCESS'), (8178, 8181, 'MEASURE'), (8228, 8235, 'SUCCESS'), (8271, 8274, 'IDENTITY'), (8275, 8279, 'MEASURE'), (8288, 8297, 'PROCESS'), (8324, 8334, 'SUCCESS'), (8344, 8355, 'SUCCESS'), (8360, 8372, 'SUCCESS'), (8383, 8386, 'MEASURE'), (8503, 8505, 'MEASURE'), (8562, 8573, 'PROCESS'), (8577, 8579, 'MEASURE'), (8604, 8611, 'IDENTITY'), (8646, 8648, 'PROCESS'), (8670, 8682, 'MEASURE'), (8697, 8704, 'IDENTITY'), (8733, 8740, 'PROCESS'), (8744, 8753, 'IDENTITY'), (8776, 8778, 'MEASURE'), (8842, 8844, 'MEASURE'), (8878, 8885, 'MEASURE'), (8894, 8896, 'IDENTITY'), (8916, 8918, 'MEASURE'), (8958, 8962, 'MEASURE'), (8969, 8971, 'IDENTITY'), (9043, 9052, 'PROCESS'), (9075, 9084, 'SUCCESS'), (9085, 9088, 'IDENTITY'), (9099, 9104, 'SUCCESS'), (9132, 9136, 'PROCESS'), (9158, 9166, 'MEASURE'), (9181, 9191, 'SUCCESS'), (9238, 9245, 'PROCESS'), (9249, 9253, 'IDENTITY'), (9278, 9290, 'SUCCESS'), (9291, 9293, 'PROCESS'), (9294, 9299, 'SUCCESS'), (9311, 9321, 'MEASURE'), (9353, 9360, 'SUCCESS'), (9361, 9363, 'IDENTITY'), (9364, 9370, 'MEASURE'), (9473, 9480, 'MEASURE'), (9502, 9510, 'IDENTITY'), (9511, 9513, 'PROCESS'), (9514, 9518, 'SUCCESS'), (9551, 9561, 'PROCESS'), (9645, 9654, 'MEASURE'), (9675, 9677, 'MEASURE'), (9700, 9706, 'SUCCESS'), (9707, 9712, 'MEASURE'), (9744, 9750, 'MEASURE'), (9827, 9832, 'MEASURE'), (9843, 9845, 'MEASURE'), (9917, 9923, 'PROCESS'), (9957, 9960, 'PROCESS'), (10004, 10011, 'IDENTITY'), (10054, 10062, 'MEASURE'), (10081, 10085, 'MEASURE'), (10125, 10129, 'MEASURE'), (10168, 10172, 'IDENTITY'), (10174, 10177, 'IDENTITY'), (10178, 10182, 'MEASURE'), (10187, 10189, 'SUCCESS'), (10190, 10192, 'IDENTITY'), (10209, 10211, 'IDENTITY'), (10219, 10226, 'IDENTITY'), (10230, 10240, 'IDENTITY'), (10262, 10274, 'MEASURE'), (10318, 10334, 'MEASURE'), (10335, 10337, 'MEASURE'), (10338, 10342, 'SUCCESS'), (10355, 10358, 'IDENTITY'), (10393, 10400, 'MEASURE'), (10408, 10415, 'MEASURE'), (10437, 10443, 'PROCESS'), (10469, 10476, 'PROCESS'), (10482, 10484, 'PROCESS'), (10485, 10492, 'MEASURE'), (10537, 10542, 'PROCESS'), (10575, 10583, 'MEASURE'), (10584, 10593, 'MEASURE'), (10628, 10640, 'MEASURE'), (10688, 10689, 'MEASURE'), (10716, 10721, 'SUCCESS'), (10782, 10786, 'MEASURE'), (10787, 10798, 'IDENTITY'), (10799, 10801, 'MEASURE'), (10818, 10824, 'MEASURE'), (10828, 10838, 'MEASURE'), (10877, 10884, 'IDENTITY'), (10885, 10892, 'MEASURE'), (10899, 10907, 'IDENTITY'), (10908, 10913, 'IDENTITY'), (10914, 10919, 'IDENTITY'), (10920, 10926, 'IDENTITY'), (11030, 11034, 'PROCESS'), (11035, 11042, 'MEASURE'), (11043, 11053, 'MEASURE'), (11054, 11061, 'MEASURE'), (11065, 11071, 'MEASURE'), (11072, 11075, 'MEASURE'), (11121, 11127, 'MEASURE'), (11128, 11137, 'IDENTITY'), (11177, 11190, 'IDENTITY'), (11226, 11229, 'MEASURE'), (11234, 11240, 'IDENTITY'), (11241, 11244, 'MEASURE'), (11268, 11270, 'PROCESS'), (11291, 11296, 'MEASURE'), (11318, 11320, 'MEASURE'), (11334, 11339, 'IDENTITY'), (11362, 11369, 'PROCESS'), (11414, 11417, 'MEASURE'), (11462, 11464, 'SUCCESS'), (11489, 11492, 'PROCESS'), (11569, 11574, 'IDENTITY'), (11575, 11578, 'MEASURE'), (11592, 11596, 'MEASURE'), (11645, 11648, 'MEASURE'), (11656, 11664, 'IDENTITY'), (11725, 11729, 'MEASURE'), (11733, 11738, 'MEASURE'), (11746, 11749, 'IDENTITY'), (11775, 11788, 'PROCESS'), (11803, 11808, 'PROCESS'), (11819, 11820, 'MEASURE'), (11841, 11844, 'MEASURE'), (11860, 11869, 'MEASURE'), (11870, 11873, 'MEASURE'), (11910, 11923, 'MEASURE'), (11962, 11974, 'MEASURE'), (11996, 12003, 'IDENTITY'), (12031, 12033, 'PROCESS'), (12061, 12079, 'SUCCESS'), (12103, 12117, 'SUCCESS'), (12118, 12122, 'MEASURE'), (12203, 12205, 'MEASURE'), (12243, 12244, 'MEASURE'), (12329, 12338, 'SUCCESS'), (12339, 12345, 'PROCESS'), (12358, 12362, 'PROCESS'), (12403, 12420, 'MEASURE'), (12489, 12493, 'MEASURE'), (12516, 12524, 'PROCESS'), (12554, 12556, 'SUCCESS'), (12560, 12570, 'MEASURE'), (12614, 12622, 'MEASURE'), (12665, 12675, 'IDENTITY'), (12676, 12702, 'MEASURE'), (12759, 12762, 'SUCCESS'), (12763, 12773, 'MEASURE'), (12774, 12778, 'SUCCESS'), (12779, 12792, 'MEASURE'), (12793, 12804, 'SUCCESS'), (12816, 12824, 'SUCCESS'), (12825, 12830, 'MEASURE'), (12844, 12851, 'MEASURE'), (12880, 12885, 'SUCCESS'), (12948, 12959, 'SUCCESS'), (12985, 12990, 'SUCCESS'), (12994, 12997, 'PROCESS'), (13033, 13035, 'SUCCESS'), (13063, 13067, 'SUCCESS'), (13068, 13071, 'MEASURE'), (13082, 13084, 'PROCESS'), (13138, 13145, 'IDENTITY'), (13146, 13148, 'SUCCESS'), (13214, 13216, 'IDENTITY'), (13217, 13218, 'PROCESS'), (13228, 13235, 'MEASURE'), (13244, 13249, 'MEASURE'), (13322, 13327, 'SUCCESS'), (13411, 13430, 'SUCCESS'), (13509, 13523, 'MEASURE'), (13553, 13555, 'SUCCESS'), (13575, 13583, 'PROCESS'), (13588, 13601, 'MEASURE'), (13628, 13636, 'MEASURE'), (13662, 13669, 'MEASURE'), (13690, 13700, 'MEASURE'), (13713, 13718, 'IDENTITY'), (13734, 13744, 'MEASURE'), (13764, 13773, 'MEASURE'), (13782, 13784, 'MEASURE'), (13806, 13810, 'PROCESS'), (13811, 13813, 'MEASURE'), (13846, 13854, 'PROCESS'), (13892, 13894, 'MEASURE'), (13969, 13971, 'MEASURE'), (13972, 13983, 'MEASURE'), (14010, 14023, 'MEASURE'), (14024, 14028, 'MEASURE'), (14029, 14031, 'PROCESS'), (14036, 14042, 'MEASURE'), (14043, 14045, 'SUCCESS'), (14088, 14096, 'MEASURE'), (14111, 14118, 'PROCESS'), (14163, 14177, 'MEASURE'), (14182, 14191, 'IDENTITY'), (14192, 14197, 'MEASURE'), (14212, 14217, 'MEASURE'), (14221, 14229, 'SUCCESS'), (14243, 14250, 'IDENTITY'), (14251, 14253, 'MEASURE'), (14289, 14297, 'MEASURE'), (14298, 14317, 'SUCCESS'), (14333, 14344, 'IDENTITY'), (14354, 14375, 'MEASURE'), (14408, 14419, 'MEASURE'), (14425, 14430, 'MEASURE'), (14454, 14463, 'MEASURE'), (14534, 14541, 'IDENTITY'), (14610, 14628, 'SUCCESS'), (14654, 14656, 'PROCESS'), (14675, 14680, 'MEASURE'), (14702, 14706, 'IDENTITY'), (14707, 14709, 'PROCESS'), (14746, 14757, 'MEASURE'), (14778, 14785, 'MEASURE'), (14851, 14858, 'IDENTITY'), (14859, 14867, 'MEASURE'), (14869, 14871, 'MEASURE'), (14886, 14893, 'MEASURE'), (14912, 14929, 'MEASURE'), (14975, 14997, 'MEASURE'), (15001, 15010, 'MEASURE'), (15056, 15068, 'MEASURE'), (15125, 15135, 'MEASURE'), (15136, 15138, 'MEASURE'), (15145, 15156, 'MEASURE'), (15209, 15211, 'PROCESS'), (15212, 15230, 'SUCCESS'), (15250, 15260, 'MEASURE'), (15351, 15363, 'MEASURE'), (15364, 15368, 'MEASURE'), (15410, 15422, 'SUCCESS'), (15435, 15439, 'MEASURE'), (15523, 15528, 'PROCESS'), (15618, 15620, 'PROCESS'), (15621, 15628, 'MEASURE'), (15638, 15646, 'MEASURE'), (15656, 15658, 'IDENTITY'), (15757, 15767, 'MEASURE'), (15780, 15788, 'MEASURE'), (15836, 15852, 'PROCESS'), (15872, 15879, 'SUCCESS'), (15891, 15894, 'MEASURE'), (15971, 15980, 'PROCESS'), (16123, 16136, 'MEASURE'), (16231, 16240, 'IDENTITY'), (16246, 16268, 'PROCESS'), (16291, 16297, 'MEASURE'), (16324, 16338, 'SUCCESS'), (16358, 16367, 'MEASURE'), (16371, 16384, 'MEASURE'), (16462, 16464, 'MEASURE'), (16486, 16488, 'IDENTITY'), (16504, 16514, 'MEASURE'), (16532, 16535, 'MEASURE'), (16554, 16569, 'MEASURE'), (16589, 16601, 'SUCCESS'), (16612, 16622, 'PROCESS'), (16623, 16625, 'MEASURE'), (16644, 16656, 'SUCCESS'), (16657, 16660, 'SUCCESS'), (16693, 16697, 'PROCESS'), (16698, 16700, 'MEASURE'), (16786, 16794, 'PROCESS'), (16819, 16829, 'MEASURE'), (16877, 16885, 'MEASURE'), (16927, 16930, 'IDENTITY'), (16931, 16934, 'MEASURE'), (16945, 16947, 'MEASURE'), (16948, 16958, 'MEASURE'), (17033, 17047, 'SUCCESS'), (17066, 17075, 'MEASURE'), (17079, 17089, 'MEASURE'), (17090, 17103, 'MEASURE'), (17115, 17122, 'IDENTITY'), (17202, 17206, 'SUCCESS'), (17233, 17246, 'MEASURE'), (17270, 17272, 'SUCCESS'), (17369, 17375, 'SUCCESS'), (17426, 17429, 'MEASURE'), (17441, 17448, 'MEASURE'), (17449, 17456, 'MEASURE'), (17457, 17461, 'MEASURE'), (17605, 17618, 'PROCESS'), (17647, 17656, 'SUCCESS'), (17657, 17680, 'SUCCESS'), (17686, 17695, 'SUCCESS'), (17698, 17706, 'SUCCESS'), (17770, 17772, 'MEASURE'), (17778, 17784, 'IDENTITY'), (17864, 17874, 'MEASURE'), (17921, 17932, 'PROCESS'), (17945, 17957, 'IDENTITY'), (18037, 18054, 'IDENTITY'), (18071, 18082, 'IDENTITY'), (18125, 18133, 'IDENTITY'), (18147, 18150, 'IDENTITY'), (18191, 18212, 'SUCCESS'), (18293, 18316, 'SUCCESS'), (18356, 18363, 'IDENTITY'), (18403, 18408, 'IDENTITY'), (18425, 18427, 'SUCCESS'), (18463, 18472, 'IDENTITY'), (18525, 18529, 'IDENTITY'), (18576, 18582, 'MEASURE'), (18587, 18603, 'MEASURE'), (18617, 18639, 'SUCCESS'), (18736, 18738, 'PROCESS'), (18741, 18745, 'MEASURE'), (18777, 18791, 'PROCESS'), (18804, 18806, 'SUCCESS'), (18821, 18843, 'MEASURE'), (18844, 18845, 'MEASURE'), (18901, 18909, 'SUCCESS'), (18930, 18932, 'MEASURE'), (18988, 18996, 'PROCESS'), (19001, 19006, 'PROCESS'), (19053, 19055, 'MEASURE'), (19116, 19123, 'SUCCESS'), (19124, 19138, 'MEASURE'), (19197, 19205, 'SUCCESS'), (19272, 19279, 'MEASURE'), (19283, 19286, 'IDENTITY'), (19309, 19321, 'PROCESS'), (19339, 19350, 'MEASURE'), (19355, 19360, 'IDENTITY'), (19387, 19389, 'IDENTITY'), (19390, 19402, 'MEASURE'), (19409, 19421, 'MEASURE'), (19458, 19460, 'PROCESS'), (19466, 19468, 'MEASURE'), (19567, 19569, 'PROCESS'), (19576, 19584, 'MEASURE'), (19636, 19647, 'MEASURE'), (19729, 19737, 'MEASURE'), (19753, 19761, 'MEASURE'), (19762, 19764, 'IDENTITY'), (19783, 19786, 'MEASURE'), (19794, 19806, 'IDENTITY'), (19860, 19869, 'PROCESS'), (19873, 19875, 'MEASURE'), (19883, 19899, 'SUCCESS'), (19900, 19913, 'SUCCESS'), (19973, 19979, 'IDENTITY'), (20005, 20010, 'PROCESS'), (20011, 20023, 'MEASURE'), (20043, 20052, 'SUCCESS'), (20074, 20081, 'SUCCESS'), (20107, 20115, 'SUCCESS'), (20116, 20124, 'IDENTITY'), (20129, 20136, 'SUCCESS'), (20137, 20146, 'MEASURE'), (20200, 20211, 'SUCCESS'), (20223, 20229, 'SUCCESS'), (20230, 20255, 'MEASURE'), (20272, 20276, 'IDENTITY'), (20349, 20354, 'SUCCESS'), (20355, 20368, 'IDENTITY'), (20369, 20371, 'PROCESS'), (20399, 20408, 'MEASURE'), (20409, 20412, 'MEASURE'), (20413, 20420, 'MEASURE'), (20452, 20471, 'IDENTITY'), (20556, 20572, 'PROCESS'), (20612, 20621, 'IDENTITY'), (20660, 20662, 'IDENTITY'), (20691, 20693, 'MEASURE'), (20759, 20771, 'IDENTITY'), (20782, 20795, 'SUCCESS'), (20796, 20803, 'SUCCESS'), (20812, 20817, 'SUCCESS'), (20827, 20838, 'SUCCESS'), (20885, 20900, 'MEASURE'), (20901, 20911, 'MEASURE'), (20972, 20982, 'SUCCESS'), (21002, 21028, 'MEASURE'), (21032, 21036, 'PROCESS'), (21044, 21048, 'MEASURE'), (21084, 21094, 'PROCESS'), (21144, 21149, 'IDENTITY'), (21180, 21188, 'SUCCESS'), (21225, 21228, 'PROCESS'), (21229, 21240, 'MEASURE'), (21257, 21268, 'IDENTITY'), (21328, 21336, 'MEASURE'), (21373, 21374, 'IDENTITY'), (21375, 21390, 'SUCCESS'), (21391, 21401, 'SUCCESS'), (21472, 21483, 'SUCCESS'), (21524, 21527, 'SUCCESS'), (21548, 21551, 'SUCCESS'), (21557, 21559, 'MEASURE'), (21636, 21640, 'IDENTITY'), (21647, 21660, 'PROCESS'), (21713, 21726, 'MEASURE'), (21836, 21845, 'PROCESS'), (21846, 21857, 'MEASURE'), (21897, 21901, 'IDENTITY'), (21924, 21929, 'PROCESS'), (21945, 21953, 'MEASURE'), (21984, 22008, 'MEASURE'), (22016, 22021, 'MEASURE'), (22097, 22120, 'SUCCESS'), (22132, 22136, 'IDENTITY'), (22160, 22175, 'MEASURE'), (22225, 22250, 'IDENTITY'), (22251, 22258, 'IDENTITY'), (22295, 22306, 'MEASURE'), (22312, 22326, 'MEASURE'), (22476, 22482, 'MEASURE'), (22494, 22508, 'MEASURE'), (22547, 22553, 'SUCCESS'), (22554, 22568, 'MEASURE'), (22585, 22593, 'SUCCESS'), (22626, 22640, 'MEASURE'), (22724, 22737, 'PROCESS'), (22765, 22783, 'MEASURE'), (22784, 22786, 'MEASURE'), (22793, 22804, 'IDENTITY'), (22821, 22836, 'IDENTITY'), (22891, 22897, 'MEASURE'), (22923, 22930, 'MEASURE'), (22946, 22948, 'IDENTITY'), (22961, 22967, 'PROCESS'), (23050, 23062, 'SUCCESS'), (23063, 23065, 'IDENTITY'), (23089, 23104, 'MEASURE'), (23224, 23236, 'SUCCESS'), (23302, 23305, 'MEASURE'), (23311, 23316, 'IDENTITY'), (23317, 23325, 'MEASURE'), (23400, 23412, 'SUCCESS'), (23416, 23424, 'SUCCESS'), (23431, 23441, 'SUCCESS'), (23460, 23471, 'SUCCESS'), (23472, 23479, 'IDENTITY'), (23485, 23492, 'SUCCESS'), (23493, 23497, 'MEASURE'), (23507, 23510, 'IDENTITY'), (23521, 23524, 'MEASURE'), (23528, 23535, 'PROCESS'), (23536, 23547, 'MEASURE'), (23661, 23663, 'SUCCESS'), (23742, 23759, 'SUCCESS'), (23764, 23776, 'IDENTITY'), (23950, 23959, 'IDENTITY'), (24043, 24045, 'SUCCESS'), (24071, 24073, 'IDENTITY'), (24074, 24076, 'MEASURE'), (24077, 24080, 'MEASURE'), (24081, 24095, 'MEASURE'), (24096, 24098, 'MEASURE'), (24099, 24108, 'MEASURE'), (24136, 24138, 'MEASURE'), (24224, 24233, 'MEASURE'), (24255, 24264, 'SUCCESS'), (24276, 24279, 'SUCCESS'), (24295, 24302, 'MEASURE'), (24312, 24318, 'IDENTITY'), (24334, 24345, 'SUCCESS'), (24406, 24419, 'PROCESS'), (24434, 24442, 'IDENTITY'), (24558, 24563, 'PROCESS'), (24579, 24583, 'SUCCESS'), (24584, 24592, 'IDENTITY'), (24662, 24676, 'IDENTITY'), (24729, 24734, 'IDENTITY'), (24739, 24747, 'IDENTITY'), (24777, 24780, 'MEASURE'), (24875, 24881, 'SUCCESS'), (24895, 24899, 'IDENTITY'), (24956, 24966, 'SUCCESS'), (24967, 24977, 'IDENTITY'), (25058, 25060, 'SUCCESS'), (25085, 25094, 'MEASURE'), (25118, 25120, 'SUCCESS'), (25145, 25156, 'MEASURE'), (25191, 25202, 'IDENTITY'), (25255, 25260, 'PROCESS'), (25268, 25274, 'MEASURE'), (25408, 25411, 'PROCESS'), (25467, 25469, 'MEASURE'), (25481, 25486, 'MEASURE'), (25658, 25666, 'MEASURE'), (25721, 25733, 'MEASURE'), (25734, 25737, 'SUCCESS'), (25899, 25918, 'SUCCESS'), (25922, 25926, 'MEASURE'), (25933, 25941, 'MEASURE'), (25954, 25958, 'MEASURE'), (26033, 26038, 'IDENTITY'), (26062, 26075, 'MEASURE'), (26076, 26079, 'MEASURE'), (26131, 26150, 'IDENTITY'), (26168, 26175, 'IDENTITY'), (26196, 26204, 'MEASURE'), (26223, 26228, 'MEASURE'), (26299, 26304, 'SUCCESS'), (26318, 26327, 'MEASURE'), (26332, 26342, 'IDENTITY'), (26377, 26390, 'PROCESS'), (26454, 26456, 'MEASURE'), (26479, 26487, 'MEASURE'), (26523, 26530, 'MEASURE'), (26552, 26556, 'IDENTITY'), (26564, 26575, 'PROCESS'), (26593, 26595, 'MEASURE'), (26657, 26664, 'MEASURE'), (26717, 26719, 'IDENTITY'), (26759, 26774, 'MEASURE'), (26782, 26785, 'MEASURE'), (26805, 26819, 'MEASURE'), (26907, 26911, 'MEASURE'), (26934, 26943, 'MEASURE'), (26994, 27012, 'SUCCESS'), (27013, 27017, 'SUCCESS'), (27018, 27020, 'SUCCESS'), (27021, 27027, 'SUCCESS'), (27069, 27073, 'SUCCESS'), (27096, 27105, 'SUCCESS'), (27106, 27119, 'MEASURE'), (27139, 27141, 'SUCCESS'), (27149, 27160, 'MEASURE'), (27189, 27196, 'IDENTITY'), (27204, 27207, 'IDENTITY'), (27208, 27220, 'SUCCESS'), (27253, 27263, 'MEASURE'), (27294, 27299, 'SUCCESS'), (27364, 27372, 'MEASURE'), (27398, 27400, 'MEASURE'), (27445, 27447, 'SUCCESS'), (27497, 27505, 'SUCCESS'), (27512, 27514, 'SUCCESS'), (27533, 27535, 'MEASURE'), (27542, 27546, 'MEASURE'), (27557, 27572, 'PROCESS'), (27656, 27662, 'SUCCESS'), (27729, 27730, 'PROCESS'), (27738, 27742, 'PROCESS'), (27743, 27751, 'MEASURE'), (27762, 27774, 'MEASURE'), (27810, 27821, 'PROCESS'), (27830, 27832, 'MEASURE'), (27861, 27862, 'PROCESS'), (27887, 27900, 'MEASURE'), (27935, 27945, 'MEASURE'), (27951, 27954, 'MEASURE'), (27965, 27978, 'IDENTITY'), (28064, 28079, 'SUCCESS'), (28080, 28086, 'MEASURE'), (28087, 28094, 'MEASURE'), (28102, 28110, 'IDENTITY'), (28184, 28196, 'SUCCESS'), (28212, 28217, 'MEASURE'), (28222, 28234, 'SUCCESS'), (28235, 28237, 'IDENTITY'), (28294, 28297, 'MEASURE'), (28316, 28320, 'PROCESS'), (28327, 28340, 'SUCCESS'), (28351, 28361, 'MEASURE'), (28362, 28372, 'IDENTITY'), (28444, 28459, 'SUCCESS'), (28467, 28470, 'IDENTITY'), (28488, 28491, 'MEASURE'), (28532, 28545, 'SUCCESS'), (28551, 28557, 'PROCESS'), (28562, 28564, 'PROCESS'), (28570, 28579, 'MEASURE'), (28585, 28594, 'IDENTITY'), (28609, 28618, 'IDENTITY'), (28650, 28660, 'SUCCESS'), (28703, 28708, 'SUCCESS'), (28722, 28739, 'IDENTITY'), (28747, 28750, 'MEASURE'), (28793, 28805, 'MEASURE'), (28835, 28840, 'PROCESS'), (28850, 28853, 'IDENTITY'), (28858, 28864, 'MEASURE'), (28875, 28884, 'SUCCESS'), (28929, 28935, 'PROCESS'), (28979, 28997, 'SUCCESS'), (29009, 29020, 'MEASURE'), (29036, 29045, 'MEASURE'), (29070, 29075, 'PROCESS'), (29100, 29107, 'MEASURE'), (29180, 29183, 'SUCCESS'), (29198, 29199, 'IDENTITY'), (29211, 29216, 'IDENTITY'), (29224, 29230, 'MEASURE'), (29239, 29247, 'PROCESS'), (29287, 29292, 'SUCCESS'), (29293, 29306, 'MEASURE'), (29358, 29360, 'IDENTITY'), (29382, 29387, 'SUCCESS'), (29405, 29406, 'SUCCESS'), (29407, 29413, 'MEASURE'), (29419, 29440, 'MEASURE'), (29464, 29466, 'SUCCESS'), (29467, 29479, 'SUCCESS'), (29493, 29494, 'MEASURE'), (29509, 29511, 'MEASURE'), (29521, 29533, 'MEASURE'), (29556, 29565, 'IDENTITY'), (29589, 29592, 'MEASURE'), (29666, 29669, 'PROCESS'), (29818, 29824, 'MEASURE'), (29831, 29844, 'SUCCESS'), (29880, 29888, 'SUCCESS'), (29900, 29902, 'IDENTITY'), (30008, 30009, 'SUCCESS'), (30017, 30023, 'PROCESS'), (30035, 30038, 'PROCESS'), (30066, 30081, 'MEASURE'), (30096, 30101, 'MEASURE'), (30125, 30128, 'MEASURE'), (30139, 30142, 'MEASURE'), (30173, 30180, 'MEASURE'), (30275, 30286, 'SUCCESS'), (30287, 30293, 'IDENTITY'), (30302, 30312, 'IDENTITY'), (30321, 30332, 'IDENTITY'), (30346, 30351, 'PROCESS'), (30373, 30380, 'MEASURE'), (30472, 30476, 'MEASURE'), (30508, 30513, 'MEASURE'), (30514, 30516, 'MEASURE'), (30522, 30527, 'SUCCESS'), (30528, 30530, 'MEASURE'), (30549, 30556, 'IDENTITY'), (30564, 30571, 'MEASURE'), (30723, 30728, 'MEASURE'), (30842, 30850, 'SUCCESS'), (30851, 30856, 'MEASURE'), (30857, 30866, 'IDENTITY'), (30867, 30870, 'MEASURE'), (30875, 30880, 'MEASURE'), (30888, 30891, 'MEASURE'), (30898, 30908, 'MEASURE'), (30913, 30929, 'MEASURE'), (30930, 30939, 'MEASURE'), (30943, 30947, 'MEASURE'), (30976, 30983, 'IDENTITY'), (31021, 31031, 'MEASURE'), (31072, 31074, 'SUCCESS'), (31075, 31083, 'MEASURE'), (31091, 31095, 'IDENTITY'), (31128, 31130, 'PROCESS'), (31155, 31158, 'MEASURE'), (31182, 31185, 'SUCCESS'), (31186, 31189, 'MEASURE'), (31196, 31199, 'MEASURE')]}]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc0bGR_et0ly",
        "outputId": "19e1fe7c-f31d-444e-ec6d-2d5b65481aa6"
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfGCBkeOEz6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ceabcf4-7ff6-41c3-ac06-289a884d292c"
      },
      "source": [
        "import random\n",
        "from spacy.training.example import Example\n",
        "iterations = 1\n",
        "# def train_spacy(data, annotations):\n",
        "#   train_data2 = data\n",
        "nlp = spacy.blank('en') # create blank Language class\n",
        "# add labels\n",
        "# print(train_data)\n",
        "entities_list = train_data[0][\"entities\"]\n",
        "# print(entities_list)\n",
        "# print(ntts_list)\n",
        "# for ent in entities_list:\n",
        "#   # ner.add_label(ent[2])\n",
        "#   parser.add_label(ent[2])\n",
        "#   print('NER', ner)\n",
        "# get names of other pipes to disable them during training\n",
        "# other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "# with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "optimizer = nlp.begin_training()\n",
        "for itn in range(iterations):\n",
        "    # print(\"Starting iteration \" + str(itn))\n",
        "    # random.shuffle(train_data[0])\n",
        "    losses = {}\n",
        "    # batch = spacy.util.minibatch(train_data, size=1000)\n",
        "    for ent in entities_list:\n",
        "      # print(train_data)\n",
        "      text = train_data[1][ent[0]:ent[1]]\n",
        "      # print(text)\n",
        "      doc = nlp.make_doc(text)\n",
        "      print(\"text:\", text, \"doc:\", doc)\n",
        "      # annotation = ent[2]\n",
        "      example = Example.from_dict(doc, annotations[1])\n",
        "      nlp.update([example], losses=losses, drop=0.3)\n",
        "      # doc = nlp(text)\n",
        "    # for ent in doc.ents:\n",
        "    #       print(ent.text, ent.label_)\n",
        "\n",
        "\n",
        "        # # create Example\n",
        "        #     doc = nlp.make_doc(text)\n",
        "        #   example = Example.from_dict(doc, annotations)\n",
        "        # # Update the model\n",
        "        # nlp.update([example], losses=losses, drop=0.3)\n",
        "\n",
        "# for text, _ in train_data:\n",
        "        # doc = nlp(text)\n",
        "        # print(\"annotations\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: goals doc: goals\n",
            "text: engineering doc: engineering\n",
            "text: achieve doc: achieve\n",
            "text: tradeoff doc: tradeoff\n",
            "text: fairness doc: fairness\n",
            "text: throughput doc: throughput\n",
            "text: users doc: users\n",
            "text: satisfied doc: satisfied\n",
            "text: satisfied doc: satisfied\n",
            "text: resources doc: resources\n",
            "text: novel doc: novel\n",
            "text: balance doc: balance\n",
            "text: throughput doc: throughput\n",
            "text: fairness doc: fairness\n",
            "text: objectives doc: objectives\n",
            "text: linear programming doc: linear programming\n",
            "text: allows doc: allows\n",
            "text: precisely doc: precisely\n",
            "text: control doc: control\n",
            "text: fairness doc: fairness\n",
            "text: commodity doc: commodity\n",
            "text: fair doc: fair\n",
            "text: optimal doc: optimal\n",
            "text: improvements doc: improvements\n",
            "text: achieves doc: achieves\n",
            "text: fairness doc: fairness\n",
            "text: solving doc: solving\n",
            "text: reduce doc: reduce\n",
            "text: extend doc: extend\n",
            "text: important doc: important\n",
            "text: practical doc: practical\n",
            "text: functions doc: functions\n",
            "text: experiments doc: experiments\n",
            "text: algorithms doc: algorithms\n",
            "text: achieve doc: achieve\n",
            "text: speedup doc: speedup\n",
            "text: insights doc: insights\n",
            "text: rapid doc: rapid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"linear programming\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"improvements\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"medical records\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: adoption doc: adoption\n",
            "text: increased doc: increased\n",
            "text: privacy doc: privacy\n",
            "text: models doc: models\n",
            "text: sensitive doc: sensitive\n",
            "text: medical records doc: medical records\n",
            "text: promising doc: promising\n",
            "text: Aggregation doc: Aggregation\n",
            "text: Teacher doc: Teacher\n",
            "text: transfers doc: transfers\n",
            "text: student doc: student\n",
            "text: model doc: model\n",
            "text: knowledge doc: knowledge\n",
            "text: teacher doc: teacher\n",
            "text: models doc: models\n",
            "text: intuitive doc: intuitive\n",
            "text: training doc: training\n",
            "text: teachers doc: teachers\n",
            "text: strong doc: strong\n",
            "text: guaranteed doc: guaranteed\n",
            "text: leaving doc: leaving\n",
            "text: its doc: its\n",
            "text: when doc: when\n",
            "text: PATE can doc: PATE can\n",
            "text: to learning doc: to learning\n",
            "text: introduce doc: introduce\n",
            "text: for doc: for\n",
            "text: are more doc: are more\n",
            "text: and doc: and\n",
            "text: and doc: and\n",
            "text: differential doc: differential\n",
            "text: Our doc: Our\n",
            "text: two doc: two\n",
            "text: of doc: of\n",
            "text: is doc: is\n",
            "text: a doc: a\n",
            "text: are more doc: are more\n",
            "text: correct doc: correct\n",
            "text: offer doc: offer\n",
            "text: better doc: better\n",
            "text: mechanisms doc: mechanisms\n",
            "text: to doc: to\n",
            "text: both high doc: both high\n",
            "text: and very doc: and very\n",
            "text: we doc: we\n",
            "text: efficiently doc: efficiently\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"differential\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"and ensuring\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"approximation\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"risksensitive imitation\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: extract doc: extract\n",
            "text: while doc: while\n",
            "text: the doc: the\n",
            "text: and ensuring doc: and ensuring\n",
            "text: in doc: in\n",
            "text: the doc: the\n",
            "text: centralized doc: centralized\n",
            "text: approximation doc: approximation\n",
            "text: extensively doc: extensively\n",
            "text: the doc: the\n",
            "text: our doc: our\n",
            "text: realworld doc: realworld\n",
            "text: ii doc: ii\n",
            "text: for doc: for\n",
            "text: income doc: income\n",
            "text: crime doc: crime\n",
            "text: rate doc: rate\n",
            "text: iii doc: iii\n",
            "text: deletion doc: deletion\n",
            "text: risksensitive imitation doc: risksensitive imitation\n",
            "text: agents doc: agents\n",
            "text: to doc: to\n",
            "text: as doc: as\n",
            "text: the doc: the\n",
            "text: first doc: first\n",
            "text: risksensitive imitation doc: risksensitive imitation\n",
            "text: and doc: and\n",
            "text: an doc: an\n",
            "text: the doc: the\n",
            "text: and doc: and\n",
            "text: imitation learning doc: imitation learning\n",
            "text: the doc: the\n",
            "text: RAIL doc: RAIL\n",
            "text: the doc: the\n",
            "text: of doc: of\n",
            "text: it doc: it\n",
            "text: the doc: the\n",
            "text: they doc: they\n",
            "text: use doc: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"imitation learning\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"preferred by\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use\n",
            "text: biases doc: biases\n",
            "text: this doc: this\n",
            "text: post doc: post\n",
            "text: forms doc: forms\n",
            "text: and doc: and\n",
            "text: well doc: well\n",
            "text: bias doc: bias\n",
            "text: How doc: How\n",
            "text: decide doc: decide\n",
            "text: definitions doc: definitions\n",
            "text: answer doc: answer\n",
            "text: must doc: must\n",
            "text: preferences doc: preferences\n",
            "text: ing doc: ing\n",
            "text: systems doc: systems\n",
            "text: treat doc: treat\n",
            "text: preferred by doc: preferred by\n",
            "text: it doc: it\n",
            "text: be doc: be\n",
            "text: elicit doc: elicit\n",
            "text: priori doc: priori\n",
            "text: asking doc: asking\n",
            "text: self doc: self\n",
            "text: often doc: often\n",
            "text: stated doc: stated\n",
            "text: actual doc: actual\n",
            "text: outlines doc: outlines\n",
            "text: perimental doc: perimental\n",
            "text: these doc: these\n",
            "text: are doc: are\n",
            "text: for doc: for\n",
            "text: told doc: told\n",
            "text: that doc: that\n",
            "text: same doc: same\n",
            "text: payment doc: payment\n",
            "text: structure doc: structure\n",
            "text: tied doc: tied\n",
            "text: that doc: that\n",
            "text: relation doc: relation\n",
            "text: a doc: a\n",
            "text: preferences doc: preferences\n",
            "text: and doc: and\n",
            "text: stated doc: stated\n",
            "text: Connections doc: Connections\n",
            "text: in doc: in\n",
            "text: Descent doc: Descent\n",
            "text: in doc: in\n",
            "text: applications doc: applications\n",
            "text: Two doc: Two\n",
            "text: approaches doc: approaches\n",
            "text: by doc: by\n",
            "text: than doc: than\n",
            "text: set doc: set\n",
            "text: uniformly sampled doc: uniformly sampled\n",
            "text: when doc: when\n",
            "text: paper doc: paper\n",
            "text: wherein doc: wherein\n",
            "text: distributed doc: distributed\n",
            "text: check doc: check\n",
            "text: crucially doc: crucially\n",
            "text: randomized doc: randomized\n",
            "text: decisions doc: decisions\n",
            "text: and doc: and\n",
            "text: each doc: each\n",
            "text: client doc: client\n",
            "text: accuracy doc: accuracy\n",
            "text: trade doc: trade\n",
            "text: server doc: server\n",
            "text: even doc: even\n",
            "text: population doc: population\n",
            "text: knowledge doc: knowledge\n",
            "text: the doc: the\n",
            "text: for doc: for\n",
            "text: and it doc: and it\n",
            "text: Along doc: Along\n",
            "text: by doc: by\n",
            "text: shuffling doc: shuffling\n",
            "text: local doc:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"applications\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"uniformly sampled\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"counterfactual\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " local\n",
            "text: and doc: and\n",
            "text: In doc: In\n",
            "text: improvement doc: improvement\n",
            "text: for doc: for\n",
            "text: from doc: from\n",
            "text: order doc: order\n",
            "text: users doc: users\n",
            "text: paper doc: paper\n",
            "text: counterfactual doc: counterfactual\n",
            "text: prediction doc: prediction\n",
            "text: the doc: the\n",
            "text: Toxicity doc: Toxicity\n",
            "text: a doc: a\n",
            "text: issue doc: issue\n",
            "text: predicting doc: predicting\n",
            "text: Some doc: Some\n",
            "text: gay doc: gay\n",
            "text: toxic doc: toxic\n",
            "text: Some doc: Some\n",
            "text: offer doc: offer\n",
            "text: measuring doc: measuring\n",
            "text: fairness doc: fairness\n",
            "text: and doc: and\n",
            "text: Further doc: Further\n",
            "text: counterfactual doc: counterfactual\n",
            "text: CLP doc: CLP\n",
            "text: counterfactual doc: counterfactual\n",
            "text: fairness doc: fairness\n",
            "text: robustness doc: robustness\n",
            "text: find doc: find\n",
            "text: and doc: and\n",
            "text: methods doc: methods\n",
            "text: and doc: and\n",
            "text: have doc: have\n",
            "text: tradeoffs doc: tradeoffs\n",
            "text: approaches doc: approaches\n",
            "text: measurement doc: measurement\n",
            "text: optimization doc: optimization\n",
            "text: new doc: new\n",
            "text: in doc: in\n",
            "text: application doc: application\n",
            "text: ML doc: ML\n",
            "text: similar doc: similar\n",
            "text: eg doc: eg\n",
            "text: to reference doc: to reference\n",
            "text: medical doc: medical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"optimization\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"to reference\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"pathologists\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: However doc: However\n",
            "text: algorithm doc: algorithm\n",
            "text: an doc: an\n",
            "text: is doc: is\n",
            "text: similar doc: similar\n",
            "text: be doc: be\n",
            "text: to doc: to\n",
            "text: this doc: this\n",
            "text: we doc: we\n",
            "text: retrieved doc: retrieved\n",
            "text: algorithm doc: algorithm\n",
            "text: and doc: and\n",
            "text: tools doc: tools\n",
            "text: with doc: with\n",
            "text: onthefly doc: onthefly\n",
            "text: what types doc: what types\n",
            "text: moments doc: moments\n",
            "text: time doc: time\n",
            "text: pathologists doc: pathologists\n",
            "text: we doc: we\n",
            "text: found doc: found\n",
            "text: refinement doc: refinement\n",
            "text: utility doc: utility\n",
            "text: of doc: of\n",
            "text: images doc: images\n",
            "text: without doc: without\n",
            "text: accuracy doc: accuracy\n",
            "text: We doc: We\n",
            "text: also doc: also\n",
            "text: strategies doc: strategies\n",
            "text: algorithm doc: algorithm\n",
            "text: ML doc: ML\n",
            "text: errors doc: errors\n",
            "text: Taken doc: Taken\n",
            "text: future doc: future\n",
            "text: often doc: often\n",
            "text: an doc: an\n",
            "text: inputs doc: inputs\n",
            "text: new doc: new\n",
            "text: systems doc: systems\n",
            "text: patterns doc: patterns\n",
            "text: have doc: have\n",
            "text: that doc: that\n",
            "text: data doc: data\n",
            "text: but doc: but\n",
            "text: also doc: also\n",
            "text: do doc: do\n",
            "text: we doc: we\n",
            "text: to doc: to\n",
            "text: people? doc: people?\n",
            "text: particular doc: particular\n",
            "text: marginalized doc: marginalized\n",
            "text: underrepresented doc: underrepresented\n",
            "text: in doc: in\n",
            "text: data doc: data\n",
            "text: may doc: may\n",
            "text: on such doc: on such\n",
            "text: As such doc: As such\n",
            "text: equity doc: equity\n",
            "text: require doc: require\n",
            "text: we doc: we\n",
            "text: develop doc: develop\n",
            "text: harms doc: harms\n",
            "text: directly doc: directly\n",
            "text: addresses doc: addresses\n",
            "text: human values doc: human values\n",
            "text: a doc: a\n",
            "text: often doc: often\n",
            "text: node doc: node\n",
            "text: activations doc: activations\n",
            "text: of doc: of\n",
            "text: relate doc: relate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"marginalized\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"underrepresented\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"human values\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"organizations\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: valueladen doc: valueladen\n",
            "text: abusive doc: abusive\n",
            "text: as well doc: as well\n",
            "text: concepts doc: concepts\n",
            "text: about doc: about\n",
            "text: human doc: human\n",
            "text: social doc: social\n",
            "text: with doc: with\n",
            "text: Concept doc: Concept\n",
            "text: Activation doc: Activation\n",
            "text: Vectors doc: Vectors\n",
            "text: models doc: models\n",
            "text: for doc: for\n",
            "text: gender doc: gender\n",
            "text: diversity doc: diversity\n",
            "text: organizations doc: organizations\n",
            "text: men doc: men\n",
            "text: reason doc: reason\n",
            "text: for doc: for\n",
            "text: is doc: is\n",
            "text: paper doc: paper\n",
            "text: of doc: of\n",
            "text: using doc: using\n",
            "text: derived doc: derived\n",
            "text: the doc: the\n",
            "text: by doc: by\n",
            "text: how doc: how\n",
            "text: While doc: While\n",
            "text: our doc: our\n",
            "text: that doc: that\n",
            "text: the doc: the\n",
            "text: platform doc: platform\n",
            "text: work doc: work\n",
            "text: fewer doc: fewer\n",
            "text: and doc: and\n",
            "text: communication doc: communication\n",
            "text: paper doc: paper\n",
            "text: a doc: a\n",
            "text: for doc: for\n",
            "text: assessing doc: assessing\n",
            "text: the doc: the\n",
            "text: in generative doc: in generative\n",
            "text: network Such doc: network Such\n",
            "text: trained doc: trained\n",
            "text: of doc: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"communication\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"in generative\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"network Such\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"methodology allows\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"configurations\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"extracting unique\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"emailcomposition assistant\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"datadependent\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "of\n",
            "text: methodology allows doc: methodology allows\n",
            "text: configurations doc: configurations\n",
            "text: that doc: that\n",
            "text: we doc: we\n",
            "text: a doc: a\n",
            "text: addressed doc: addressed\n",
            "text: during doc: during\n",
            "text: show doc: show\n",
            "text: extracting unique doc: extracting unique\n",
            "text: also doc: also\n",
            "text: strategy doc: strategy\n",
            "text: eg doc: eg\n",
            "text: describing doc: describing\n",
            "text: exposure doc: exposure\n",
            "text: predictive doc: predictive\n",
            "text: emailcomposition assistant doc: emailcomposition assistant\n",
            "text: can doc: can\n",
            "text: be trained doc: be trained\n",
            "text: with doc: with\n",
            "text: datadependent doc: datadependent\n",
            "text: constraints doc: constraints\n",
            "text: fairness doc: fairness\n",
            "text: goals doc: goals\n",
            "text: achieve doc: achieve\n",
            "text: other doc: other\n",
            "text: constrained doc: constrained\n",
            "text: terms doc: terms\n",
            "text: how doc: how\n",
            "text: at doc: at\n",
            "text: they doc: they\n",
            "text: are doc: are\n",
            "text: at doc: at\n",
            "text: problem doc: problem\n",
            "text: as doc: as\n",
            "text: on doc: on\n",
            "text: a doc: a\n",
            "text: dataset doc: dataset\n",
            "text: other doc: other\n",
            "text: build doc: build\n",
            "text: twodataset approach doc: twodataset approach\n",
            "text: experimentally doc: experimentally\n",
            "text: in doc: in\n",
            "text: practice doc: practice\n",
            "text: potential for doc: potential for\n",
            "text: existing doc: existing\n",
            "text: broadly doc: broadly\n",
            "text: classifier doc: classifier\n",
            "text: which doc: which\n",
            "text: metrics of doc: metrics of\n",
            "text: subgroups doc: subgroups\n",
            "text: on doc: on\n",
            "text: such doc: such\n",
            "text: as doc: as\n",
            "text: inequity doc: inequity\n",
            "text: in doc: in\n",
            "text: of doc: of\n",
            "text: performance doc: performance\n",
            "text: unintentional doc: unintentional\n",
            "text: bias doc: bias\n",
            "text: In doc: In\n",
            "text: spirit doc: spirit\n",
            "text: of doc: of\n",
            "text: societal doc: societal\n",
            "text: explore doc: explore\n",
            "text: PEF identifies doc: PEF identifies\n",
            "text: operating doc: operating\n",
            "text: point doc: point\n",
            "text: curve doc: curve\n",
            "text: subgroup doc: subgroup\n",
            "text: closest doc: closest\n",
            "text: to doc: to\n",
            "text: multiple doc: multiple\n",
            "text: subgroup accuracies doc: subgroup accuracies\n",
            "text: demonstrate doc: demonstrate\n",
            "text: increases performance doc: increases performance\n",
            "text: datasets In doc: datasets In\n",
            "text: paper doc: paper\n",
            "text: calls for doc: calls for\n",
            "text: machine doc: machine\n",
            "text: paper that machine doc: paper that machine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"twodataset approach\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"experimentally\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"potential for\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"unintentional\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"PEF identifies\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"subgroup accuracies\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"increases performance\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"paper that machine\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: of doc: of\n",
            "text: makes doc: makes\n",
            "text: kind doc: kind\n",
            "text: of doc: of\n",
            "text: rationalize doc: rationalize\n",
            "text: seeming doc: seeming\n",
            "text: machine doc: machine\n",
            "text: learning doc: learning\n",
            "text: We doc: We\n",
            "text: general doc: general\n",
            "text: adopt reliabilism doc: adopt reliabilism\n",
            "text: epistemological theory doc: epistemological theory\n",
            "text: epistemic doc: epistemic\n",
            "text: warranted if doc: warranted if\n",
            "text: argue that doc: argue that\n",
            "text: in doc: in\n",
            "text: where model doc: where model\n",
            "text: is doc: is\n",
            "text: not sufficient and doc: not sufficient and\n",
            "text: deployment doc: deployment\n",
            "text: then suggest doc: then suggest\n",
            "text: that doc: that\n",
            "text: consequences doc: consequences\n",
            "text: does doc: does\n",
            "text: offer doc: offer\n",
            "text: of doc: of\n",
            "text: machine doc: machine\n",
            "text: We study doc: We study\n",
            "text: of doc: of\n",
            "text: difficulty doc: difficulty\n",
            "text: language doc: language\n",
            "text: microaggressions doc: microaggressions\n",
            "text: further doc: further\n",
            "text: the doc: the\n",
            "text: introduce "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"adopt reliabilism\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"epistemological theory\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"warranted if\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"not sufficient and\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"then suggest\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"consequences\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"microaggressions\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"we finetune a\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"collecting annotations\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"use interrater\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"data goodness\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc: introduce\n",
            "text: we finetune a doc: we finetune a\n",
            "text: baselines doc: baselines\n",
            "text: collecting annotations doc: collecting annotations\n",
            "text: humans doc: humans\n",
            "text: use interrater doc: use interrater\n",
            "text: a measure doc: a measure\n",
            "text: data goodness doc: data goodness\n",
            "text: to doc: to\n",
            "text: of doc: of\n",
            "text: thresholds doc: thresholds\n",
            "text: for doc: for\n",
            "text: from annotators doc: from annotators\n",
            "text: and training doc: and training\n",
            "text: especially doc: especially\n",
            "text: on doc: on\n",
            "text: We present a doc: We present a\n",
            "text: new doc: new\n",
            "text: that doc: that\n",
            "text: is doc: is\n",
            "text: measures doc: measures\n",
            "text: which is a doc: which is a\n",
            "text: based on doc: based on\n",
            "text: the doc: the\n",
            "text: xRR doc: xRR\n",
            "text: We doc: We\n",
            "text: opensource doc: opensource\n",
            "text: and analyze it doc: and analyze it\n",
            "text: framework doc: framework\n",
            "text: argue this doc: argue this\n",
            "text: framework can doc: framework can\n",
            "text: measure doc: measure\n",
            "text: with doc: with\n",
            "text: submitted for doc: submitted for\n",
            "text: at doc: at\n",
            "text: of the doc: of the\n",
            "text: and doc:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"from annotators\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"and training\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"We present a\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"and analyze it\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"framework can\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"submitted for\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"are explained\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"and userfacing research\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and\n",
            "text: lessons doc: lessons\n",
            "text: learned doc: learned\n",
            "text: from doc: from\n",
            "text: are explained doc: are explained\n",
            "text: technical doc: technical\n",
            "text: and userfacing research doc: and userfacing research\n",
            "text: review is doc: review is\n",
            "text: powerful doc: powerful\n",
            "text: of doc: of\n",
            "text: coding doc: coding\n",
            "text: authors of doc: authors of\n",
            "text: inequitable doc: inequitable\n",
            "text: and outcomes doc: and outcomes\n",
            "text: where we withheld doc: where we withheld\n",
            "text: information doc: information\n",
            "text: software doc: software\n",
            "text: one doc: one\n",
            "text: anonymous author code doc: anonymous author code\n",
            "text: on reviewerauthor power doc: on reviewerauthor power\n",
            "text: barrier doc: barrier\n",
            "text: Based doc: Based\n",
            "text: we doc: we\n",
            "text: implement doc: implement\n",
            "text: zone doc: zone\n",
            "text: option doc: option\n",
            "text: revealing author doc: revealing author\n",
            "text: reveal author identity doc: reveal author identity\n",
            "text: in doc: in\n",
            "text: wide doc: wide\n",
            "text: reports on the doc: reports on the\n",
            "text: of doc: of\n",
            "text: training materials for doc: training materials for\n",
            "text: a doc: a\n",
            "text: prostate doc: prostate\n",
            "text: we doc: we\n",
            "text: deepened doc: deepened\n",
            "text: teams doc: teams\n",
            "text: to doc: to\n",
            "text: machine doc: machine\n",
            "text: learning model doc: learning model\n",
            "text: boundary doc:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"and outcomes\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"where we withheld\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"anonymous author code\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"on reviewerauthor power\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"revealing author\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"reveal author identity\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"reports on the\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"training materials for\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"learning model\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"materials by\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"experimental\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Conventional\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"reliable due\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"and AI signifies\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " boundary\n",
            "text: utility doc: utility\n",
            "text: the doc: the\n",
            "text: materials by doc: materials by\n",
            "text: information doc: information\n",
            "text: found doc: found\n",
            "text: an doc: an\n",
            "text: experimental doc: experimental\n",
            "text: Conventional doc: Conventional\n",
            "text: as doc: as\n",
            "text: in doc: in\n",
            "text: AI doc: AI\n",
            "text: in India doc: in India\n",
            "text: analysis of doc: analysis of\n",
            "text: fairness doc: fairness\n",
            "text: in India doc: in India\n",
            "text: We doc: We\n",
            "text: not doc: not\n",
            "text: reliable due doc: reliable due\n",
            "text: treatment doc: treatment\n",
            "text: ML doc: ML\n",
            "text: and AI signifies doc: and AI signifies\n",
            "text: unquestioning doc: unquestioning\n",
            "text: can be doc: can be\n",
            "text: where doc: where\n",
            "text: the distance doc: the distance\n",
            "text: oppressed doc: oppressed\n",
            "text: Instead doc: Instead\n",
            "text: fairness doc: fairness\n",
            "text: in India doc: in India\n",
            "text: provide doc: provide\n",
            "text: a roadmap doc: a roadmap\n",
            "text: communities doc: communities\n",
            "text: FairML doc: FairML\n",
            "text: ecosystems The widespread doc: ecosystems The widespread\n",
            "text: cell doc: cell"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"unquestioning\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"the distance\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"ecosystems The widespread\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"beneficiaries\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"automated messaging\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"during pregnancy\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"healthworker\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"time to place\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"drops To assist\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"key technical contribution\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "text: their doc: their\n",
            "text: beneficiaries doc: beneficiaries\n",
            "text: in doc: in\n",
            "text: describes doc: describes\n",
            "text: our doc: our\n",
            "text: work in doc: work in\n",
            "text: automated messaging doc: automated messaging\n",
            "text: during pregnancy doc: during pregnancy\n",
            "text: challenge doc: challenge\n",
            "text: is doc: is\n",
            "text: of doc: of\n",
            "text: healthworker doc: healthworker\n",
            "text: time to place doc: time to place\n",
            "text: crucial doc: crucial\n",
            "text: calls doc: calls\n",
            "text: interaction doc: interaction\n",
            "text: drops To assist doc: drops To assist\n",
            "text: nonprofits doc: nonprofits\n",
            "text: MultiArmed doc: MultiArmed\n",
            "text: key technical contribution doc: key technical contribution\n",
            "text: this doc: this\n",
            "text: is a doc: is a\n",
            "text: historical doc: historical\n",
            "text: major doc: major\n",
            "text: our RMAB doc: our RMAB\n",
            "text: via doc: via\n",
            "text: a realworld doc: a realworld\n",
            "text: improvement doc: improvement\n",
            "text: calls to doc: calls to\n",
            "text: 7 doc: 7\n",
            "text: weeks to reduce doc: weeks to reduce\n",
            "text: engagement doc: engagement\n",
            "text: improvement doc: improvement\n",
            "text: 3O% doc: 3O%\n",
            "text: the doc: the\n",
            "text: of doc: of\n",
            "text: real doc: real\n",
            "text: public health doc: public health\n",
            "text: for realworld doc: for realworld\n",
            "text: SonicHoop doc: SonicHoop\n",
            "text: is equipped doc: is equipped\n",
            "text: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"weeks to reduce\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"public health\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"for realworld\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"interactive sonification\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"and second transforming\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"instrument that\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"3 sonification strategies\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"2 professional\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"expressions to\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"their movement\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"sound improves\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"a pedagogical\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"instrument as well\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"other acrobatic\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "over doc: over\n",
            "text: touch doc: touch\n",
            "text: the hoop doc: the hoop\n",
            "text: interactive sonification doc: interactive sonification\n",
            "text: touch doc: touch\n",
            "text: and second transforming doc: and second transforming\n",
            "text: hoop doc: hoop\n",
            "text: instrument that doc: instrument that\n",
            "text: 3 sonification strategies doc: 3 sonification strategies\n",
            "text: ambient doc: ambient\n",
            "text: observation doc: observation\n",
            "text: 2 professional doc: 2 professional\n",
            "text: search doc: search\n",
            "text: expressions to doc: expressions to\n",
            "text: affect doc: affect\n",
            "text: their movement doc: their movement\n",
            "text: auditory doc: auditory\n",
            "text: sound improves doc: sound improves\n",
            "text: a pedagogical doc: a pedagogical\n",
            "text: instrument as well doc: instrument as well\n",
            "text: as doc: as\n",
            "text: interactive doc: interactive\n",
            "text: other acrobatic doc: other acrobatic\n",
            "text: people doc: people\n",
            "text: machine doc: machine\n",
            "text: to doc: to\n",
            "text: across doc: across\n",
            "text: when to rely doc: when to rely\n",
            "text: on doc: on\n",
            "text: Identifying the doc: Identifying the\n",
            "text: intersection doc: intersection\n",
            "text: NLP doc: NLP\n",
            "text: paper doc: paper\n",
            "text: advances doc: advances\n",
            "text: translations doc: translations\n",
            "text: identify doc: identify\n",
            "text: directions doc: directions\n",
            "text: translation doc: translation\n",
            "text: systems doc: systems\n",
            "text: support doc: support\n",
            "text: more doc: more\n",
            "text: and doc: and\n",
            "text:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"when to rely\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Identifying the\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"intersection\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"translations\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"questions for HCI\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"NLP research\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"responsibility\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"significantly\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"farmers in the\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " use doc: use\n",
            "text: machine doc: machine\n",
            "text: translation doc: translation\n",
            "text: We doc: We\n",
            "text: questions for HCI doc: questions for HCI\n",
            "text: NLP research doc: NLP research\n",
            "text: resources doc: resources\n",
            "text: As doc: As\n",
            "text: it doc: it\n",
            "text: is doc: is\n",
            "text: our doc: our\n",
            "text: responsibility doc: responsibility\n",
            "text: to doc: to\n",
            "text: draw from doc: draw from\n",
            "text: on doc: on\n",
            "text: realities doc: realities\n",
            "text: fieldwork doc: fieldwork\n",
            "text: and doc: and\n",
            "text: propose doc: propose\n",
            "text: future doc: future\n",
            "text: building AI doc: building AI\n",
            "text: significantly doc: significantly\n",
            "text: lives We doc: lives We\n",
            "text: these doc: these\n",
            "text: will doc: will\n",
            "text: serve as doc: serve as\n",
            "text: farmers in the doc: farmers in the\n",
            "text: bring doc: bring\n",
            "text: Internet doc: Internet\n",
            "text: has doc: has\n",
            "text: mobile doc: mobile\n",
            "text: have doc: have\n",
            "text: to develop doc: to develop\n",
            "text: connecting doc: connecting\n",
            "text: on doc: on\n",
            "text: Lite apps doc: Lite apps\n",
            "text: as doc: as\n",
            "text: constrained doc: constrained\n",
            "text: benefits to doc: benefits to\n",
            "text: their doc: their\n",
            "text: target doc: target\n",
            "text: for doc: for\n",
            "text: by doc: by\n",
            "text: diary doc: diary\n",
            "text: learning doc: learning\n",
            "text: expectations doc: expectations\n",
            "text: and doc: and\n",
            "text: challenging the way doc: challenging the way\n",
            "text: make doc: make\n",
            "text: Although doc: Although\n",
            "text: deep doc: deep\n",
            "text: music doc: music\n",
            "text: shown that it doc: shown that it\n",
            "text: can doc: can\n",
            "text: class of algorithms doc: class of algorithms\n",
            "text: present doc: present\n",
            "text: musician doc: musician\n",
            "text: total doc: total\n",
            "text: faced doc: faced\n",
            "text: leveraged doc: leveraged\n",
            "text: repurposed doc: repurposed\n",
            "text: overcome some doc: overcome some\n",
            "text: as doc: as\n",
            "text: multiple doc: multiple\n",
            "text: musical doc: musical\n",
            "text: song doc: song\n",
            "text: recombining doc: recombining\n",
            "text: ML doc: ML"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"expectations\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"challenging the way\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"shown that it\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"class of algorithms\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"overcome some\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "text: numbers doc: numbers\n",
            "text: of doc: of\n",
            "text: algorithmically doc: algorithmically\n",
            "text: the doc: the\n",
            "text: teams not only doc: teams not only\n",
            "text: with doc: with\n",
            "text: exploring doc: exploring\n",
            "text: findings reflect a doc: findings reflect a\n",
            "text: need doc: need\n",
            "text: to doc: to\n",
            "text: design doc: design\n",
            "text: that doc: that\n",
            "text: steerable doc: steerable\n",
            "text: interpretable doc: interpretable\n",
            "text: in doc: in\n",
            "text: will enable doc: will enable\n",
            "text: explore doc: explore\n",
            "text: can doc: can\n",
            "text: extend their doc: extend their\n",
            "text: mission is doc: mission is\n",
            "text: share doc: share\n",
            "text: achieved doc: achieved\n",
            "text: of doc: of\n",
            "text: is doc: is\n",
            "text: cover in doc: cover in\n",
            "text: to doc: to\n",
            "text: We doc: We\n",
            "text: that doc: that\n",
            "text: new approach to doc: new approach to\n",
            "text: shared doc: shared\n",
            "text: a doc: a\n",
            "text: that doc: that\n",
            "text: fulfills"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"algorithmically\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"teams not only\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"findings reflect a\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"interpretable\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"extend their\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"new approach to\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"It separates\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Wikipedia and\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"this notation\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " doc: fulfills\n",
            "text: It separates doc: It separates\n",
            "text: maintaining doc: maintaining\n",
            "text: in doc: in\n",
            "text: a doc: a\n",
            "text: Wikipedia and doc: Wikipedia and\n",
            "text: Wikilambda doc: Wikilambda\n",
            "text: can doc: can\n",
            "text: this notation doc: this notation\n",
            "text: the integration doc: the integration\n",
            "text: of the doc: of the\n",
            "text: results doc: results\n",
            "text: existing doc: existing\n",
            "text: available to doc: available to\n",
            "text: their doc: their\n",
            "text: language and doc: language and\n",
            "text: at doc: at\n",
            "text: and doc: and\n",
            "text: with doc: with\n",
            "text: contributions doc: contributions\n",
            "text: what their doc: what their\n",
            "text: respective doc: respective\n",
            "text: knowledge asset doc: knowledge asset\n",
            "text: can doc: can\n",
            "text: the doc: the\n",
            "text: vastly expand doc: vastly expand\n",
            "text: people doc: people\n",
            "text: do doc: do\n",
            "text: knowledge doc: knowledge\n",
            "text: Wikimedia doc: Wikimedia\n",
            "text: new venue doc: new venue\n",
            "text: creativity doc: creativity\n",
            "text: These doc: These\n",
            "text: will considerably doc: will considerably\n",
            "text: the doc: the\n",
            "text: enable every doc: enable every\n",
            "text: share doc: share\n",
            "text: the doc: the\n",
            "text: of all doc: of all\n",
            "text: Headbased doc: Headbased\n",
            "text: people doc: people\n",
            "text: devices This paper doc: devices This paper\n",
            "text: calibration doc: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"the integration\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"available to\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"language and\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"contributions\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"knowledge asset\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"vastly expand\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"will considerably\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"enable every\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"devices This paper\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"with subjects\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calibration\n",
            "text: mechanism doc: mechanism\n",
            "text: makes doc: makes\n",
            "text: that is doc: that is\n",
            "text: Law doc: Law\n",
            "text: a doc: a\n",
            "text: study doc: study\n",
            "text: method doc: method\n",
            "text: existing doc: existing\n",
            "text: Mouse doc: Mouse\n",
            "text: with subjects doc: with subjects\n",
            "text: we doc: we\n",
            "text: Fitts doc: Fitts\n",
            "text: a doc: a\n",
            "text: mobile doc: mobile\n",
            "text: tracker with subjects doc: tracker with subjects\n",
            "text: We doc: We\n",
            "text: extend prior doc: extend prior\n",
            "text: a doc: a\n",
            "text: of doc: of\n",
            "text: difficulties doc: difficulties\n",
            "text: promising doc: promising\n",
            "text: bps doc: bps\n",
            "text: was doc: was\n",
            "text: target doc: target\n",
            "text: moving nearly doc: moving nearly\n",
            "text: suggests doc: suggests\n",
            "text: is doc: is\n",
            "text: a doc: a\n",
            "text: design doc: design\n",
            "text: for doc: for\n",
            "text: Video summaries doc: Video summaries\n",
            "text: are a doc: are a\n",
            "text: for doc: for\n",
            "text: and doc: and\n",
            "text: amounts doc: amounts\n",
            "text: potentially doc: potentially\n",
            "text: biased doc: biased\n",
            "text: particular doc: particular\n",
            "text: depicted in doc: depicted in\n",
            "text: video doc: video\n",
            "text: to help doc: to help\n",
            "text: most doc: most\n",
            "text: video doc: video\n",
            "text: In doc: In\n",
            "text: paper doc: paper\n",
            "text: we doc: we\n",
            "text: results doc: results\n",
            "text: utility doc: utility\n",
            "text: video doc: video\n",
            "text: original doc: original\n",
            "text: video doc: video\n",
            "text: subtitles doc: subtitles\n",
            "text: and doc: and\n",
            "text: video doc: video\n",
            "text: and doc: and\n",
            "text: prominence doc: prominence\n",
            "text: explanations are doc: explanations are\n",
            "text: generated doc: generated\n",
            "text: meet doc: meet\n",
            "text: express doc: express\n",
            "text: prominence doc: prominence\n",
            "text: of doc: of\n",
            "text: coverage doc: coverage\n",
            "text: user doc: user\n",
            "text: of doc: of\n",
            "text: for doc: for\n",
            "text: for doc: for\n",
            "text: end doc: end\n",
            "text: Our doc: Our\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"tracker with subjects\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"extend prior\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"difficulties\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"moving nearly\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Video summaries\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"explanations are\" with entities \"[(11, 16, 'SUCCESS'), (28, 39, 'PROCESS'), (46, 53...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK-oSAIXEz6s"
      },
      "source": [
        "sampletext = (\"A significant number of college students suffer from mental health issues that impact their physical, social, and occupational outcomes. Various scalable technologies have been proposed in order to mitigate the negative impact of mental health disorders. However, the evaluation for these technologies, if done at all, often reports mixed results on improving users' mental health. We need to better understand the factors that align a user's attributes and needs with technology-based interventions for positive outcomes. In psychotherapy theory, therapeutic alliance and rapport between a therapist and a client is regarded as the basis for therapeutic success. In prior works, social robots have shown the potential to build rapport and a working alliance with users in various settings. In this work, we explore the use of a social robot coach to deliver positive psychology interventions to college students living in on-campus dormitories. We recruited 35 college students to participate in our study and deployed a social robot coach in their room. The robot delivered daily positive psychology sessions among other useful skills like delivering the weather forecast, scheduling reminders, etc. We found a statistically significant improvement in participants' psychological wellbeing, mood, and readiness to change behavior for improved wellbeing after they completed the study. Furthermore, students' personality traits were found to have a significant association with intervention efficacy. Analysis of the post-study interview revealed students' appreciation of the robot's companionship and their concerns for privacy.\")\n",
        "doc = nlp(sampletext)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZD8OX92Ez6t"
      },
      "source": [
        "## [Visual Display](https://spacy.io/usage/visualizers#ent) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OPKx1wjEz6t"
      },
      "source": [
        "from spacy import displacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW7jeH4IEz6u"
      },
      "source": [
        "displacy.render(doc, style=\"ent\") # or displacy.serve(doc, style=\"ent\") if not from jupyter notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn76T3qoEz6w"
      },
      "source": [
        "### Note\n",
        " * Read this paper by [Akbik et al.](https://alanakbik.github.io/papers/coling2018.pdf) should help in understanding the algorithm behind the sequence labelling i.e. multiple word entities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6rvuP_LEz6w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}